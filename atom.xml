<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-03-31T12:54:58+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using Hortonworks Hoya at SequenceIQ]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/24/hoya-at-sequenceiq/"/>
    <updated>2014-03-24T03:54:09+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/24/hoya-at-sequenceiq</id>
    <content type="html"><![CDATA[<p>With this blog post we are starting a series of articles where we&rsquo;d like to describe how we use YARN, why it is central to our product stack and why we believe that Hortonworks Hoya will be a determining building block in the Hadoop ecosystem.</p>

<p>While we don&rsquo;t want to get in details about <a href="http://hortonworks.com/hadoop/yarn/">YARN</a>, we&rsquo;d like to briefly explain the advantages of running an application on YARN, and introduce you to <a href="https://github.com/hortonworks/hoya">Hortonworks Hoya</a>.</p>

<p>At SequenceIQ we are building a multi-tenant, scale on demand data platform, with unpredictable batch and streaming workloads.
Before YARN we have tried different cluster management frameworks with Hadoop and managed to have pretty good results with <a href="http://aws.amazon.com/autoscaling/">Amazon EC2 Autoscaling groups</a>. Being true believers in open source and the need to diversify of provisioning Hadoop on different environments we needed to find an open source and &lsquo;standardised&rsquo; solution &ndash; welcome YARN.
(for example we provision Hadoop on <a href="http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker/">Docker</a>).</p>

<p>YARN separates the processing engine from the resource management &ndash; and acting effectively as an OS for Hadoop.
With YARN, you can now run multiple applications in Hadoop, all sharing a common resource management and improving cluster utilisation (we will release some metrics soon).
YARN also provides the following features out of the box:</p>

<ul>
<li>Multi-tenancy</li>
<li>Management and monitoring</li>
<li>High availability</li>
<li>Security</li>
<li>Failover and recovery</li>
</ul>


<p>All of the above and the effort of the Hadoop community and the wide adoption convinced us to start implementing our platform to run on top of YARN.
During our proof of concepts we went as far as starting all our non Hadoop (and not YARN compatible) applications on YARN &ndash; using <a href="https://github.com/hortonworks/hoya">Hoya</a>.</p>

<p>Hoya was introduced by Hortonworks mid last year &ndash; with the purpose to create Apache HBase clusters on YARN (since than it supports Apache Accumulo as well).
The code evolved pretty fast and now Hoya is a framework/application which allows you to deploy existing distributed applications on YARN &ndash; and benefit all the nice features of YARN.</p>

<p>In order to support different applications Hoya has a plugin provider architecture (supported plugins are in the <em>org.apache.hoya.providers</em> package).
Once a plugin is implemented (pretty straightforward, took us a few days only to understand and build a Flume and Tomcat plugin), the application is started in a YARN container and is monitored and controlled by YARN/Hoya.
The clusters can be started, stopped, frozen and re-sized dynamically &ndash; and in case of container failures Hoya deploys a replacement.</p>

<p>For a better architectural understanding of Hoya please read the following blog <a href="http://hortonworks.com/blog/hoya-hbase-on-yarn-application-architecture/">post</a>.</p>

<p>In this first post we would like to help you to get familiar with the benefits offered by Hoya and start an HBase cluster, re-scale it dynamically, freeze and stop.
First and foremost you will need an installation of Hadoop (2.3), the latest Hoya release (0.13.1) and HBase (0.98).
For your convenience we have put together an automated install script which lets you start with Hoya in a few minutes.</p>

<p>The script is available from our <a href="https://github.com/sequenceiq/hoya-docker/blob/master/hoya-centos-install.sh">GitHub page</a>.</p>

<p>Once Hadoop, HBase and Hoya are installed you can create an HBase cluster.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>create-hoya-cluster<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  hoya create hbase --role master 1 --role worker 1
</span><span class='line'>    --manager localhost:8032
</span><span class='line'>    --filesystem hdfs://localhost:9000 --image hdfs://localhost:9000/hbase.tar.gz
</span><span class='line'>    --appconf file:///tmp/hoya-master/hoya-core/src/main/resources/org/apache/hoya/providers/hbase/conf
</span><span class='line'>    --zkhosts localhost
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This will launch a 2 node HBase cluster (1 Master and 1 RegionServer). Now lets increase the number of RegionServers.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>flex-hoya-cluster<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  <span class="nv">num_of_workers</span><span class="o">=</span><span class="nv">$1</span>
</span><span class='line'>  hoya flex hbase --role worker <span class="nv">$num_of_workers</span> --manager localhost:8032 --filesystem hdfs://localhost:9000
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>This will start as many RegionServers as specified &ndash; in new YARN containers. Also the size of the cluster can be decreased if the load on the system does not demand for a larger number of RegionServers. The cluster can also be freezed (Hoya takes care about persisting the state).</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>freeze-hoya-cluster<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  hoya freeze hbase --manager localhost:8032 --filesystem hdfs://localhost:9000
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Finally when you&rsquo;d like to destroy the cluster and the state associated with the application you can use:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>destroy-hoya-cluster<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  hoya destroy hbase --manager localhost:8032 --filesystem hdfs://localhost:9000
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you see installing Hoya and starting different applications (HBase in this case) is very simple &ndash; and all the nice features of YARN are instantly available for any clustered applications.
In our next post we will drive you through the steps of creating your own Hoya provider, deploy it and run on a YARN cluster.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop 2.3 with docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker/"/>
    <updated>2014-03-19T03:54:09+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker</id>
    <content type="html"><![CDATA[<p>You want to try out hadoop 2.3? Go to the zoo and <a href="http://sethgodin.typepad.com/seths_blog/2005/03/dont_shave_that.html">shave a yak</a>.
Or simply just use <a href="https://www.docker.io/">docker</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t sequenceiq/hadoop-docker /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<h2>Testing</h2>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># start ssh and hdfs</span>
</span><span class='line'><span class="nb">cd</span> <span class="nv">$HADOOP_PREFIX</span>
</span><span class='line'>
</span><span class='line'><span class="c"># run the mapreduce</span>
</span><span class='line'>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.3.0.jar grep input output <span class="s1">&#39;dfs[a-z.]+&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="c"># check the output</span>
</span><span class='line'>bin/hdfs dfs -cat output/*
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>Yak shaving an elefant</h2>

<p>I had problems installing hadoop 2.3 and by googling i stumbled upon this <a href="http://mail-archives.apache.org/mod_mbox/hadoop-mapreduce-user/201403.mbox/%3C53192FD4.2040003@oss.nttdata.co.jp%3E">email thread</a>,
which references an <a href="http://aajisaka.github.io/hadoop-project/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation">alternative hadoop docs</a> deployed on github.</p>

<p>By following that description i run into an other issue:
hadoop is delivered with 32 bit native libraries. No big deal &hellip;</p>

<h2>Hadoop native libraries</h2>

<p>Of course there is an official <a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries Guide</a> it instructs you
to simple download the sources and <strong>mvn package</strong>. But than you face a new issue: missing <em>protobuf</em>. Eeeasy &hellip;</p>

<h2>Protobuf 2.5</h2>

<p>Unfortunately <strong>yum install protobuf</strong> installs an older 2.3 version, which is close but no cigar.
 So you download protobuf source, and <strong>./configure &amp;&amp; make &amp;&amp; make install</strong></p>

<p>To succeed on that one you have to install a couple of development packages, and there you go.</p>

<h2>Bintray</h2>

<p>I wanted to save you those steps so created a binary distro of the native libs
compiled with 64 bit CentOS. So I created <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.3.0/view/files">Bintray r̨epo</a>. Enjoy</p>

<h2>Automate everything</h2>

<p>As I&rsquo;m an automation fetishist, a Docker file was created, and released in the official <a href="https://index.docker.io/u/sequenceiq/hadoop-docker/">docker repo</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YARN Capacity Scheduler]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/14/yarn-capacity-scheduler/"/>
    <updated>2014-03-14T15:17:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/14/yarn-capacity-scheduler</id>
    <content type="html"><![CDATA[<p>Since the emergence of Hadoop 2 and the YARN based architecture we have a platform where we can run multiple applications (of different types) not constrained only to MapReduce. Different applications or different MapReduce job profiles have different resource needs, however since Hadoop 2.0 is a multi tenant platform the different users could have different access patterns or need for cluster capacity. In Hadoop 2.0 this is achieved through YARN schedulers — to allocate resources to various applications subject to constraints of capacities and queues (for more information on YARN follow this <a href="http://hortonworks.com/hadoop/yarn/">link</a> or feel free to ask us should you have any questions).</p>

<p>In Hadoop 2.0, the scheduler is a pluggable piece of code that lives inside the <em>ResourceManager</em> (the JobTracker in MR1) &ndash; the ultimate authority that arbitrates resources among all the applications in the system. The scheduler in YARN does not perform monitoring or status tracking and offers no guarantees to restart failed tasks — check our sample <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-queue-tests">GitHub</a> project to check how monitoring or progress can be tracked.</p>

<p>The Capacity Scheduler was designed to allow significantly higher cluster utilization while still providing predictability for Hadoop workloads, while sharing resources in a predictable and simple manner, using the common notion of <em>job queues</em>.</p>

<p>In our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-queue-tests">example</a> we show you how to use the Capacity Scheduler, configure queues with different priorities, submit MapReduce jobs into these queues, monitor and track the progress of the jobs &ndash; and ultimately see the differences between execution times and throughput of different queue setups.</p>

<p>First, let’s config the Capacity Scheduler (you can use xml, <a href="http://ambari.apache.org/">Apache Ambari</a> or you can configure queues programmatically). In this example we use a simple xml configuration.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.queues<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>default,highPriority,lowPriority<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.highPriority.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>70<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.lowPriority.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>20<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>We have 3 queues, with different queue setups/priorities. Each queue is given a <em>minimum</em> guaranteed percentage of total cluster capacity available &ndash; the total guaranteed capacity must equal 100%. In our example the <em>highPriority</em> queue has 70% of the resources, the <em>lowPriority</em> 20%, and the default queue has the remaining 10%. While it is not highlight in the example above, the Capacity Scheduler provides elastic resource scheduling, which means that if there are idle resources in the cluster, then one queue can take up more of the cluster capacity than was minimally allocated . In our case we could allocate a <em>maximum</em> capacity to the <em>lowPriority</em> queue:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root. lowPriority.maximum-capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>50<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>Now lets submit some jobs into these queues. We will use the QuasiMonteCarlo.java example (coming with Hadoop) &ndash; a MapReduce job that estimates the value of Pi, and submit the same MapReduce jobs into the low and high priority queues.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="c1">//get a configuration</span>
</span><span class='line'><span class="n">Configuration</span> <span class="n">priorityConf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
</span><span class='line'><span class="n">priorityConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">&quot;mapreduce.job.queuename&quot;</span><span class="o">,</span> <span class="n">queueName</span><span class="o">);</span>
</span><span class='line'><span class="err">………………………</span>      
</span><span class='line'><span class="c1">//submit the job</span>
</span><span class='line'><span class="n">JobID</span> <span class="n">jobID</span> <span class="o">=</span> <span class="n">QuasiMonteCarlo</span><span class="o">.</span><span class="na">submitPiEstimationMRApp</span><span class="o">(</span><span class="err">“</span><span class="n">Pi</span> <span class="n">estimation</span> <span class="nl">into:</span> <span class="err">&quot;</span><span class="o">+</span> <span class="n">queueName</span><span class="o">,</span> <span class="mi">10</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="n">tempDir</span><span class="o">,</span> <span class="n">priorityConf</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Once the jobs are submitted in the different queues, you can track the MapReduce job progress and monitor the queues through YARN. Using YARNRunner you can get ahold of a job status, and retrieve different informations:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="c1">//print overall job M/R progresses</span>
</span><span class='line'><span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;\nJob &quot;</span> <span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getJobName</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;in queue (&quot;</span> <span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getQueue</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;)&quot;</span> <span class="o">+</span> <span class="s">&quot; progress M/R: &quot;</span> <span class="o">+</span>              <span class="n">jobStatus</span><span class="o">.</span><span class="na">getMapProgress</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;/&quot;</span> <span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getReduceProgress</span><span class="o">());</span>
</span><span class='line'><span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;Tracking URL : &quot;</span> <span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getTrackingUrl</span><span class="o">());</span>
</span><span class='line'><span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;Reserved memory : &quot;</span> <span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getReservedMem</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;, used memory : &quot;</span><span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getUsedMem</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot; and          used slots : &quot;</span><span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getNumUsedSlots</span><span class="o">());</span>
</span><span class='line'>  
</span><span class='line'><span class="c1">// list map &amp; reduce tasks statuses and progress     </span>
</span><span class='line'><span class="n">TaskReport</span><span class="o">[]</span> <span class="n">reports</span> <span class="o">=</span> <span class="n">yarnRunner</span><span class="o">.</span><span class="na">getTaskReports</span><span class="o">(</span><span class="n">jobID</span><span class="o">,</span> <span class="n">TaskType</span><span class="o">.</span><span class="na">MAP</span><span class="o">);</span>
</span><span class='line'><span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">reports</span><span class="o">.</span><span class="na">length</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
</span><span class='line'><span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;MAP: Status &quot;</span> <span class="o">+</span> <span class="n">reports</span><span class="o">[</span><span class="n">i</span><span class="o">].</span><span class="na">getCurrentStatus</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot; with task ID &quot;</span> <span class="o">+</span> <span class="n">reports</span><span class="o">[</span><span class="n">i</span><span class="o">].</span><span class="na">getTaskID</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;, and                 progress &quot;</span> <span class="o">+</span> <span class="n">reports</span><span class="o">[</span><span class="n">i</span><span class="o">].</span><span class="na">getProgress</span><span class="o">());</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Same way the queue capacity can be tracked as well:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">ArrayNode</span> <span class="n">queues</span> <span class="o">=</span> <span class="o">(</span><span class="n">ArrayNode</span><span class="o">)</span> <span class="n">jsonNode</span><span class="o">.</span><span class="na">path</span><span class="o">(</span><span class="s">&quot;scheduler&quot;</span><span class="o">).</span><span class="na">path</span><span class="o">(</span><span class="s">&quot;schedulerInfo&quot;</span><span class="o">).</span><span class="na">path</span><span class="o">(</span><span class="s">&quot;queues&quot;</span><span class="o">).</span><span class="na">get</span><span class="o">(</span><span class="s">&quot;queue&quot;</span><span class="o">);</span>
</span><span class='line'><span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">queues</span><span class="o">.</span><span class="na">size</span><span class="o">();</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
</span><span class='line'><span class="n">JsonNode</span> <span class="n">queueNode</span> <span class="o">=</span> <span class="n">queues</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">i</span><span class="o">);</span>                       
</span><span class='line'><span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;queueName / usedCapacity / absoluteUsedCap / absoluteCapacity / absMaxCapacity: &quot;</span> <span class="o">+</span>
</span><span class='line'>                  <span class="n">queueNode</span><span class="o">.</span><span class="na">findValue</span><span class="o">(</span><span class="s">&quot;queueName&quot;</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; / &quot;</span> <span class="o">+</span>
</span><span class='line'>                  <span class="n">queueNode</span><span class="o">.</span><span class="na">findValue</span><span class="o">(</span><span class="s">&quot;usedCapacity&quot;</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; / &quot;</span> <span class="o">+</span>
</span><span class='line'>                  <span class="n">queueNode</span><span class="o">.</span><span class="na">findValue</span><span class="o">(</span><span class="s">&quot;absoluteUsedCapacity&quot;</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; / &quot;</span> <span class="o">+</span>
</span><span class='line'>                  <span class="n">queueNode</span><span class="o">.</span><span class="na">findValue</span><span class="o">(</span><span class="s">&quot;absoluteCapacity&quot;</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; / &quot;</span> <span class="o">+</span>
</span><span class='line'>                  <span class="n">queueNode</span><span class="o">.</span><span class="na">findValue</span><span class="o">(</span><span class="s">&quot;absoluteMaxCapacity&quot;</span><span class="o">));</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can run the example with <code>hadoop jar yarn-queue-tests-1.0-SNAPSHOT.jar com.sequenceiq.yarntest.client.JobClient</code>, and play with different queue setups. Once you have changed the queue setup you can refresh the settings with <code>yarn rmadmin -refreshQueues</code>.</p>

<p>You can check the progress through the <a href="https://gist.github.com/matyix/9528220">logs</a>.The <a href="http://sandbox.hortonworks.com:8088/cluster/scheduler">cluster statistics</a> and <a href="http://sandbox.hortonworks.com:8088/cluster/apps">application statistics</a> are available as well (we run this example on Hortonworks HDP2 sandbox, but any other Hadoop 2 distribution works &ndash; you can set your own cluster on Amazon EC2 using SequenceIQ&rsquo;s setup scripts from our <a href="https://github.com/sequenceiq/hadoop-cloud-scripts">GitHub</a> page).</p>

<p>As you expect, the jobs submitted into the  <code>highPriority</code> queue are finished earlier than those submitted into the <code>lowPriority</code> one &ndash; though (in case of submitting into the same queue) the MapReduce jobs should take the same time (as they are the same MapReduce job, have the same job profile).</p>

<p>This is a good way to start experimenting multi-tenancy and parallel jobs submission into a shared cluster (beyond the Fair Scheduler). At <a href="http://sequenceiq.com">SequenceIQ</a> we are working on a heuristic YARN scheduler &ndash; where we can adapt to increased work loads, submit jobs into queues based on different customer QoS profiles, and increase or downsize our cloud based cluster based on load and capacity.</p>

<p>For more details and updates please follow us through our <a href="http://www.linkedin.com/company/sequenceiq/">LinkedIn</a> page.
You can access the code from our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-queue-tests">GitHub</a> repository.</p>

<p>Thanks,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data cleaning with MapReduce and Morphlines]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines/"/>
    <updated>2014-03-11T09:21:07+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines</id>
    <content type="html"><![CDATA[<p>In one of our <a href="http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality/">previous</a> posts we showed how easy is to extend the Kite Morphlines framework with your custom commands. In this post we are going to use it to remove columns from a dataset to demonstrate how it can be used and embeded in MapReduce jobs.
Download the MovieLens + IMDb/Rotten Tomatoes dataset from <a href="http://grouplens.org/datasets/hetrec-2011/">Grouplens</a>, extract it, and it should contain a file called user_ratedmovies.dat.
It is a simple tsv file &ndash; we are going to use the same column names as it shows in the first line (header)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>userID   movieID rating  date_day  date_month  date_year date_hour  date_minute  date_second
</span><span class='line'>75        3       1       29       10           2006      23          17          16
</span><span class='line'>75        32      4.5     29       10           2006      23          23          44
</span><span class='line'>75        110     4       29       10           2006      23          30          8
</span><span class='line'>75        160     2       29       10           2006      23          16          52
</span><span class='line'>75        163     4       29       10           2006      23          29          30
</span><span class='line'>75        165     4.5     29       10           2006      23          25          15
</span><span class='line'>75        173     3.5     29       10           2006      23          17          37</span></code></pre></td></tr></table></div></figure>


<p>Let’s just pretend that we don’t need all the data from the file and remove the last 3 columns (date_hour, date_minute, date_second). We can achieve this task with the following 2 commands:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  readCSV {
</span><span class='line'>          separator : "\t"
</span><span class='line'>          columns : [userID,movieID,rating,date_day,date_month,date_year,date_hour,date_minute,date_second]
</span><span class='line'>          ignoreFirstLine : false
</span><span class='line'>          trim : true
</span><span class='line'>          charset : UTF-8
</span><span class='line'>  }
</span><span class='line'>}  
</span><span class='line'>
</span><span class='line'>{
</span><span class='line'>  java {
</span><span class='line'>        imports : "import java.util.*;"
</span><span class='line'>        code: """
</span><span class='line'>          record.removeAll("date_hour");
</span><span class='line'>          record.removeAll("date_minute");
</span><span class='line'>          record.removeAll("date_second");
</span><span class='line'>        return child.process(record);
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>Now lets create our mapper only job to process the data. What we need to do is build the Morphlines command chain by parsing the configuration file as shown</p>

<figure class='code'><figcaption><span>protected void setup(Context context)</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">File</span> <span class="n">morphLineFile</span> <span class="o">=</span> <span class="k">new</span> <span class="n">File</span><span class="o">(</span><span class="n">context</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">MORPHLINE_FILE</span><span class="o">));</span>
</span><span class='line'><span class="n">String</span> <span class="n">morphLineId</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">MORPHLINE_ID</span><span class="o">);</span>
</span><span class='line'><span class="n">RecordEmitter</span> <span class="n">recordEmitter</span> <span class="o">=</span> <span class="k">new</span> <span class="n">RecordEmitter</span><span class="o">(</span><span class="n">context</span><span class="o">);</span>
</span><span class='line'><span class="n">MorphlineContext</span> <span class="n">morphlineContext</span> <span class="o">=</span> <span class="k">new</span> <span class="n">MorphlineContext</span><span class="o">.</span><span class="na">Builder</span><span class="o">().</span><span class="na">build</span><span class="o">();</span>
</span><span class='line'><span class="n">Command</span> <span class="n">morphline</span> <span class="o">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="na">kitesdk</span><span class="o">.</span><span class="na">morphline</span><span class="o">.</span><span class="na">base</span><span class="o">.</span><span class="na">Compiler</span><span class="o">().</span><span class="na">compile</span><span class="o">(</span><span class="n">morphLineFile</span><span class="o">,</span> <span class="n">morphLineId</span><span class="o">,</span> <span class="n">morphlineContext</span><span class="o">,</span> <span class="n">recordEmitter</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>and pass the lines through it.</p>

<figure class='code'><figcaption><span>protected void map(Object key, Text value, Context context)</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">Record</span> <span class="n">record</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Record</span><span class="o">()</span>
</span><span class='line'><span class="n">record</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="n">Fields</span><span class="o">.</span><span class="na">ATTACHMENT_BODY</span><span class="o">,</span> <span class="k">new</span> <span class="n">ByteArrayInputStream</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="na">toString</span><span class="o">().</span><span class="na">getBytes</span><span class="o">()));</span>
</span><span class='line'><span class="k">if</span> <span class="o">(!</span><span class="n">morphline</span><span class="o">.</span><span class="na">process</span><span class="o">(</span><span class="n">record</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;Morphline failed to process record: {}&quot;</span><span class="o">,</span> <span class="n">record</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="n">record</span><span class="o">.</span><span class="na">removeAll</span><span class="o">(</span><span class="n">Fields</span><span class="o">.</span><span class="na">ATTACHMENT_BODY</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Notice that the compile method takes an important parameter called finalChild which is in our example the <code>RecordEmitter</code>.
The returned command will feed records into finalChild which means if this parameter is not provided a DropRecord command will
be assigned automatically. In Apache Flume there is a Collector command to avoid loosing any transformed record.
The only thing left is to outbox the processed record and write the results to HDFS. The RecordEmitter will serve this purpose:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'><span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">process</span><span class="o">(</span><span class="n">Record</span> <span class="n">record</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">line</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">record</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
</span><span class='line'>  <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">line</span><span class="o">,</span> <span class="kc">null</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">LOGGER</span><span class="o">.</span><span class="na">warn</span><span class="o">(</span><span class="s">&quot;Cannot write record to context&quot;</span><span class="o">,</span> <span class="n">e</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="k">return</span> <span class="kc">true</span><span class="o">;</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>By default the readCSV command extracts the ATTACHMENT_BODY into headers with id provided in the columns field so the
transformed data will look like this (3 columns were dropped):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">{</span><span class="n">date_day</span><span class="o">=[</span><span class="mi">10</span><span class="o">],</span> <span class="n">date_month</span><span class="o">=[</span><span class="mi">10</span><span class="o">],</span> <span class="n">date_year</span><span class="o">=[</span><span class="mi">2008</span><span class="o">],</span> <span class="n">movieID</span><span class="o">=[</span><span class="mi">62049</span><span class="o">],</span> <span class="n">rating</span><span class="o">=[</span><span class="mf">4.5</span><span class="o">],</span> <span class="n">userID</span><span class="o">=[</span><span class="mi">71534</span><span class="o">]}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The source code is available in our samples repository on <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub</a>.
It is just a simple example but you can go further and download a much bigger dataset with 10 millions of lines and process it with multiple nodes to see how it scales.</p>

<p>Enjoy,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS and java.nio.channels]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/07/read-from-hdfs/"/>
    <updated>2014-03-07T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/07/read-from-hdfs</id>
    <content type="html"><![CDATA[<p>Many times there is a need to access files or interact with HDFS from Java applications or libraries. Hadoop has built in many tools in order to work or interact with HDFS &ndash; however in case you&rsquo;d like to read into a content of a file remotely (e.g. retrieve the headers of a CSV/TSV file) random exceptions can occurs. One of these remote exceptions coming from the HDFS NameNode is a <em>java.io.IOException: File /user/abc/xyz/ could only be replicated to 0 nodes, instead of 1.</em></p>

<p>Such an exception can be reproduced by the following code snippet:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>java BufferedInputStream bufferedInputStream
</span><span class='line'>
</span><span class='line'>/**
</span><span class='line'> * For the sake of readability, try/cacth/finally blocks are removed 
</span><span class='line'> * Don't Say We Didn't Warn You
</span><span class='line'> */
</span><span class='line'>
</span><span class='line'>FileSystem fs = FileSystem.get(configuration);
</span><span class='line'>          Path filePath = getFilePath(dataPath);
</span><span class='line'>
</span><span class='line'>BufferedInputStream bufferedInputStream = new BufferedInputStream(fs.open(filePath));
</span><span class='line'>  listReader = new CsvListReader(new BufferedReader(new InputStreamReader(bufferedInputStream)),
</span><span class='line'>                      CsvPreference.STANDARD_PREFERENCE);
</span><span class='line'>                     </span></code></pre></td></tr></table></div></figure>


<p>The exception looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ERROR SimpleFeatureSelector:67 - Exception {}
</span><span class='line'>java.lang.IllegalStateException: Must not use direct buffers with InputStream API
</span><span class='line'>  at com.google.common.base.Preconditions.checkState(Preconditions.java:176)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
</span><span class='line'>  at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:170)</span></code></pre></td></tr></table></div></figure>


<p><em>Note: actually all HDFS operations fail in case of the underlying input stream does not have a readable channel (check the java.nio.channels package. RemoteBlockReader2 needs channel based inputstreams to deal with direct buffers.</em></p>

<p>Digging into details and checking the Hadoop 2.2 source code we find the followings:</p>

<p>Through the<code>org.apache.hadoop.hdfs.BlockReaderFactory</code> you can get access to a BlockReader implementation like <code>org.apache.hadoop.hdfs.RemoteBlockReader2</code>, which it is responsible for reading a single block from a single datanode.</p>

<p>The blockreader is retrieved in the following way:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@SuppressWarnings</span><span class="o">(</span><span class="s">&quot;deprecation&quot;</span><span class="o">)</span>
</span><span class='line'><span class="kd">public</span> <span class="kd">static</span> <span class="n">BlockReader</span> <span class="nf">newBlockReader</span><span class="o">(</span>
</span><span class='line'>                                     <span class="n">Conf</span> <span class="n">conf</span><span class="o">,</span>
</span><span class='line'>                               <span class="n">Socket</span> <span class="n">sock</span><span class="o">,</span> <span class="n">String</span> <span class="n">file</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">ExtendedBlock</span> <span class="n">block</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">Token</span><span class="o">&lt;</span><span class="n">BlockTokenIdentifier</span><span class="o">&gt;</span> <span class="n">blockToken</span><span class="o">,</span>
</span><span class='line'>                                     <span class="kt">long</span> <span class="n">startOffset</span><span class="o">,</span> <span class="kt">long</span> <span class="n">len</span><span class="o">,</span>
</span><span class='line'>                                     <span class="kt">int</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="kt">boolean</span> <span class="n">verifyChecksum</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">String</span> <span class="n">clientName</span><span class="o">)</span>
</span><span class='line'>                                     <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">conf</span><span class="o">.</span><span class="na">useLegacyBlockReader</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">RemoteBlockReader</span><span class="o">.</span><span class="na">newBlockReader</span><span class="o">(</span>
</span><span class='line'>          <span class="n">sock</span><span class="o">,</span> <span class="n">file</span><span class="o">,</span> <span class="n">block</span><span class="o">,</span> <span class="n">blockToken</span><span class="o">,</span> <span class="n">startOffset</span><span class="o">,</span> <span class="n">len</span><span class="o">,</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="n">verifyChecksum</span><span class="o">,</span> <span class="n">clientName</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">RemoteBlockReader2</span><span class="o">.</span><span class="na">newBlockReader</span><span class="o">(</span>
</span><span class='line'>          <span class="n">sock</span><span class="o">,</span> <span class="n">file</span><span class="o">,</span> <span class="n">block</span><span class="o">,</span> <span class="n">blockToken</span><span class="o">,</span> <span class="n">startOffset</span><span class="o">,</span> <span class="n">len</span><span class="o">,</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="n">verifyChecksum</span><span class="o">,</span> <span class="n">clientName</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>In order to avoid the exception and use the right version of the block reader, the followin property <code>conf.useLegacyBlockReader</code> should be TRUE.</p>

<p>Long story short, the configuration set of a job should be set to: <code>configuration.set("dfs.client.use.legacy.blockreader", "true")</code>.</p>

<p>Unluckily in all cases when interacting with HDFS, and the underlying input stream does not have a readable channel, you can&rsquo;t use the <em>RemoteBlockReader2</em> implementation, but you have to fall back to the old legacy <em>RemoteBlockReader</em>.</p>

<p>Hope this helps,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Accessing HDP2 sandbox from the host]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/05/access-hdp2-sandbox/"/>
    <updated>2014-03-05T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/05/access-hdp2-sandbox</id>
    <content type="html"><![CDATA[<p>During development of a Hadoop project people have many options of where and how to run Hadoop. We at SequenceIQ use different environments as well (cloud based, VM or host) &ndash; and different versions/vendor distributions. A very popular distribution among developers is the Hortonworks Sandbox &ndash; which contains the latest releases across Hadoop (2.2.0) and the key related projects into a single integrated and tested platform.
While using the sandbox gets you going running a single node Hadoop (pseudo distributed) in less than 5 minutes, many developers find inconvenient to &lsquo;live&rsquo; and work inside the VM when deploying, debugging or submitting jobs into a Hadoop cluster.</p>

<p>There is a well documented VM host file configuration on the <a href="http://docs.hortonworks.com/">Hortonworks site</a> describing how to start interacting with the VM sandbox from outside (e.g host machine), but quite soon this will turn into a port-forwarding saga (those who know how many ports does Hadoop and the ecosystem use will know what we mean). An easier and more elegant way is to use a SOCKS5 proxy (which comes with SSL by default).
Check this short goal/problem/resolution and code example snippet if you&rsquo;d like to interact with the Hortonworks Sandbox from your host (outside the VM).</p>

<h2>Goal</h2>

<ul>
<li>accessing the pseudo distributed hadoop cluster from the  host</li>
<li>reading / writing to the  HDFS</li>
<li>submitting  M/R jobs to the RM</li>
</ul>


<h2>Problem(s)</h2>

<ul>
<li>it&rsquo;s hard to reach resources inside the sandbox (e.g. interact with HDFS, or the DataNode)</li>
<li>lots of ports need to be portforwarded</li>
<li>entries to be added to the hosts file of the  host machine</li>
<li>circumstantial configuration of clients  accessing the sandbox</li>
</ul>


<h2>Resolution</h2>

<ul>
<li>use an SSL socks proxy</li>
</ul>


<h2>Example</h2>

<ul>
<li>check the following sample from our <em><a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/hdp-sandbox-access">GitHub page</a></em></li>
</ul>


<p>Start the SOCKS proxy</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>ssh root@127.0.0.1 -p 2222 -D 1099
</span></code></pre></td></tr></table></div></figure>


<p>Once the proxy is up and running, edit the core-site.xml</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>      <span class="nt">&lt;name&gt;</span>hadoop.socks.server<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>      <span class="nt">&lt;value&gt;</span>localhost:1099<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>      <span class="nt">&lt;name&gt;</span>hadoop.rpc.socket.factory.class.default<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>      <span class="nt">&lt;value&gt;</span>org.apache.hadoop.net.SocksSocketFactory<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now you can run the test client</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>     
</span><span class='line'><span class="c"># You can use Maven</span>
</span><span class='line'>mvn <span class="nb">exec</span>:java -Dexec.mainClass<span class="o">=</span><span class="s2">&quot;com.sequenceiq.samples.SandboxTester&quot;</span> -Dexec.args<span class="o">=</span><span class="s2">&quot;hdfs sandbox 8020&quot;</span> -Dhadoop.home.dir<span class="o">=</span>/tmp
</span><span class='line'>      
</span><span class='line'><span class="c"># or run from the JAR file</span>
</span><span class='line'>      
</span><span class='line'>java -jar sandbox-playground-1.0.jar hdfs sandbox 8020
</span></code></pre></td></tr></table></div></figure>


<p>As you see it&rsquo;s pretty easy and convenient to use the Hortonworks sandbox as a pre-configured development environment.</p>

<p>In case you&rsquo;d like to use (as we do most of the time) a Hadoop cluster in the cloud (Amazon EC2), check our previous blog post <a href="http://blog.sequenceiq.com/blog/2014/02/07/hdp2-on-amazon/">HDP2 on Amazon</a>.
In case you ever wondered whether it&rsquo;s possible to use Hadoop with Docker please follow this blog.</p>

<p>Hope this helps,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ETL - producing better quality data]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality/"/>
    <updated>2014-02-28T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality</id>
    <content type="html"><![CDATA[<p>On my way to work this morning I read an interesting article about the quality of data being produced by different systems and applications. While the article was emphasizing that the quality of the data should not be an IT problem (but management), our believe is that at the high volume, velocity and variety (the &ldquo;3Vs&rdquo; of big data) the data is produced today, the process of producing data is a shared responsibility between management and the IT department.</p>

<p>Since the emerging of Hadoop, the TCO of storing large amounts of data in HDFS is lower than ever before &ndash; and now it makes sense to store all the data an enterprise produces in order to find patterns, correlations and break the data silos &ndash; something which was very specific for different departments within an organization. Storing such an amount of data (structured, unstructured, logs, clickstream, etc) inevitable produces a &lsquo;bad&rsquo; data quality &ndash; but this depends on your point of view. For us data is just data &ndash; we don&rsquo;t want to qualify it &ndash; and has it&rsquo;s own intrinsic value, but the quality of it depends on the ETL process. When someone engages with our API and the xTract Spacetime platform, among the first step is the configuration of data sources, and the attached ETL processes. We offer an extremely sophisticated ETL process and the ability to &lsquo;clean&rsquo; the data (batch or streaming) while arrives into xTract Spacetime, but we always suggest our customers to keep the raw data as well.</p>

<p>During the architecture of the xTract Spacetime platform we have tried and PoCd different ETL frameworks and implementations &ndash; and we choose <a href="https://github.com/kite-sdk/kite/tree/master/kite-morphlines">Kite Morphlines</a> being at the core of our ETL process. Morphlines is an open source framework that reduces the time and skills necessary to build and change Hadoop ETL stream processing applications that extract, transform and load data into Apache Solr, HBase, HDFS, Enterprise Data Warehouses, or Analytic Online Dashboards.</p>

<!-- more -->


<p>Since runs on Hadoop, scalability is not a problem &ndash; we have seen enterprises producing 50 terabytes data per day and missing the 24 hour ETL window, by not being able to scale horizontally their ETL processes. Morphlines is built on top of the Kite framework (a great framework for making easier to build systems on top of the Hadoop), and it&rsquo;s extremely easy to extend. We would like to show and give you examples about how to use and create a Morphlines Command to implement your custom transformation (if the many existing ones does not fit your requirement).</p>

<p>The implementation of a Command starts with implementing a CommandBuilder</p>

<p>Actually a new morphline command implementation is not that hard. You have to implement a builder class (see below), define the name of the command and  extend the AbstractCommand base class. That simple.</p>

<figure class='code'><figcaption><span>ToLowerCaseBuilder implements CommandBuilder</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'>  <span class="kd">public</span> <span class="n">Collection</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="nf">getNames</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">Collections</span><span class="o">.</span><span class="na">singletonList</span><span class="o">(</span><span class="s">&quot;toLowerCase&quot;</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span>toLowerCase Morphlines command</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'><span class="kd">protected</span> <span class="kt">boolean</span> <span class="nf">doProcess</span><span class="o">(</span><span class="n">Record</span> <span class="n">record</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">ListIterator</span> <span class="n">iter</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">fieldName</span><span class="o">).</span><span class="na">listIterator</span><span class="o">();</span>
</span><span class='line'>  <span class="k">while</span> <span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="na">hasNext</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">iter</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">transformFieldValue</span><span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="na">next</span><span class="o">()));</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>    <span class="k">return</span> <span class="kd">super</span><span class="o">.</span><span class="na">doProcess</span><span class="o">(</span><span class="n">record</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="kd">private</span> <span class="n">Object</span> <span class="nf">transformFieldValue</span><span class="o">(</span><span class="n">Object</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="na">toString</span><span class="o">().</span><span class="na">toLowerCase</span><span class="o">(</span><span class="n">locale</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To configure your new morhline command</p>

<figure class='code'><figcaption><span>toLowerCase config</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">morphlines</span> <span class="o">:</span> <span class="o">[</span>
</span><span class='line'>  <span class="o">{</span>
</span><span class='line'>   <span class="n">id</span> <span class="o">:</span> <span class="n">morphline1</span>
</span><span class='line'>       <span class="n">importCommands</span> <span class="o">:</span> <span class="o">[</span><span class="s">&quot;com.sequenceiq.samples.**&quot;</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>       <span class="n">commands</span> <span class="o">:</span> <span class="o">[</span>
</span><span class='line'>         <span class="o">{</span>
</span><span class='line'>           <span class="n">toLowerCase</span> <span class="o">{</span>
</span><span class='line'>             <span class="n">field</span> <span class="o">:</span> <span class="n">Name</span>
</span><span class='line'>             <span class="n">locale</span> <span class="o">:</span> <span class="n">en_us</span>
</span><span class='line'>           <span class="o">}</span>
</span><span class='line'>         <span class="o">}</span>
</span><span class='line'>       <span class="o">]</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>There is a custom tester tool which waiting a file as an input, a file as a config and an expected output. Among our plans is to build a UI on top of Kite Morphlines as well &ndash; part of our product stack. While we consider the Morhplines configuration, and defining the transformations simple and easy to use, many of our users might prefer a custom UI whee you can define your own ETL process visually.</p>

<p>That’s it. You can find the samples at our <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub page</a>.
Hope you enjoy it, let us know if you need help or have any questions.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vote for us - 2014 Hadoop Summit San Jose]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/02/26/vote-for-us/"/>
    <updated>2014-02-26T10:17:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/02/26/vote-for-us</id>
    <content type="html"><![CDATA[<p>While we are extremely proud that our abstract came 2nd (out of 107) in the 2014 Hadoop Summit in Amsterdam (see you all there in April 2-3), we will not stop there and our plan is to continue the hard work and we&rsquo;re looking forward to meet you at 2014 Hadoop Summit in San Jose.
We would like to ask for your support by submitting your vote for our session in the largest Hadoop conference in the world.</p>

<p>Please use the following link to vote, or read our abstract below.</p>

<p><a href="http://hadoopsummit.uservoice.com/forums/242807-hadoop-deployment-operations-track/suggestions/5568417-moving-to-hadoop-2-0-yarn-at-sequenceiq">Vote for us</a></p>

<p>Should you have any questions regarding our abstract, the technical solution or implementation detailsm feel free to contact us or
check our <a href="https://github.com/sequenceiq">GitHub</a> page.</p>

<p><strong>Moving to Hadoop 2.0/YARN at SequenceIQ</strong></p>

<p>A showcase of our efforts to bring all our Hadoop based applications under one common cluster management framework &ndash; YARN.
Our deployment consists of MR2, HBase, Mahout and Hive-all running within one single auto-scaling cluster. We have faced many challenges such as load imbalances, SLA misses, cluster scheduling and VM container deployments &ndash; and would like to share our struggle and solution with the community.
As a startup, cost savings is important for us &ndash; switching to Hadoop 2.0 helped us save significant costs through better utilization of our hardware and cloud VMs. Our decision and investment of moving to YARN has paid off &ndash; and opened up new business and technical opportunities.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Custom Apache Flume source]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/02/22/custom-flume-source/"/>
    <updated>2014-02-22T14:45:48+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/02/22/custom-flume-source</id>
    <content type="html"><![CDATA[<p>The process of data analytics starts with collecting the data into a common system, in our case a Hadoop cluster. Flume is an Apache project aiming to help us solve this problem in a very efficient and elegant way.</p>

<p>In Flume terminology a source is responsible to listen and consume events coming from many distributed clients and forwards them to one or more channels. Events can have any arbitrary format, it all depends on what source do we use. Flume provides many sources, but only a few of them is capable to collect data through network.</p>

<p>In this article I will discuss how you can implement your own that meets your demands through creating a websocket source.
There are two types of sources: event driven and pollable. In case of a pollable source, Flume will start a thread to periodically call the following method to check whether there is new data available or not:</p>

<figure class='code'><figcaption><span>PollableSource interface</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="n">Status</span> <span class="nf">process</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">EventDeliveryException</span><span class="o">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>With event driven source you will have to take care yourself of receiving the data from the clients. For our websocket example we will use embedded Jetty 9.1. Extend the AbstractEventDrivenSource class and override the mandatory methods to bootstrap the source. In the doConfigure method you can ask the properties you need from the context. These properties are coming from your agent’s configuration file. More on this later..</p>

<figure class='code'><figcaption><span>protected void doConfigure(Context context)</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">host</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="n">HOST_KEY</span><span class="o">);</span>
</span><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">port</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getInteger</span><span class="o">(</span><span class="n">PORT_KEY</span><span class="o">);</span>
</span><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">path</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="n">PATH_KEY</span><span class="o">);</span>
</span><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">enableSsl</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getBoolean</span><span class="o">(</span><span class="n">SSL_KEY</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Eventually the doStart will kick off the embedded Jetty as shown:</p>

<figure class='code'><figcaption><span>protected void doStart()</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="k">try</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">JettyWebSocketServer</span> <span class="n">server</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JettyWebSocketServer</span><span class="o">(</span><span class="n">host</span><span class="o">,</span> <span class="n">port</span><span class="o">,</span> <span class="n">path</span><span class="o">,</span> <span class="n">getChannelProcessor</span><span class="o">());</span>
</span><span class='line'>    <span class="n">server</span><span class="o">.</span><span class="na">start</span><span class="o">();</span>
</span><span class='line'><span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">LOGGER</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="s">&quot;Error starting jetty server&quot;</span><span class="o">,</span> <span class="n">e</span><span class="o">);</span>
</span><span class='line'>    <span class="k">throw</span> <span class="k">new</span> <span class="nf">FlumeException</span><span class="o">(</span><span class="n">e</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>Channel processor plays an important role here. Its purpose to forward the incoming events to the configured channels.</p>

<p>Creating an embedded Jetty server is pretty easy and straightforward even with SSL support. I am not going into details you can find the source code here <a href="https://github.com/sequenceiq/sequenceiq-samples">https://github.com/sequenceiq/sequenceiq-samples</a> You will have to create a Servlet which will create a new listener for every session or you can just simply ignore some requests based on different headers. On new message all you have to do is create a flume event out of it and pass is to the channelprocessor.</p>

<figure class='code'><figcaption><span>public void onWebSocketText(String s) </span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">SimpleEvent</span> <span class="n">event</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SimpleEvent</span><span class="o">();</span>
</span><span class='line'><span class="n">event</span><span class="o">.</span><span class="na">setBody</span><span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="na">getBytes</span><span class="o">());</span>
</span><span class='line'><span class="n">channelProcessor</span><span class="o">.</span><span class="na">processEvent</span><span class="o">(</span><span class="n">event</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>From this point the data will travel through the configured channels and sinks to end up on its final destination. It is committed in one transaction so if any component fails the whole process fails.</p>

<p>To deploy your custom source put the packaged jar to Flume’s classpath.</p>

<blockquote><p>Flume now supports a special directory called plugins.d which automatically picks up plugins that are packaged in a specific format.</p></blockquote>


<p>e.g plugins.d/websocket/lib/yoursource.jar</p>

<p>From now on you can use it:<br/>
agent.sources = websocket<br/>
agent.sources.websocket.type = com.sequenceiq.samples.flume.source.JettyWebSocketSource<br/>
agent.sources.websocket.host = localhost<br/>
agent.sources.websocket.port = 60000<br/>
agent.sources.websocket.path = /flume</p>

<p>Test it directly from your browser:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">var</span> <span class="nx">ws</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">WebSocket</span><span class="p">(</span><span class="s2">&quot;ws://127.0.0.1:60000/flume&quot;</span><span class="p">)</span>
</span><span class='line'><span class="nx">ws</span><span class="p">.</span><span class="nx">send</span><span class="p">(</span><span class="s2">&quot;Some message&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>That&rsquo;s it. Hope you enjoyed. We will be back soon with some ETL processing examples.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Set up HDP2 on Amazon EC2]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/02/07/hdp2-on-amazon/"/>
    <updated>2014-02-07T16:17:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/02/07/hdp2-on-amazon</id>
    <content type="html"><![CDATA[<p>During the last years we have seen many blog entries and articles about how to set up Hadoop on Amazon EC2. All these tutorials and articles had one thing in common &ndash; you had to go through a large number of manual (and painful) steps, read screenshots and redo the whole thing all over again, in case you needed a new cluster.</p>

<p>Since we use Amazon EC2 quite a lot, and Hadoop as well (Hortonworks distribution) we have gone through these steps many times &ndash; and have scripted the whole process from the first steps up to launching an N node Hadoop/HDP2 cluster in less then five minutes.</p>

<p>Moreover, the cluster is a &lsquo;production ready&rsquo; setup from infrastructural point of view &ndash; it is provisioned in a logically isolated section of the cloud (Virtual Private Cloud), with his own IP address range, creation of subnets, and configuration of route tables and network gateways.</p>

<p>Once the instances are provisoned, the HDP2 setup is done by Apache Ambari &ndash; for more advanced users we will provide the setup thorugh Ambari&rsquo;s RESTful API &ndash; watch this space or our GitHub page.</p>

<p>All the EC2 instances are tagged with the user name &ndash; thus you can create different clusters for different employees, all under the same AWS account (with IAM).</p>

<p>We believe that this is the right way to provision Hadoop in the cloud &ndash; during development and testing we had to provision Hadoop clusters of different sizes, and going through these steps manually would take lots of time.
This way we are able to provision clusters in the cloud in the matter of minutes &ndash; independently of the size.</p>

<p>The script is available at: <a href="https://github.com/sequenceiq/hadoop-cloud-scripts">https://github.com/sequenceiq/hadoop-cloud-scripts</a></p>

<p>Enjoy,
SequenceIQ</p>
]]></content>
  </entry>
  
</feed>
