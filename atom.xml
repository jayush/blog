<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-09-09T12:53:42+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Apache Ambari 1.7.0 early access]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/05/apache-ambari-1-7-0-ea/"/>
    <updated>2014-09-05T16:37:37+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/05/apache-ambari-1-7-0-ea</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we use <a href="http://ambari.apache.org/">Apache Ambari</a> every day &ndash; it’s our tool to provision Hadoop clusters.</p>

<p>Beside that we are contributors to Ambari, we are so excited about the coming Apache Ambari 1.7.0 new <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=30755705">features</a> that we could not help and put together an <strong>early access</strong> <a href="https://github.com/sequenceiq/docker-ambari/tree/1.7.0-ea">Ambari 1.7.0 Docker container</a>.</p>

<p>Give it a try, and provision an arbitrary number of Hadoop cluster on your laptop (or production environment), using our container and Ambari shell. Let us know how it works for you. Enjoy.</p>

<h3>Get the Docker container</h3>

<p>In case you don’t have Docker browse among our previous posts &ndash; we have a few posts about howto’s, examples and best practices in general for Docker and in particular about how to run the full Hadoop stack on Docker.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/ambari:1.7.0-ea</span></code></pre></td></tr></table></div></figure>




<!--more-->


<p>Once you have the container you are almost ready to go &ndash; we always automate everything and <strong>over simplify</strong> Hadoop provisioning.</p>

<h3>Get ambari-functions</h3>

<p>Get the following <code>ambari-functions</code> <a href="https://github.com/sequenceiq/docker-ambari/blob/1.7.0-ea/ambari-functions">file</a> from our GitHub.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Lo .amb j.mp/docker-ambari-170ea && . .amb</span></code></pre></td></tr></table></div></figure>


<h3>Create your cluster</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>amb-deploy-cluster 4</span></code></pre></td></tr></table></div></figure>


<p><strong>Whaaat?</strong> No really, that’s it &ndash; we have just provisioned you a 4 node Hadoop cluster in less than 2 minutes. Docker, Apache Ambari and Ambari Shell combined is quite powerful, isn&rsquo;t it? You can always start playing with your desired services by changing the <a href="https://github.com/sequenceiq/ambari-rest-client/tree/master/src/main/resources/blueprints">blueprints</a> &ndash; the full Hadoop stack is supported.</p>

<p>If you’d like to play around and understand how this works check our previous blog posts &ndash; a good start is this first post about one of our contribution, the <a href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/">Ambari Shell</a>.</p>

<p>You have just seen how easy is to provision a Hadoop cluster on your laptop, if you’d like to see how we provision a Hadoop cluster in the cloud using the very same Docker image you can check our open source, cloud agnostic Hadoop as a Service API &ndash; <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a>. Last week we have released a project called <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">Periscope</a> &ndash; the industry&rsquo;s first open source autoscaling API for Hadoop.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL on HBase with Apache Phoenix]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/04/sql-on-hbase-with-apache-phoenix/"/>
    <updated>2014-09-04T12:24:11+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/04/sql-on-hbase-with-apache-phoenix</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we use HBase to store large amounts of high velocity data and interact with them &ndash; many times we use native HBase interfaces but recently there was a need (internal and external) to access the data through an SQL interface.</p>

<h2>Introduction</h2>

<p>HBase is an open-source, distributed, versioned, non-relational database modeled after Google&rsquo;s Bigtable. It&rsquo;s designed to handle
billions of rows and millions of columns. However, using it as a relational database where you would store your data normalized,
split into multiple tables is not easy and most likely you will struggle with it as you would do in any other non-relational database.
Here comes <a href="http://phoenix.apache.org/">Apache Phoenix</a> in the picture. It&rsquo;s an SQL skin over HBase delivered as a
client-embedded JDBC driver targeting low latency queries. The project is in incubating state and under heavy development, but you
can already start embracing it.</p>

<h2>Installation</h2>

<p>Download the appropriate distribution from <a href="http://xenia.sote.hu/ftp/mirrors/www.apache.org/phoenix/">here</a>:</p>

<ul>
<li>Phoenix 3.x &ndash; HBase 0.94.x</li>
<li>Phoenix 4.x &ndash; HBase 0.98.1+</li>
</ul>


<p><em>Note the compatibilities between the HBase and Phoenix versions</em></p>

<p>Alternatively you can clone the <a href="https://github.com/apache/phoenix/tree/4.0">repository</a> and build it yourself (mvn clean install -DskipTests).
It should produce a jar file like this: phoenix-<code>version</code>-client.jar. Copy it to HBase&rsquo;s classpath (easiest way is to copy into
HBASE_HOME/lib). If you have multiple nodes it has to be there on every node. Restart the RegionServers and you are good to go. That&rsquo;s it?
Yes!</p>

<h2>Sample</h2>

<p>We&rsquo;ve pre-cooked a <a href="https://github.com/sequenceiq/phoenix-docker">Docker</a> image for you so you can follow this sample and play with it (the image is based on Hadoop 2.5, HBase 0.98.5, Phoenix 4.1.0):</p>

<h3>Normal launch</h3>

<p><code>docker run -it sequenceiq/phoenix:v4.1onHbase-0.98.5</code></p>

<h3>Alternative launch with sqlline</h3>

<p><code>docker run -it sequenceiq/phoenix:v4.1onHbase-0.98.5 /etc/bootstrap-phoenix.sh -sqlline</code></p>

<!-- more -->


<h3>Create tables</h3>

<p>The downloaded or built distribution&rsquo;s bin directory contains a pure-Java console based utility called sqlline.py. You can use this
to connect to HBase via the Phoenix JDBC driver. You need to specify the Zookeeper&rsquo;s QuorumPeer&rsquo;s address. If the default (2181) port is
used then type <em>sqlline.py localhost</em> (to quit type: !quit). Let&rsquo;s create two different tables:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='mysql'><span class='line'><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="nf">CUSTOMERS</span> <span class="p">(</span><span class="n">ID</span> <span class="kt">INTEGER</span> <span class="k">NOT</span> <span class="no">NULL</span> <span class="k">PRIMARY</span> <span class="k">KEY</span><span class="p">,</span> <span class="n">NAME</span> <span class="kt">VARCHAR</span><span class="p">(</span><span class="mi">40</span><span class="p">)</span> <span class="k">NOT</span> <span class="no">NULL</span><span class="p">,</span> <span class="n">AGE</span> <span class="kt">INTEGER</span> <span class="k">NOT</span> <span class="no">NULL</span><span class="p">,</span> <span class="n">CITY</span> <span class="kt">CHAR</span><span class="p">(</span><span class="mi">25</span><span class="p">));</span>
</span><span class='line'><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="nf">ORDERS</span> <span class="p">(</span><span class="n">ID</span> <span class="kt">INTEGER</span> <span class="k">NOT</span> <span class="no">NULL</span> <span class="k">PRIMARY</span> <span class="k">KEY</span><span class="p">,</span> <span class="kt">DATE</span> <span class="kt">DATE</span><span class="p">,</span> <span class="n">CUSTOMER_ID</span> <span class="kt">INTEGER</span><span class="p">,</span> <span class="n">AMOUNT</span> <span class="kt">DOUBLE</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>It&rsquo;s worth checking which <a href="http://phoenix.apache.org/language/datatypes.html">datatypes</a> and
<a href="http://phoenix.apache.org/language/functions.html">functions</a> are currently supported. These tables will be translated into
HBase tables and the metadata is stored along with it and versioned, such that snapshot queries over prior versions will automatically
use the correct schema. You can check with HBase shell as <code>describe 'CUSTOMERS'</code></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='mysql'><span class='line'><span class="n">DESCRIPTION</span>                                                                                                                         <span class="n">ENABLED</span>
</span><span class='line'> <span class="s1">&#39;CUSTOMERS&#39;</span><span class="p">,</span> <span class="err">{</span><span class="n">TABLE_ATTRIBUTES</span> <span class="o">=&gt;</span> <span class="err">{</span><span class="n">coprocessor</span><span class="err">$</span><span class="mi">1</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apache.phoenix.coprocessor.ScanRegionObserver|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">2</span> <span class="o">=&gt;</span> <span class="s1">&#39;|or true</span>
</span><span class='line'><span class="s1"> g.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">3</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apache.phoenix.coprocessor.GroupedAggreg</span>
</span><span class='line'><span class="s1"> ateRegionObserver|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">4</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apache.phoenix.coprocessor.ServerCachingEndpointImpl|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">5</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apa</span>
</span><span class='line'><span class="s1"> che.phoenix.hbase.index.Indexer|1073741823|index.builder=org.apache.phoenix.index.PhoenixIndexBuilder,org.apache.hadoop.hbase.inde</span>
</span><span class='line'><span class="s1"> x.codec.class=org.apache.phoenix.index.PhoenixIndexCodec&#39;</span><span class="err">}</span><span class="p">,</span> <span class="err">{</span><span class="n">NAME</span> <span class="o">=&gt;</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">DATA_BLOCK_ENCODING</span> <span class="o">=&gt;</span> <span class="s1">&#39;FAST_DIFF&#39;</span><span class="p">,</span> <span class="n">BLOOMFILTER</span> <span class="o">=&gt;</span> <span class="s1">&#39;ROW&#39;</span>
</span><span class='line'> <span class="p">,</span> <span class="n">REPLICATION_SCOPE</span> <span class="o">=&gt;</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">VERSIONS</span> <span class="o">=&gt;</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="n">COMPRESSION</span> <span class="o">=&gt;</span> <span class="s1">&#39;NONE&#39;</span><span class="p">,</span> <span class="n">MIN_VERSIONS</span> <span class="o">=&gt;</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">TTL</span> <span class="o">=&gt;</span> <span class="s1">&#39;2147483647&#39;</span><span class="p">,</span> <span class="n">KEEP_DELETED_CELLS</span> <span class="o">=</span>
</span><span class='line'> <span class="o">&gt;</span> <span class="s1">&#39;true&#39;</span><span class="p">,</span> <span class="n">BLOCKSIZE</span> <span class="o">=&gt;</span> <span class="s1">&#39;65536&#39;</span><span class="p">,</span> <span class="n">IN_MEMORY</span> <span class="o">=&gt;</span> <span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="n">BLOCKCACHE</span> <span class="o">=&gt;</span> <span class="s1">&#39;true&#39;</span><span class="err">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see there are bunch of co-processors. Co-processors were introduced in version 0.92.0 to push arbitrary computation out
to the HBase nodes and run in parallel across all the RegionServers. There are two types of them: <code>observers</code> and <code>endpoints</code>.
Observers allow the cluster to behave differently during normal client operations. Endpoints allow you to extend the cluster’s
capabilities, exposing new operations to client applications. Phoenix uses them to translate the SQL queries to scans and that&rsquo;s
why it can operate so quickly. It is also possible to map an existing HBase table to a Phoenix table. In this case the binary
representation of the row key and key values must match one of the Phoenix data types.</p>

<h3>Load data</h3>

<p>After the tables are created fill them with data. For this purpose we&rsquo;ll use the <a href="http://www.jooq.org/">Jooq</a> library&rsquo;s fluent API.
The related sample project (Spring based) can be found in our
<a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/phoenix-jooq">GitHub</a> repository. To connect you&rsquo;ll need Phoenix&rsquo;s
JDBC driver on your classpath (org.apache.phoenix.jdbc.PhoenixDriver). The url to connect to should be familiar as it uses the same Zookeeper QuorumPeer&rsquo;s address:
<code>jdbc:phoenix:localhost:2181</code>. Unfortunately Jooq&rsquo;s insert statement is not suitable for us since the JDBC driver only supports the
upsert statement so we cannot make use of the fluent API here.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">String</span> <span class="n">userSql</span> <span class="o">=</span> <span class="n">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;upsert into customers values (%d, &#39;%s&#39;, %d, &#39;%s&#39;)&quot;</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">i</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">escapeSql</span><span class="o">(</span><span class="n">names</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="n">names</span><span class="o">.</span><span class="na">size</span><span class="o">()</span> <span class="o">-</span> <span class="mi">1</span><span class="o">))),</span>
</span><span class='line'>                    <span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="mi">40</span><span class="o">)</span> <span class="o">+</span> <span class="mi">18</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">escapeSql</span><span class="o">(</span><span class="n">locales</span><span class="o">[</span><span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="n">locales</span><span class="o">.</span><span class="na">length</span> <span class="o">-</span> <span class="mi">1</span><span class="o">)].</span><span class="na">getDisplayCountry</span><span class="o">()));</span>
</span><span class='line'><span class="n">String</span> <span class="n">orderSql</span> <span class="o">=</span> <span class="n">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;upsert into orders values (%d, CURRENT_DATE(), %d, %d)&quot;</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">i</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">i</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="mi">1</span><span class="n">_000_000</span><span class="o">));</span>
</span><span class='line'><span class="n">dslContext</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="n">userSql</span><span class="o">);</span>
</span><span class='line'><span class="n">dslContext</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="n">orderSql</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Query</h3>

<p>On the generated data let&rsquo;s create queries:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">dslContext</span>
</span><span class='line'>          <span class="o">.</span><span class="na">select</span><span class="o">()</span>
</span><span class='line'>          <span class="o">.</span><span class="na">from</span><span class="o">(</span><span class="n">tableByName</span><span class="o">(</span><span class="s">&quot;customers&quot;</span><span class="o">).</span><span class="na">as</span><span class="o">(</span><span class="s">&quot;c&quot;</span><span class="o">))</span>
</span><span class='line'>          <span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">tableByName</span><span class="o">(</span><span class="s">&quot;orders&quot;</span><span class="o">).</span><span class="na">as</span><span class="o">(</span><span class="s">&quot;o&quot;</span><span class="o">)).</span><span class="na">on</span><span class="o">(</span><span class="s">&quot;o.customer_id = c.id&quot;</span><span class="o">)</span>
</span><span class='line'>          <span class="o">.</span><span class="na">where</span><span class="o">(</span><span class="n">fieldByName</span><span class="o">(</span><span class="s">&quot;o.amount&quot;</span><span class="o">).</span><span class="na">lessThan</span><span class="o">(</span><span class="n">amount</span><span class="o">))</span>
</span><span class='line'>          <span class="o">.</span><span class="na">orderBy</span><span class="o">(</span><span class="n">fieldByName</span><span class="o">(</span><span class="s">&quot;c.name&quot;</span><span class="o">).</span><span class="na">asc</span><span class="o">())</span>
</span><span class='line'>          <span class="o">.</span><span class="na">fetch</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>This query resulted the following:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">+----+------------+-----+-----------+----+----------+-------------+--------+</span>
</span><span class='line'><span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">ID</span><span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">NAME</span>      <span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">AGE</span><span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">CITY</span>     <span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">ID</span><span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">DATE</span>    <span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">CUSTOMER_ID</span><span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">AMOUNT</span><span class="o">|</span>
</span><span class='line'><span class="o">+----+------------+-----+-----------+----+----------+-------------+--------+</span>
</span><span class='line'><span class="o">|</span> <span class="mi">976</span><span class="o">|</span><span class="n">Bogan</span><span class="o">,</span> <span class="n">Elias</span><span class="o">|</span>   <span class="mi">26</span><span class="o">|</span><span class="n">Japan</span>      <span class="o">|</span> <span class="mi">976</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">976</span><span class="o">|</span>  <span class="mf">8664.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">827</span><span class="o">|</span><span class="n">Constrictor</span> <span class="o">|</span>   <span class="mi">29</span><span class="o">|{</span><span class="kc">null</span><span class="o">}</span>     <span class="o">|</span> <span class="mi">827</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">827</span><span class="o">|</span>  <span class="mf">7856.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">672</span><span class="o">|</span><span class="n">Hardwire</span>    <span class="o">|</span>   <span class="mi">31</span><span class="o">|</span><span class="n">Tunisia</span>    <span class="o">|</span> <span class="mi">672</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">672</span><span class="o">|</span>  <span class="mf">9292.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">746</span><span class="o">|</span><span class="n">Lady</span> <span class="n">Killer</span> <span class="o">|</span>   <span class="mi">37</span><span class="o">|</span><span class="n">Cyprus</span>     <span class="o">|</span> <span class="mi">746</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">746</span><span class="o">|</span>  <span class="mf">1784.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">242</span><span class="o">|</span><span class="n">Lifeforce</span>   <span class="o">|</span>   <span class="mi">35</span><span class="o">|</span><span class="n">Switzerland</span><span class="o">|</span> <span class="mi">242</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">242</span><span class="o">|</span>  <span class="mf">5406.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">487</span><span class="o">|</span><span class="n">Topspin</span>     <span class="o">|</span>   <span class="mi">48</span><span class="o">|{</span><span class="kc">null</span><span class="o">}</span>     <span class="o">|</span> <span class="mi">487</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">487</span><span class="o">|</span>  <span class="mf">6512.0</span><span class="o">|</span>
</span><span class='line'><span class="o">+----+------------+-----+-----------+----+----------+-------------+--------+</span>
</span></code></pre></td></tr></table></div></figure>


<p>The same thing could&rsquo;ve been achieved with sqlline also.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='mysql'><span class='line'><span class="k">select</span> <span class="n">c</span><span class="p">.</span><span class="n">name</span> <span class="k">as</span> <span class="n">name</span><span class="p">,</span> <span class="n">o</span><span class="p">.</span><span class="n">amount</span> <span class="k">as</span> <span class="n">amount</span><span class="p">,</span> <span class="n">o</span><span class="p">.</span><span class="kt">date</span> <span class="k">as</span> <span class="kt">date</span> <span class="k">from</span> <span class="n">customers</span> <span class="k">as</span> <span class="n">c</span> <span class="k">inner</span> <span class="k">join</span> <span class="n">orders</span> <span class="k">as</span> <span class="n">o</span> <span class="k">on</span> <span class="n">o</span><span class="p">.</span><span class="n">id</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="n">id</span> <span class="k">where</span> <span class="n">o</span><span class="p">.</span><span class="n">amount</span> <span class="o">&lt;</span> <span class="mi">10000</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Nested queries are not supported yet, but it will come soon.</p>

<h2>Summary</h2>

<p>As you saw it is pretty easy to get started with Phoenix both command line and programmatically. There are lots of lacking features, but
the contributors are dedicated and working hard to make this project moving forward. Next step? ORM for HBase? It is also ongoing.. :)</p>

<p>Follow up with <a href="https://www.linkedin.com/company/sequenceiq/">us</a> if you are interested in HBase and building an SQL interface on top.
Don&rsquo;t hesitate to contact us should you have any questions.</p>

<p><a href="http://sequenceiq.com/">SequenceIQ</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SLA policies for autoscaling Hadoop clusters]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/01/sla-samples-periscope/"/>
    <updated>2014-09-01T16:37:37+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/01/sla-samples-periscope</id>
    <content type="html"><![CDATA[<p>Last week we have <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">announced</a> and open sourced <a href="http://sequenceiq.com/periscope/">Periscope</a> &ndash; the industry’s first SLA policy based autoscaling API for Hadoop YARN clusters. In this post we’d like to come up with some examples, setting up alarms and attach scaling policies to your Hadoop cluster.</p>

<p>Periscope is built on existing (and coming/contributed by us) features provided by Apache Hadoop, YARN, Ambari, Docker containers and SequenceIQ’s <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>. Just FYI, <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> is our open source and cloud agnostic Hadoop as a Service API, built on Docker containers. While Periscope can attach scaling policies to <code>static</code> and <code>dynamic</code> clusters &ndash; in this post we’d like to emphasize Periscope’s capabilities while working with &mdash; `dynamic &ndash; cloud based Hadoop deployments  &ndash; such as Hadoop clusters deployed with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>.</p>

<p>SLAs policies are configured based on <code>alarms</code>, whereas an alarm is created based on <code>metrics</code> &ndash; these entities are explained below.</p>

<h2>Alarms</h2>

<p>An alarm watches a <code>metric</code> over a specified time period, and used by one or more action or scaling policy based on the value of the metric relative to a given threshold over the time period. A few of the supported <code>metrics</code> are listed below:</p>

<p>*<code>PENDING_CONTAINERS</code>&ndash; pending YARN containers</p>

<p>*<code>PENDING_APPLICATIONS</code> &ndash; pending/queued YARN applications</p>

<p>*<code>LOST_NODES</code> &ndash; cluster nodes lost</p>

<p>*<code>UNHEALTHY_NODES</code> &ndash; unhealthy cluster nodes</p>

<p>*<code>GLOBAL_RESOURCES</code> &ndash; global resources</p>

<!--more-->


<p>Measured <code>metrics</code> are compared with pre-configured values using operators. The <code>comparison operators</code> are: <code>LESS_THAN</code>, <code>GREATER_THAN</code>, <code>LESS_OR_EQUAL_THAN</code>, <code>GREATER_OR_EQUAL_THAN</code>, <code>EQUALS</code>.
In order to avoid reacting for sudden spikes in the system and apply policies only in case of a sustained system stress, <code>alarms</code> have to be sustained over a <code>period</code> of time.  The <code>period</code> specifies the time period in minutes during the alarm has to be sustained. Also a <code>threshold</code> can be configured, which specifies the variance applied by the operator for the selected <code>metric</code>.</p>

<p>For the <code>alarm</code> related REST operations you can check the <a href="http://docs.periscope.apiary.io/reference/alarms">API</a> documentation. Alarms can issue <code>notifications</code> as well &ndash; for example if a metric is reached for the configured time and threshold a notification event is raised &ndash; in the given example below this notification is an email.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># set metric alarms
</span><span class='line'>curl -X POST -H "Content-Type: application/json" -d '{"alarms":[{"alarmName":"pendingContainerHigh","description":"Number of pending containers is high","metric":"PENDING_CONTAINERS","threshold":10,"comparisonOperator":"GREATER_THAN","period":1},{"alarmName":"freeGlobalResourcesRateLow","description":"Low free global resource rate","metric":"GLOBAL_RESOURCES","threshold":1,"comparisonOperator":"EQUALS","period":1,"notifications":[{"target":[“mick.fanning@aspworldtour.com"],"notificationType":"EMAIL"}]}]}' localhost:8081/clusters/1/alarms | jq .
</span><span class='line'>curl -X PUT -H "Content-Type: application/json" -d '{"alarmName":"unhealthyNodesHigh","description":"Number of unhealthy nodes is high","metric":"UNHEALTHY_NODES","threshold":5,"comparisonOperator":"GREATER_OR_EQUAL_THAN","period":5}' localhost:8081/clusters/1/alarms | jq .</span></code></pre></td></tr></table></div></figure>


<h2>SLA scaling policies</h2>

<p>Scaling is the ability to increase or decrease the capacity of the Hadoop cluster or application.  When scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined.
Periscope will do the heavy lifting and based on the alarms and the scaling policy linked to them it executes the associated policy. By default a fully configured and running <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> cluster contains no SLA policies.  An SLA scaling policy can contain multiple <code>alarms</code>.</p>

<p>As an alarm is triggered a <code>scalingAdjustment</code> is applied, however to keep the cluster size within boundaries a <code>minSize</code> and <code>maxSize</code> is attached to the cluster &ndash; thus a scaling policy can never over or undersize a cluster. Also in order to avoid stressing the cluster we have introduced a <code>cooldown</code> period (minutes) &ndash; though an alarm is raised and there is an associated scaling policy, the system will not apply the policy within the configured timeframe. In an SLA scaling policy the triggered policies are applied in order.</p>

<p>Hosts can be added or removed from specific <code>hostgroups</code>. Periscope and Cloudbreak uses Apache Ambari to provision a Hadoop cluster. Ambari host groups are a set of machines with the same Hadoop “components” installed. You can set up a cluster having different hostgroups &ndash; and run different services, thus having a heterogenous cluster.</p>

<p>In the following example we downscale a cluster when the unused resources are high.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># set scaling policy
</span><span class='line'>curl -X POST -H "Content-Type: application/json" -d '{"minSize":2,"maxSize":10,"cooldown":30,"scalingPolicies":[{"name":"downScaleWhenHighResource","adjustmentType":"NODE_COUNT","scalingAdjustment":2,"hostGroup":"slave_1","alarmId":"101"},{"name":"upScaleWhenHighPendingContainers","adjustmentType":"PERCENTAGE","scalingAdjustment":40,"hostGroup":"slave_1","alarmId":"100"}]}' localhost:8081/clusters/1/policies | jq .</span></code></pre></td></tr></table></div></figure>


<p>For the <code>policy</code> related REST operations you can check the <a href="http://docs.periscope.apiary.io/reference/scaling-policy">API</a> documentation.</p>

<p>Let us know how Periscope works for you &ndash; and for updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Infrastructure management with CloudFormation]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/29/aws-cloudformation-makes-everything-easier/"/>
    <updated>2014-08-29T14:13:06+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/29/aws-cloudformation-makes-everything-easier</id>
    <content type="html"><![CDATA[<p>Our Hadoop as a Service solution, <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> integrates with multiple cloud providers to deploy Hadoop clusters in the cloud. It means that every time a cluster is requested, Cloudbreak goes to the selected cloud provider and creates a new, separated infrastructure through the provider’s API. Building this infrastructure can be a real pain and can cause a lot of problems &ndash; it involves a lot of API calls, the polling of created building blocks, the management of failures and the necessary rollbacks to name a few. With the help of <a href="http://aws.amazon.com/cloudformation/">AWS CloudFormation</a> we were able to avoid most of these problems when integrating AWS in Cloudbreak.</p>

<h3>Problems with the traditional approach</h3>

<p>When Cloudbreak creates a Hadoop cluster it should first create the underlying infrastructure on the cloud provider. The building blocks are a bit different on every provider, the following resources are created on AWS:</p>

<ul>
<li>a virtual private cloud (VPC)</li>
<li>a subnet</li>
<li>an internet gateway</li>
<li>a route table</li>
<li>an auto scaling group and its launch configuration</li>
<li>a security group</li>
</ul>


<p>Although AWS has a pretty good API and great SDKs to communicate with it, we needed to deal with the above described problems if we would like to create all of these elements one by one through the Java SDK. The code would start with something like this with the creation of the VPC:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">AmazonEC2Client</span> <span class="n">amazonEC2Client</span> <span class="o">=</span> <span class="k">new</span> <span class="n">AmazonEC2Client</span><span class="o">(</span><span class="n">basicSessionCredentials</span><span class="o">);</span>
</span><span class='line'><span class="n">amazonEC2Client</span><span class="o">.</span><span class="na">setRegion</span><span class="o">(</span><span class="n">region</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'><span class="n">CreateVpcRequest</span> <span class="n">vpcRequest</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CreateVpcRequest</span><span class="o">().</span><span class="na">withCidrBlock</span><span class="o">(</span><span class="mf">10.0</span><span class="o">.</span><span class="mf">0.0</span><span class="o">/</span><span class="mi">24</span><span class="o">);</span>
</span><span class='line'><span class="n">CreateVpcResponse</span> <span class="n">vpcResponse</span> <span class="o">=</span> <span class="n">amazonEC2Client</span><span class="o">.</span><span class="na">createVpc</span><span class="o">(</span><span class="n">createVpcRequest</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'><span class="c1">//poll vpc creation until it’s state is available</span>
</span><span class='line'><span class="n">waitForVPC</span><span class="o">(</span><span class="n">amazonEC2Client</span><span class="o">,</span> <span class="n">vpcResponse</span><span class="o">.</span><span class="na">getVpc</span><span class="o">());</span>
</span><span class='line'>
</span><span class='line'><span class="n">ModifyVpcAttributeRequest</span> <span class="n">modifyVpcRequest</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ModifyVpcAttributeRequest</span><span class="o">().</span><span class="na">withEnableDnsHostnames</span><span class="o">(</span><span class="kc">true</span><span class="o">).</span><span class="na">withEnableDnsSupport</span><span class="o">(</span><span class="kc">true</span><span class="o">);</span>
</span><span class='line'><span class="n">amazonEC2Cient</span><span class="o">.</span><span class="na">modifyVpcAttribute</span><span class="o">(</span><span class="n">modifyVpcRequest</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>




<!--more-->


<p>The above code is only a taste of the whole thing. The VPC is one of the most simple resources with very few attributes to set. Also the polling of the creation process isn’t detailed here as well as failure handling. In addition the different resources would be scattered around the code making it impossible to have an overview of the whole stack and making it much harder to find bugs or to modify some attributes. With CloudFormation all of the above problems can be solved very easily.</p>

<h3>Introduction to CloudFormation</h3>

<p>According to the <a href="http://aws.amazon.com/cloudformation/">AWS CloudFormation documentation</a> it was designed to create and manage a collection of related AWS resources easily and provisioning and updating them in an orderly and predictable fashion. What it really means is that the resources can be described declaratively in a JSON document (a <em>template</em>) and the whole <em>stack</em> can be created/updated/deleted with a simple API call. AWS also handles failures, and rollbacks the whole stack if something goes wrong. Furthermore it is able to send notifications to <em>SNS topics</em> when some event occurs (e.g.: a resource creation started or the resource is completed), making the polling of resource creations unnecessary.</p>

<h3>Template structure</h3>

<p>We don’t want to give a detailed introduction on how the structure of a CloudFormation template look like, the <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html">AWS documentation</a> covers it really well and there are also a lot of <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/sample-templates-services-us-west-2.html">samples</a>.
Instead we’re trying to focus on the advantages that CloudFormation gave us while using it, so let’s jump in the middle and start with a simple example. The declaration of a VPC in a template looks like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="s2">&quot;Resources&quot;</span> <span class="err">:</span> <span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;MY_VPC&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;Type&quot;</span> <span class="p">:</span> <span class="s2">&quot;AWS::EC2::VPC&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;Properties&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;CidrBlock&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;10.0.0.0/16&quot;</span> <span class="p">},</span>
</span><span class='line'>      <span class="nt">&quot;EnableDnsSupport&quot;</span> <span class="p">:</span> <span class="s2">&quot;true&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;EnableDnsHostnames&quot;</span> <span class="p">:</span> <span class="s2">&quot;true&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;Tags&quot;</span> <span class="p">:</span> <span class="p">[</span>
</span><span class='line'>        <span class="p">{</span> <span class="nt">&quot;Key&quot;</span> <span class="p">:</span> <span class="s2">&quot;Application&quot;</span><span class="p">,</span> <span class="nt">&quot;Value&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;Ref&quot;</span> <span class="p">:</span> <span class="s2">&quot;AWS::StackId&quot;</span> <span class="p">}</span> <span class="p">},</span>
</span><span class='line'>        <span class="p">{</span> <span class="nt">&quot;Key&quot;</span> <span class="p">:</span> <span class="s2">&quot;Network&quot;</span><span class="p">,</span> <span class="nt">&quot;Value&quot;</span> <span class="p">:</span> <span class="s2">&quot;Public&quot;</span> <span class="p">}</span>
</span><span class='line'>      <span class="p">]</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The JSON syntax can be a bit complicated sometimes, especially when dealing with a lot of references to other properties with the <em>&ldquo;Ref&rdquo;</em> keyword or some other built-in CloudFormation <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html">functions</a>, but it is much clearer than the Java code above.
Other than resources, there are other parts of a CloudFormation template (<em>Conditions</em>, <em>Mappings</em>, <em>Outputs</em>, <em>Intrinsic Functions</em>) but here we will cover only one more: <em>Parameters</em>.</p>

<p>Declaring a parameter means that there is no hard-coded value for a given attribute, rather it is given to the template when creating the stack. If you’d like to have an EC2 Instance  in your template but you don’t want to hardcode its type, you can have a parameter like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="s2">&quot;Parameters&quot;</span> <span class="err">:</span> <span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;InstanceType&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;Description&quot;</span> <span class="p">:</span> <span class="s2">&quot;EC2 instance type&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;Type&quot;</span> <span class="p">:</span> <span class="s2">&quot;String&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;Default&quot;</span> <span class="p">:</span> <span class="s2">&quot;m3.medium&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;AllowedValues&quot;</span> <span class="p">:</span> <span class="p">[</span> <span class="s2">&quot;m3.medium&quot;</span><span class="p">,</span><span class="s2">&quot;m3.large&quot;</span><span class="p">,</span><span class="s2">&quot;m3.xlarge&quot;</span><span class="p">,</span><span class="s2">&quot;m3.2xlarge&quot;</span><span class="p">],</span>
</span><span class='line'>    <span class="nt">&quot;ConstraintDescription&quot;</span> <span class="p">:</span> <span class="s2">&quot;must be a valid EC2 instance type.&quot;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>After it’s declared, you can reference it from a resource with the <em>Ref</em> keyword:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="s2">&quot;EC2Instance&quot;</span> <span class="err">:</span> <span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;Type&quot;</span> <span class="p">:</span> <span class="s2">&quot;AWS::EC2::Instance&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;Properties&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;SecurityGroups&quot;</span> <span class="p">:</span> <span class="p">[</span> <span class="p">{</span> <span class="nt">&quot;Ref&quot;</span> <span class="p">:</span> <span class="s2">&quot;InstanceSecurityGroup&quot;</span> <span class="p">}</span> <span class="p">],</span>
</span><span class='line'>    <span class="nt">&quot;InstanceType&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;Ref&quot;</span> <span class="p">:</span> <span class="s2">&quot;InstanceType&quot;</span> <span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;KeyName&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;Ref&quot;</span> <span class="p">:</span> <span class="s2">&quot;KeyName&quot;</span> <span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;ImageId&quot;</span> <span class="p">:</span> <span class="s2">&quot;ami-123456&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;EbsOptimized&quot;</span> <span class="p">:</span> <span class="s2">&quot;true&quot;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can reference not only parameters, but other resources as well. In the above code example there is a reference to <em>InstanceSecurityGroup</em> that is an <em>AWS::EC2::SecurityGroup</em> type resource and that is declared in an other part of the template.</p>

<h3>Creating the stack</h3>

<p>So we’ve declared a few resources, how can we tell AWS to create the stack? Let’s see how it looks like with the Java SDK (two parameters are passed to the template):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">CreateStackRequest</span> <span class="n">createStackRequest</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CreateStackRequest</span><span class="o">()</span>
</span><span class='line'>    <span class="o">.</span><span class="na">withStackName</span><span class="o">(</span><span class="err">“</span><span class="n">MyCFStack</span><span class="s">&quot;)</span>
</span><span class='line'><span class="s">    .withTemplateBody(templateAsString)</span>
</span><span class='line'><span class="s">    .withNotificationARNs(notificationTopicARN)</span>
</span><span class='line'><span class="s">    .withParameters(</span>
</span><span class='line'><span class="s">        new Parameter().withParameterKey(&quot;</span><span class="n">InstanceCount</span><span class="s">&quot;).withParameterValue(“3&quot;</span><span class="o">),</span>
</span><span class='line'>        <span class="k">new</span> <span class="nf">Parameter</span><span class="o">().</span><span class="na">withParameterKey</span><span class="o">(</span><span class="s">&quot;InstanceType&quot;</span><span class="o">).</span><span class="na">withParameterValue</span><span class="o">(</span><span class="err">“</span><span class="n">m3</span><span class="o">.</span><span class="na">large</span><span class="err">&quot;</span><span class="o">));</span>
</span><span class='line'>
</span><span class='line'><span class="n">client</span><span class="o">.</span><span class="na">createStack</span><span class="o">(</span><span class="n">createStackRequest</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>And that’s it. It’s every code that should written in Java to create the complete stack. It is pretty straightforward, the only thing that needs to be explained is the <em>notification ARN</em> part. It is the identifier of an <em>SNS topic</em> and it is detailed below.</p>

<h3>Callbacks</h3>

<p>CloudFormation is able to send notifications to SNS <em>topics</em> when an event occurs. An event is when a resource creation is started, finished or failed (and the same with delete). SNS is Amazon’s Simple Notification Service that enables endpoints to subscribe to a topic, and when a message is sent to a topic every subscriber receives that message. AWS supports a lot of endpoint types. It can send notifications by email or text message, to Amazon Simple Queue Service (SQS) queues, or to any HTTP/HTTPS endpoint. In the Cloudbreak project we’re using HTTP endpoints as callback URLs. We’re also creating topics and subscriptions from code but that could fill up another full blog post.</p>

<p>If you just like to try SNS, you can create a topic and a subscription from the AWS Console. After you have a confirmed subscription of an HTTP endpoint (e.g.: <em>example.com/sns</em>), you can very easily create an HTTP endpoint in Java (with some help from <a href="http://spring.io/">Spring</a>):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@RequestMapping</span><span class="o">(</span><span class="n">value</span><span class="o">=</span><span class="s">&quot;sns&quot;</span><span class="o">,</span> <span class="n">method</span> <span class="o">=</span> <span class="n">RequestMethod</span><span class="o">.</span><span class="na">POST</span><span class="o">)</span>
</span><span class='line'><span class="nd">@ResponseBody</span>
</span><span class='line'><span class="kd">public</span> <span class="n">ResponseEntity</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="nf">receiveSnsMessage</span><span class="o">(</span><span class="nd">@RequestBody</span> <span class="n">String</span> <span class="n">request</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// parse and handle request</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>For a more detailed example see the <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/java/com/sequenceiq/cloudbreak/controller/AmazonSnsController.java">controller class</a> in Cloudbreak.
So every time a CloudFormation stack event occurs, Cloudbreak receives a message that is parsed and handled correctly &ndash; there is no need to poll the creation of resources and dealing with timeouts.</p>

<h3>Failures and rollbacks</h3>

<p>It is always possible that something will go wrong when creating a stack with a lot of resources. With the traditional approach you must keep track of the resources that were created and you will have to implement some rollback logic that gets called when something unexpected happens and that rolls back the already created elements somehow. With CloudFormation these things are completely done by AWS.</p>

<p>The resources in the stack are tracked so the only thing you have to save is the identifier of the stack. If one of the resources fails to be created AWS rolls back every other resource and puts the stack in <em>ROLLBACK_COMPLETED</em> state. It also sends the failure message to the SNS topic with the exact cause of the failure.
The same is true if you’d like to delete the stack. The only call that you will have to send to the AWS API is the deletion of the stack (very similar to the creation in Java). CloudFormation will delete every resource one by one and will take care of failures.</p>

<h3>Notes</h3>

<p>The template we used in Cloudbreak is available <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/resources/templates/aws-cf-stack.ftl">here</a>. It is not a pure CloudFormation template because of some limitations &ndash; the number of attached volumes cannot be specified dynamically and it is not possible to specify it as a parameter if spot priced instances are needed or not &ndash; we ended up generating the template with Freemarker.</p>

<h3>Terraform</h3>

<p>The <a href="http://www.hashicorp.com/products">company</a> that brought us <a href="http://www.vagrantup.com/">Vagrant</a>, <a href="http://www.packer.io/">Packer</a> and a few more useful things has recently announced a new project named <a href="http://www.terraform.io/intro/index.html">Terraform</a>. Terraform is inspired by tools like CloudFormation or <a href="https://wiki.openstack.org/wiki/Heat">OpenStack’s Heat</a>, but goes further as it supports multiple cloud platforms and their services can also be combined. If you’re interested in managing infrastructure from code and configuration you should check out that project too, we’ll keep an eye on it for sure.</p>

<p>Stay tuned and make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Periscope - autoscaling for Hadoop YARN]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/"/>
    <updated>2014-08-27T16:37:37+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope</id>
    <content type="html"><![CDATA[<p><em>Periscope is a powerful, fast, thick and top-to-bottom right-hander, eastward from Sumbawa&rsquo;s famous west-coast. Timing is critical, as needs a number of elements to align before it shows its true colors.</em></p>

<p><em>Periscope brings QoS and autoscaling to Hadoop YARN. Built on cloud resource management and YARN schedulers, allows to associate SLA policies to applications.</em></p>

<p>After the very positive reception of <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> &ndash; the first open source and cloud agnostic Hadoop as a Service API &ndash; today we are releasing the <code>public beta</code> version of our open source <strong>SLA policy based autoscaling API</strong> for Hadoop YARN clusters.</p>

<h2>Overview</h2>

<p>The purpose of Periscope is to bring QoS and autoscaling to a multi-tenant Hadoop YARN cluster, while allowing to apply SLA policies to individual applications.
At <a href="http://sequenceiq.com">SequenceIQ</a> working with multi-tenant Hadoop clusters for quite a while, we have always seen the same frustration and fight for resource between users.
The <strong>FairScheduler</strong> was partially solving this problem &ndash; bringing in fairness based on the notion of <a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Ghodsi.pdf">Dominant Resource Fairness</a>.
With the emergence of Hadoop 2 YARN and the <strong>CapacityScheduler</strong> we had the option to maximize throughput and utilization for a multi-tenant cluster in an operator-friendly manner.
The scheduler works around the concept of queues. These queues are typically setup by administrators to reflect the economics of the shared cluster.
While this is a pretty good abstraction and brings some level of SLA for predictable workloads, it often needs proper design ahead.
The queue hierarchy and resource allocation needs to be changed when new tenants and workloads are moved to the cluster.</p>

<p>Periscope was designed around the idea of <code>autoscaling</code> clusters &ndash; without any need to preconfigure queues, cluster nodes or apply capacity planning ahead.</p>

<!--more-->


<h2>How it works</h2>

<p>Periscope monitors the application progress, the number of YARN containers/resources and their allocation, queue depths, the number of available cluster nodes and their health.
Since we have switched to YARN a while ago (been among the first adopters) we have run an open source <a href="https://github.com/sequenceiq/yarn-monitoring">monitoring project</a>, based on R.
We have been collecting metrics from the YARN Timeline server, Hadoop Metrics2 and Ambari&rsquo;s Nagios/Ganglia &ndash; and profiling the applications and correlating with these metrics.
One of the key findings was that while low level metrics are good to understand the cluster health &ndash; they might not necessarily help on making decisions when applying different SLA policies on a multi-tenant cluster.
Focusing on higher level building blocks as queue depth, YARN containers, etc actually brings in the same quality of service, while not being lost in low level details.</p>

<p>Periscope works with two types of Hadoop clusters: <code>static</code> and <code>dynamic</code>. Periscope does not require any pre-installation &ndash; the only thing it requires is to be <code>attached</code> to an Ambari server&rsquo;s REST API.</p>

<h2>Clusters</h2>

<h3>Static clusters</h3>

<p>From Periscope point of view we consider a cluster <code>static</code> when the cluster capacity can&rsquo;t be increased horizontally.
This means that the hardware resources are already given &ndash; and the throughput can&rsquo;t be increased by adding new nodes.
Periscope introspects the job submission process, monitors the applications and applies the following SLAs:</p>

<ol>
<li> Application ordering &ndash; can guarantee that a higher priority application finishes before another one (supporting parallel or sequential execution)</li>
<li> Moves running applications between priority queues</li>
<li> <em>Attempts</em> to enforce time based SLA (execution time, finish by, finish between, recurring)</li>
<li> <em>Attempts</em> to enforce guaranteed cluster capacity requests ( x % of the resources)</li>
<li> Support for distributed (but not YARN ready) applications using Apache Slider</li>
<li> Attach priorities to SLAs</li>
</ol>


<p><em>Note: not all of the features above are supported in the first <code>public beta</code> version. There are dependencies we contributed to Hadoop, YARN and Ambari and they will be included in the next releases (2.6 and 1.7)</em></p>

<h3>Autoscaling clusters</h3>

<p>From Periscope point of view we consider a cluster <code>dynamic</code> when the cluster capacity can be increased horizontally.
This means that nodes can be added or removed on the fly &ndash; thus the cluster’s throughput can be increased or decreased based on the cluster load and scheduled applications.
Periscope works with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> to add or remove nodes from the cluster based on the SLA policies and thus continuously provide a high <em>quality of service</em> for the multi-tenand Hadoop cluster.
Just to refresh memories &ndash; <a href="http://sequenceiq.com/products.html">Cloudbreak</a> is <a href="http://sequenceiq.com">SequenceIQ&rsquo;s</a> open source, cloud agnostic Hadoop as a Service API.
Given the option of provisioning or decommissioning cluster nodes on the fly, Periscope allows you to use the following set of SLAs:</p>

<ol>
<li> Application ordering &ndash; can guarantee that a higher priority application finishes before another one (supporting parallel or sequential execution)</li>
<li> Moves running applications between priority queues</li>
<li> <em>Enforce</em> time based SLA (execution time, finish by, finish between, recurring) by increasing cluster capacity and throughput</li>
<li> Smart decommissioning &ndash; avoids HDFS storms, keeps <code>paid</code> nodes alive till the last minute</li>
<li> <em>Enforce</em> guaranteed cluster capacity requests ( x % of the resources)</li>
<li> <em>Private</em> cluster requests &ndash; supports provisioning of short lived private clusters with the possibility to merge them.</li>
<li> Support for distributed (but not YARN ready) applications using Apache Slider</li>
<li> Attach priorities to SLAs</li>
</ol>


<p><em>Note: not all of the features above are supported in the first <code>public beta</code> version. There are dependencies we contributed to Hadoop, YARN and Ambari and they will be included in the next releases (2.6 and 1.7)</em></p>

<h3>High level technical details</h3>

<p>When we have started to work on Periscope we checked different solutions &ndash; and we quickly realized that there are no any open source solutions available.
Apache YARN in general, and the scheduler API&rsquo;s in particular have solved few of the issues we had &ndash; and they have certainly bring some level of SLA to Hadoop.
At <a href="https://sequenceiq.com">SequenceIQ</a> we run all our different applications on YARN &ndash; and when we decided to create a heuristic scheduler we knew from very beginning that it has to be built on the functionality given by YARN.
In order to create Periscope we had to contribute code to YARN, Hadoop and Ambari &ndash; and were trying to add all the low level features directly into the YARN codebase.
Periscope has a <a href="http://docs.periscope.apiary.io/">REST API</a> and supports pluggable SLA policies.
We will follow up with technical details in coming blog posts, so make sure you subscribe to on of our social channels.</p>

<h3>Resources</h3>

<p>Get the code : <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></p>

<p>Documentation: <a href="http://sequenceiq.com/periscope">http://sequenceiq.com/periscope</a></p>

<p>API documentation: <a href="http://docs.periscope.apiary.io/">http://docs.periscope.apiary.io/</a></p>

<h3>What&rsquo;s next, etc</h3>

<p>This is the first <code>public beta</code> release of Periscope made available on our <a href="https://github.com/sequenceiq/periscope">GitHub</a> page.
While we are already using this internally we would like the community to help us battle test it, let us know if you find issues or raise feature requests. We are always happy to help.</p>

<p>Further releases will bring tighter integration with Ambari (especially around cluster resources), an enhanced (or potentially new) YARN scheduler and a Machine learning based job classification model.</p>

<p>We would like to say a big <em>thank you</em> for the YARN team &ndash; this effort would have not been possible without their contribution. Also we would like to thank them by supporting us with our contributions as well.
At SequenceIQ we are 100% committed to open source &ndash; and releasing Periscope under an <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache 2 licence</a> was never a question.</p>

<p>Stay tuned and make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>

<p>Enjoy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Submit a Spark job to YARN from code]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java/"/>
    <updated>2014-08-22T08:09:28+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java</id>
    <content type="html"><![CDATA[<p>In our previous Apache Spark related post we showed you how to write a simple machine learning job. In this post we’d like to show you how to submit a Spark job from code. At SequenceIQ we submit jobs to different clusters &ndash; based on load, customer profile, associated SLAs, etc. Doing this the <code>documented</code> way was cumbersome so we needed a way to submit Spark jobs (and in general all of our jobs running in a YARN cluster) from code. Also due to the <code>dynamic</code> clusters, and changing job configurations we can’t use hardcoded parameters &ndash; in a previous <a href="http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service/">blog post</a> we highlighted how are we doing all these.</p>

<h2>Business as usual</h2>

<p>Basically as you from the <a href="https://spark.apache.org/docs/1.0.1/submitting-applications.html">Spark documentation</a>, you have to use the <a href="https://github.com/apache/spark/blob/master/bin/spark-submit">spark-submit</a> script to submit a job. In nutshell SparkSubmit is called
by the <a href="https://github.com/apache/spark/blob/master/bin/spark-class">spark-class</a> script with a lots of decorated arguments. In our example we examine only the YARN part of the submissions.
As you can see in <a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala">SparkSubmit.scala</a> the YARN <a href="https://github.com/apache/spark/blob/master/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala">Client</a> is loaded and its main method invoked (based on the arguments of the script).</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// If we&#39;re deploying into YARN, use yarn.Client as a wrapper around the user class</span>
</span><span class='line'><span class="k">if</span> <span class="o">(!</span><span class="n">deployOnCluster</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">childMainClass</span> <span class="k">=</span> <span class="n">args</span><span class="o">.</span><span class="n">mainClass</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">isUserJar</span><span class="o">(</span><span class="n">args</span><span class="o">.</span><span class="n">primaryResource</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">childClasspath</span> <span class="o">+=</span> <span class="n">args</span><span class="o">.</span><span class="n">primaryResource</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span> <span class="k">else</span> <span class="k">if</span> <span class="o">(</span><span class="n">clusterManager</span> <span class="o">==</span> <span class="nc">YARN</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">childMainClass</span> <span class="k">=</span> <span class="s">&quot;org.apache.spark.deploy.yarn.Client&quot;</span>
</span><span class='line'>  <span class="n">childArgs</span> <span class="o">+=</span> <span class="o">(</span><span class="s">&quot;--jar&quot;</span><span class="o">,</span> <span class="n">args</span><span class="o">.</span><span class="n">primaryResource</span><span class="o">)</span>
</span><span class='line'>  <span class="n">childArgs</span> <span class="o">+=</span> <span class="o">(</span><span class="s">&quot;--class&quot;</span><span class="o">,</span> <span class="n">args</span><span class="o">.</span><span class="n">mainClass</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">...</span>
</span><span class='line'><span class="c1">// Here we invoke the main method of the Client</span>
</span><span class='line'><span class="k">val</span> <span class="n">mainClass</span> <span class="k">=</span> <span class="nc">Class</span><span class="o">.</span><span class="n">forName</span><span class="o">(</span><span class="n">childMainClass</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="n">loader</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">mainMethod</span> <span class="k">=</span> <span class="n">mainClass</span><span class="o">.</span><span class="n">getMethod</span><span class="o">(</span><span class="s">&quot;main&quot;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="mi">0</span><span class="o">).</span><span class="n">getClass</span><span class="o">)</span>
</span><span class='line'><span class="k">try</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">mainMethod</span><span class="o">.</span><span class="n">invoke</span><span class="o">(</span><span class="kc">null</span><span class="o">,</span> <span class="n">childArgs</span><span class="o">.</span><span class="n">toArray</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span> <span class="k">catch</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">case</span> <span class="n">e</span><span class="k">:</span> <span class="kt">InvocationTargetException</span> <span class="o">=&gt;</span> <span class="n">e</span><span class="o">.</span><span class="n">getCause</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">case</span> <span class="n">cause</span><span class="k">:</span> <span class="kt">Throwable</span> <span class="o">=&gt;</span> <span class="k">throw</span> <span class="n">cause</span>
</span><span class='line'>    <span class="k">case</span> <span class="kc">null</span> <span class="k">=&gt;</span> <span class="k">throw</span> <span class="n">e</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>It’s a pretty straightforward way to submit a Spark job to a YARN cluster, though you will need to change manually the parameters which as passed as arguments.</p>

<!--more-->


<h2>Submitting the job from Java code</h2>

<p>In case if you would like to submit a job to YARN from Java code, you can just simply use this Client class directly in your application.
(but you have to make sure that every environment variable what you will need is set properly).</p>

<h3>Passing Configuration object</h3>

<p>In the main method the org.apache.hadoop.conf.Configuration object is not passed to the Client class. A <code>Configuration</code> is created explicitly in the constructor, which is actually okay (then client configurations are loaded from $HADOOP_CONF_DIR/core-site.xml and $HADOOP_CONF_DIR/yarn-site.xml).
But what if you want to use (for example) an <a href="http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service/">Ambari Configuration Service</a> for retrieve your configuration, instead of using hardcoded ones?</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="o">...</span> <span class="c1">// Client class - constructor</span>
</span><span class='line'>  <span class="k">def</span> <span class="k">this</span><span class="o">(</span><span class="n">clientArgs</span><span class="k">:</span> <span class="kt">ClientArguments</span><span class="o">,</span> <span class="n">spConf</span><span class="k">:</span> <span class="kt">SparkConf</span><span class="o">)</span> <span class="k">=</span>
</span><span class='line'>    <span class="k">this</span><span class="o">(</span><span class="n">clientArgs</span><span class="o">,</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">(),</span> <span class="n">spConf</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="o">...</span> <span class="c1">// Client object - main method</span>
</span><span class='line'><span class="nc">System</span><span class="o">.</span><span class="n">setProperty</span><span class="o">(</span><span class="s">&quot;SPARK_YARN_MODE&quot;</span><span class="o">,</span> <span class="s">&quot;true&quot;</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">sparkConf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">()</span>
</span><span class='line'>
</span><span class='line'><span class="k">try</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">args</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ClientArguments</span><span class="o">(</span><span class="n">argStrings</span><span class="o">,</span> <span class="n">sparkConf</span><span class="o">)</span>
</span><span class='line'>  <span class="k">new</span> <span class="nc">Client</span><span class="o">(</span><span class="n">args</span><span class="o">,</span> <span class="n">sparkConf</span><span class="o">).</span><span class="n">run</span><span class="o">()</span>
</span><span class='line'><span class="o">}</span> <span class="k">catch</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">case</span> <span class="n">e</span><span class="k">:</span> <span class="kt">Exception</span> <span class="o">=&gt;</span> <span class="o">{</span>
</span><span class='line'>    <span class="nc">Console</span><span class="o">.</span><span class="n">err</span><span class="o">.</span><span class="n">println</span><span class="o">(</span><span class="n">e</span><span class="o">.</span><span class="n">getMessage</span><span class="o">)</span>
</span><span class='line'>    <span class="nc">System</span><span class="o">.</span><span class="n">exit</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="nc">System</span><span class="o">.</span><span class="n">exit</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Fortunately, the configuration can be passed here (there is a <code>Configuration</code> field in the Client), but you have to write your own main method.</p>

<h3>Code example</h3>

<p>In our example we also use the 2 client XMLs as configuration (for demonstration purposes only), the main difference here is that we read the properties from the XMLs and filling them in the Configuration. Then we pass the Configuration object to the Client (which is directly invoked here).</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'> <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">config</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">()</span>
</span><span class='line'>    <span class="n">fillProperties</span><span class="o">(</span><span class="n">config</span><span class="o">,</span> <span class="n">getPropXmlAsMap</span><span class="o">(</span><span class="s">&quot;config/core-site.xml&quot;</span><span class="o">))</span>
</span><span class='line'>    <span class="n">fillProperties</span><span class="o">(</span><span class="n">config</span><span class="o">,</span> <span class="n">getPropXmlAsMap</span><span class="o">(</span><span class="s">&quot;config/yarn-site.xml&quot;</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'>    <span class="nc">System</span><span class="o">.</span><span class="n">setProperty</span><span class="o">(</span><span class="s">&quot;SPARK_YARN_MODE&quot;</span><span class="o">,</span> <span class="s">&quot;true&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">sparkConf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">()</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">cArgs</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ClientArguments</span><span class="o">(</span><span class="n">args</span><span class="o">,</span> <span class="n">sparkConf</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">new</span> <span class="nc">Client</span><span class="o">(</span><span class="n">cArgs</span><span class="o">,</span> <span class="n">config</span><span class="o">,</span> <span class="n">sparkConf</span><span class="o">).</span><span class="n">run</span><span class="o">()</span>
</span><span class='line'>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To build the project use this command from the spark-submit directory:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./gradlew clean build
</span></code></pre></td></tr></table></div></figure>


<p>After building it you find the required jars in spark-submit-runner/build/libs (<code>uberjar</code> with all required dependencies) and spark-submit-app/build/libs. Put them in the same directory (do this also with this <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/spark-submit/spark-submit-runner/src/main/resources">config folder</a> too). After that run this command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>java -cp spark-submit-runner-1.0.jar com.sequenceuq.spark.SparkRunner <span class="se">\</span>
</span><span class='line'>  --jar spark-submit-app-1.0.jar <span class="se">\</span>
</span><span class='line'>  --class com.sequenceiq.spark.Main <span class="se">\</span>
</span><span class='line'>  --driver-memory 1g <span class="se">\</span>
</span><span class='line'>  --executor-memory 1g <span class="se">\</span>
</span><span class='line'>  --executor-cores 1 <span class="se">\</span>
</span><span class='line'>  --arg hdfs://sandbox:9000/input/sample.txt <span class="se">\</span>
</span><span class='line'>  --arg /output <span class="se">\</span>
</span><span class='line'>  --arg 10 <span class="se">\</span>
</span><span class='line'>  --arg 10
</span></code></pre></td></tr></table></div></figure>


<p>During the submission note that: not just the app jar, but the spark-submit-runner jar is also uploaded (which is an <code>uberjar</code>) to the HDFS. To avoid this, you have to upload it to the HDFS manually and set the <strong>SPARK_JAR</strong> environment variable.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">export </span><span class="nv">SPARK_JAR</span><span class="o">=</span><span class="s2">&quot;hdfs:///spark/spark-submit-runner-1.0.jar&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>If you get &ldquo;Permission denied&rdquo; exception on submit, you should set the <strong>HADOOP_USER_NAME</strong> environment variable to root (or something with proper rights).</p>

<p>As usual for us we ship the code &ndash; you can get it from our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/spark-submit">GitHub</a> samples repository; the sample input is available <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/spark-clustering/data/input.txt">here</a>.</p>

<p>If you would like to play with Spark, you can use our <a href="https://registry.hub.docker.com/u/sequenceiq/spark/">Spark Docker container</a> available as a trusted build on Docker.io repository.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Hadoop 2.5.0 on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker/"/>
    <updated>2014-08-18T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/18/hadoop-2-5-0-docker</id>
    <content type="html"><![CDATA[<p>A few weeks ago we have released an Apache Hadoop 2.3 Docker image &ndash; in a very short time this become the most <a href="https://registry.hub.docker.com/search?q=hadoop">popular</a> Hadoop image in the Docker <a href="https://registry.hub.docker.com/">registry</a>.</p>

<p>Following on the success of our Hadoop 2.3.0, 2.4.0 and 2.4.1 Docker <a href="https://registry.hub.docker.com/u/sequenceiq/hadoop-docker/">image</a>, the feedbacks and requests we have received and aligning with the Hadoop release cycle, we have released an Apache Hadoop 2.5.0 Docker image &ndash; same as the previous version this is available as a trusted and automated build on the official Docker <a href="https://registry.hub.docker.com/">registry</a>.</p>

<p>Please note that beside this Hadoop image, we have released and maintain a <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/">pseudo-distributed</a> and <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">fully distributed</a> Hadoop Docker image provisioned with Apache Ambari. As they are provisioned with Ambari you have the option to change, add and remove Hadoop components using cluster blueprints.</p>

<p>Also we are happy to let you know that this release of Apache Hadoop contains a few of SequenceIQ&rsquo;s open source <strong>contributions</strong> and <a href="https://issues.apache.org/jira/browse/YARN-2250">fixes</a> around YARN schedulers.
We are working on an SLA enforcer for Hadoop &ndash; very soon to be open sourced &ndash; and part of that work we are contributing back to the community. Also there is a major contribution of ours coming in the next release of Hadoop &ndash; 2.6.0.
Stay tuned and follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>

<h2>Build the image</h2>

<p>In case you&rsquo;d like to try directly from the <a href="https://github.com/sequenceiq/hadoop-docker">Dockerfile</a> you can build the image as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build  -t sequenceiq/hadoop-docker .</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<h2>Pull the image</h2>

<p>As it is also released as an official Docker image from Docker&rsquo;s automated build repository &ndash; you can always pull or refer the image when launching containers.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/hadoop-docker:2.5.0</span></code></pre></td></tr></table></div></figure>


<h2>Start a container</h2>

<p>In order to use the Docker image you have just build or pulled use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t sequenceiq/hadoop-docker:2.5.0 /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<h2>Testing</h2>

<p>You can run one of the stock examples:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd $HADOOP_PREFIX
</span><span class='line'># run the mapreduce
</span><span class='line'>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0.jar grep input output 'dfs[a-z.]+'
</span><span class='line'>
</span><span class='line'># check the output
</span><span class='line'>bin/hdfs dfs -cat output/*</span></code></pre></td></tr></table></div></figure>


<h2>Hadoop native libraries, build, Bintray, etc</h2>

<p>The Hadoop build process is no easy task &ndash; requires lots of libraries and their right version, protobuf, etc and takes some time &ndash; we have simplified all these, made the build and released a 64b version of Hadoop nativelibs on our <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.4.1/view/files">Bintray repo</a>. Enjoy.</p>

<p>Should you have any questions let us know through our social channels as <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fair play]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/16/fairplay/"/>
    <updated>2014-08-16T14:45:15+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/16/fairplay</id>
    <content type="html"><![CDATA[<p>Recently we’ve been asked an interesting question &ndash; how fair is the YARN <a href="http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/FairScheduler.html">FairScheduler</a> &ndash; while we never use internally the fair scheduler after a quick test the short answer is &ndash; <strong>very fair</strong>.</p>

<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we always use the <a href="http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">CapacityScheduler</a> &ndash; actually an enhanced version of it (coming with the 2.6.0 release of Hadoop). Since the emergence of YARN and the new schedulers we were working on a solution to bring SLA to Hadoop &ndash; and part of this work was our contribution to <a href="https://issues.apache.org/jira/browse/YARN-1495">Apache YARN schedulers</a> and <a href="http://ambari.apache.org/">Apache Ambari</a>. Anyway, we decided to configure a FairScheduler in one of our 20 node test cluster and run a quick test.</p>

<h3>Fair scheduler</h3>

<p>Remember than before YARN only one resource represented a resource on a cluster &ndash; the <code>slot</code>. Every node had slots, and your MR job was taking up slots , regardless of their actual resource usage (CPU, memory). It worked but for sure it wasn’t a fair game &ndash; and caused lots of frustration between administrators of applications competing for <code>slots</code>. We have seen many over and undersubscribed nodes in terms of CPU and memory. YARN introduced the concept of containers and the ability to request/attach resources to them (vCores and memory).</p>

<p>While this seams already a big step forward comparing with slots, it brought up other problems &ndash; with multiple resources as <code>vCores</code> and <code>memory</code> and <code>disk</code> and <code>network i/o</code> in the future it’s pretty challenging to share them fairly. With a single resource it would we pretty straightforward &ndash; nevertheless the community based on a <a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Ghodsi.pdf">research paper</a> coming out from UC Berkeley (Ghodsi et al) managed to get this working through (again a community effort) this <a href="https://issues.apache.org/jira/browse/YARN-326">YARN ticket</a>.</p>

<p>Now let’s battle test how fair is the scheduler when running two MR application with changing resource usage &ndash; how well the dominant resource fairness works.</p>

<!--more-->


<h3>The test</h3>

<p>We decided to take a pretty easy MR job with 64 input files. In order to bring in some  variables, the input files are a multiple of 4MB, distributed as the smallest is 4MB and the largest is 256MB. The used <code>block size</code> is 256MB, and the number of nodes in the cluster is <strong>20</strong>. We are using and open sourced an <strong>R based</strong> <a href="https://github.com/sequenceiq/yarn-monitoring">YARN monitoring</a> project &ndash; feel free to use it and let us know if you have any feedback.</p>

<p>We were running two jobs &ndash; and the task&rsquo;s input was descending e.g. <em>task_1398345200850_0079_m_000001</em> has a 252MB input file and <em>task_1398345200850_0079_m_000063</em> has a 4MB input. Obliviously the tasks were not necessarily executed in this order, because the order depends on when the nodemanager asks for task.</p>

<p>See the <code>timeboxed</code> result of the two runs.</p>

<p><strong>Run 61</strong></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/run61.png" alt="" /></p>

<p><strong>Run 62</strong></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/run62.png" alt="" /></p>

<p>While the <code>timeboxed</code> version will not really help to decide the resource usage and the elapsed time (which should be pretty much equal) it’s good to show the time spent on different nodes. Many times generating these charts helped us to identify hardware or other software/configuration issues on different nodes (for example when a run execution is outside of the standard deviation). You can use our R project and file to generate charts as such with the help of <a href="https://github.com/sequenceiq/yarn-monitoring/blob/master/RProjects/TimeBoxes.R">TimeBoxes.R</a> file.</p>

<p>Now if we compare the two execution files and place it on the same chart we will actually see that the FairScheduler is <strong>fairly Fair</strong>.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/test8_active_mapppers_num.png" alt="" /></p>

<h3>Random ideas</h3>

<p>While the purpose of these tests was to show that the fair scheduler distributes resources in a fair way &ndash; sorry I can’t help &ndash; we can see that the executions of the map tasks are not optimal, but at least stable. Also we can notice that the execution order depends also on the blocks locations; if you should know/consider the blocks location ahead the execution could be more optimal.</p>

<p>Measured a few other things as well &ndash; will discuss this on a different post &ndash; and from those charts you can see that the elapsed time of a task grow even as there are free slots.  Also as the number of mappers come closer to the available free slots of the cluster the average elapsed times of the tasks grow &ndash; due to different reasons (which we will share on a forthcoming post).</p>

<p>Since we are not really using the <strong>FairScheduler</strong> and we had one now configured we decided to run a few of our performance tests as well, and while submitting jobs like <code>crazy</code> using the fair scheduler we managed to <code>logjam</code> the cluster.
We have never seen this before while using the <strong>CapacityScheduler</strong> &ndash; and digging into details we figured that the FairScheduler is missing the <code>yarn.scheduler.capacity.maximum-am-resource-percent</code> property. This <a href="https://issues.apache.org/jira/browse/YARN-1913">issue</a> appears to be a bug in the FairScheduler &ndash; fixed in the 2.5 release.</p>

<p>While we don’t want to make any comparison between the two schedulers I think that the FairScheduler is a very viable and good option for those having a cluster and doesn’t want to bother with <strong>capacity planning ahead</strong>. Also I was impressed by the fine grain rules which you can use with the FairScheduler while deciding on the resource allocations.</p>

<p>Note that we are working and open sourcing a project which brings SLA to Hadoop and allows auto-scaling using <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> &ndash; our open source, cloud agnostic Hadoop as a Service API. The project is called <strong>Periscope</strong> and will be open sourced very soon.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker intercontainer networking explained]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/12/docker-networking/"/>
    <updated>2014-08-12T08:53:15+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/12/docker-networking</id>
    <content type="html"><![CDATA[<p>The purpose of this blog entry is to cover advanced topics regarding Docker networking and explain different concepts to inter-connect Docker containers when the containers are running on different host machines.
For the demonstration we are using VMs on <a href="https://www.virtualbox.org/">VirtualBox</a> launched with <a href="http://www.vagrantup.com/">Vagrant</a>, but the explained networking concepts work also on Amazon EC2 (with VPC) and Azure unless stated otherwise.</p>

<p>To set up the the test environment clone the <a href="https://github.com/sequenceiq/sequenceiq-samples">SequenceIQ&rsquo;s samples repository</a> and follow the instructions.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone git@github.com:sequenceiq/sequenceiq-samples.git
</span><span class='line'>cd sequenceiq-samples/docker-networking
</span><span class='line'>vagrant up</span></code></pre></td></tr></table></div></figure>


<p>The <code>vagrant up</code> command launches the test setup, which conatins two Ubuntu 14.04 VMs with the network configuration:</p>

<ul>
<li><a href="https://www.virtualbox.org/manual/ch06.html#network_nat">NAT</a></li>
<li><a href="https://docs.vagrantup.com/v2/networking/private_network.html">Private networking</a></li>
</ul>


<p>The NAT (related to eth0 interface on VMs) is used only for access the external network from VMs e.g. download files from debian repository, but it is not used for inter-container communication. The Vagrant sets up a properly configured Host Only Networking in VirtualBox therefore the VMs can communicate with each other on the defined IP addresses:</p>

<ul>
<li>vm1: 192.168.40.11</li>
<li>vm2: 192.168.40.12</li>
</ul>


<p>Let&rsquo;s see how Docker containers running on these VMs can send IP packets to each other.</p>

<!--more-->


<h2>Setting up bridge0</h2>

<p>The Docker attaches all containers to the virtual subnet implemented by docker0, this means that by default on both VMs the Docker containers will be launched with IP addresses from range 172.17.42.1/24. This is a problem for some of the solutions explained below, because if the containers on different hosts have the same IP addresses then we won&rsquo;t be able to properly route the IP packets between them. Therefore on each VMs a network bridge is created with the following subnets:</p>

<ul>
<li>vm1: 172.17.51.1/24</li>
<li>vm2: 172.17.52.1/24</li>
</ul>


<p>This means that every container luanched on vm1 will get an IP address from range 172.17.51.2 &ndash; 172.17.51.255 and containers on vm2 will have an address from range 172.17.52.2 &ndash; 172.17.52.255.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># do not execute, it was already executed on vm1 as root during provision from Vagrant</span>
</span><span class='line'>brctl addbr bridge0
</span><span class='line'>sudo ifconfig bridge0 172.17.51.1 netmask 255.255.255.0
</span><span class='line'>sudo bash -c <span class="s1">&#39;echo DOCKER_OPTS=\&quot;-b=bridge0\&quot; &gt;&gt; /etc/default/docker&#39;</span>
</span><span class='line'>sudo service docker restart
</span><span class='line'>
</span><span class='line'><span class="c"># do not execute, it was already executed on vm1 as root during provision from Vagrant</span>
</span><span class='line'>sudo brctl addbr bridge0
</span><span class='line'>sudo ifconfig bridge0 172.17.52.1 netmask 255.255.255.0
</span><span class='line'>sudo bash -c <span class="s1">&#39;echo DOCKER_OPTS=\&quot;-b=bridge0\&quot; &gt;&gt; /etc/default/docker&#39;</span>
</span><span class='line'>sudo service docker restart
</span></code></pre></td></tr></table></div></figure>


<p>As noted in the comments the above configuration is already executed during the provisioning of VMs and it was copied here just for the sake of clarity and completeness.</p>

<h2>Expose container ports to host</h2>

<p>Probably the simplest way to solve inter-container communication is to expose ports from container to the host. This can be done with the <code>-p</code> switch. E.g. exposing the port 3333 is as simple as:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># execute on vm1</span>
</span><span class='line'>sudo docker run -it --rm --name cont1 -p 3333:3333 ubuntu /bin/bash -c <span class="s2">&quot;nc -l 3333&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="c"># execute on vm2</span>
</span><span class='line'>sudo docker run -it --rm --name cont2 ubuntu /bin/bash -c <span class="s2">&quot;nc -w 1 -v 192.168.40.11 3333&quot;</span>
</span><span class='line'><span class="c">#Result: Connection to 192.168.40.11 3333 port [tcp/*] succeeded!</span>
</span></code></pre></td></tr></table></div></figure>


<p>This might be well suited for cases when the communication ports are defined in advance (e.g. MySQL will run on port 3306), but will not work when the application uses dynamic ports for communication (like Hadoop does with IPC ports).</p>

<h2>Host networking</h2>

<p>If the container is started with <code>--net=host</code> then it avoids placing the container inside of a separate network stack, but as the Docker documentation says this option &ldquo;tells Docker to not containerize the container&rsquo;s networking&rdquo;. The <code>cont1</code> container can bind directly to the network interface of host therefore the <code>nc</code> will be available directly on 192.168.40.11.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># execute on vm1</span>
</span><span class='line'>sudo docker run -it --rm --name cont1 --net<span class="o">=</span>host ubuntu /bin/bash -c <span class="s2">&quot;nc -l 3333&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="c"># execute on vm2</span>
</span><span class='line'>sudo docker run -it --rm --name cont2 ubuntu /bin/bash -c <span class="s2">&quot;nc -w 1 -v 192.168.40.11 3333&quot;</span>
</span><span class='line'><span class="c">#Result: Connection to 192.168.40.11 3333 port [tcp/*] succeeded!</span>
</span></code></pre></td></tr></table></div></figure>


<p>Of course if you want to access cont2 from cont1 then cont2 also needs to be started with <code>--net=host</code> option.
The host networking is very powerful solution for inter-container communication, but it has its drawbacks, since the ports used by the container can collide with the ports used by host or other containers utilising &mdash;net=host option, because all of them are sharing the same network stack.</p>

<h2>Direct Routing</h2>

<p>So far we have seen methods where the containers have used the IP address of host to communicate with each other, but there are solutions to inter-connect the containers by using their own IPs. If we are using the containers own IPs for routing then it is important that we shall be able to distinguish based on IP which container is running on vm1 and which one is running on on vm2, this was the reason why the bridge0 interface was created as explained in &ldquo;Setting up bridge0&rdquo; section.
To make the things a bit easier to understand I have created a simplified diagram of the network interfaces in our current test setup. If I would like to oversimplify the thing then I would say that, we shall setup the routing in that way that the packets from one container are following the red lines shown on the diagram.</p>

<p style="text-align:center;"> <img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/docker-networking/img/routing.png"></p>

<p>To achive this we need to configure the routing table on hosts in that way that every packet which destination is 172.17.51.0/24 is forwarded to vm1 and every IP packet where the destination is 172.17.52.0/24 is forwarded to vm2. To repeat it shortly, the containers running on vm1 are placed to subnet 172.17.51.0/24, containers on vm2 are on subnet 172.17.52.0/24.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># execute on vm1</span>
</span><span class='line'>sudo route add -net 172.17.52.0 netmask 255.255.255.0 gw 192.168.40.12
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.51.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont1  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont1)</span>
</span><span class='line'>nc -l 3333
</span><span class='line'>
</span><span class='line'><span class="c"># execute on vm2</span>
</span><span class='line'>sudo route add -net 172.17.51.0  netmask 255.255.255.0  gw 192.168.40.11
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.52.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont2  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont2)</span>
</span><span class='line'>nc -w 1 -v 172.17.51.2 3333
</span><span class='line'><span class="c">#Result: Connection to 172.17.51.2 3333 port [tcp/*] succeeded!</span>
</span></code></pre></td></tr></table></div></figure>


<p>The <code>route add</code> command adds the desired routing to the route table, but you might wonder why the iptables configuration is necessary. The reason for that the Docker by default sets up a rule to the nat table to masquerade all IP packets that are leaving the machine. In our case we definitely don&rsquo;t want this, therefore we delete all MASQUERADE rules with -F option. At this point we already would be able to make the connection from one container to other and vice verse, but the containers would not be able to communicate with the outside world, therefore an iptables rule needs to be added to masquerade the packets that are going outside of 172.17.0.0/16. I need to mention the another approach would be to use the <a href="https://docs.docker.com/articles/networking/#between-containers">&mdash;iptables=false</a> option of the daemon to avoid any manipulation in the iptables and you can do all the config manually.</p>

<p>Such kind of direct routing from one vm to other vm works great and easy to set up, but cannot be used if the hosts are not on the same subnet. If the host are located the on different subnet the tunneling might be an option as you will see it in the next section.</p>

<p><em>Note: This solution works on Amazon EC2 instances only if the <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_NAT_Instance.html#EIP_Disable_SrcDestCheck">Source/Destionation Check</a> is disabled.</em></p>

<p><em>Note: Due to the packet filtering policy of Azure this method cannot be used there.</em></p>

<h2>Generic Routing Encapsulation (GRE) tunnel</h2>

<p>GRE is a tunneling protocol that can encapsulate a wide variety of network layer protocols inside virtual point-to-point links.
The main idea is to create a GRE tunnel between the VMs and send all traffic through it:</p>

<p style="text-align:center;"> <img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/docker-networking/img/gre.png"></p>

<p>In order to create a tunnel you need to specify the name, the type (which is gre in our case) and the IP address of local and the remote end. Consequently the tun2 name used for the tunnel on on vm1 since from vm1 perspective that is the tunnel endpoint which leads to vm2 and every packet sent to tun2 to will eventually come out on vm2 end.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#GRE tunnel config execute on vm1</span>
</span><span class='line'>sudo iptunnel add tun2 mode gre <span class="nb">local </span>192.168.40.11 remote 192.168.40.12
</span><span class='line'>sudo ifconfig tun2 10.0.201.1
</span><span class='line'>sudo ifconfig tun2 up
</span><span class='line'>sudo route add -net 172.17.52.0 netmask 255.255.255.0 dev tun2
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.51.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont1  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont1)</span>
</span><span class='line'>nc -l 3333
</span><span class='line'>
</span><span class='line'><span class="c">#GRE tunnel config execute on vm2</span>
</span><span class='line'>sudo iptunnel add tun1 mode gre <span class="nb">local </span>192.168.40.12 remote 192.168.40.11
</span><span class='line'>sudo ifconfig tun1 10.0.202.1
</span><span class='line'>sudo ifconfig tun1 up
</span><span class='line'>sudo route add -net 172.17.51.0 netmask 255.255.255.0 dev tun1
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.52.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont2  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont2)</span>
</span><span class='line'>nc -w 1 -v 172.17.51.2 3333
</span><span class='line'><span class="c">#Result: Connection to 172.17.51.2 3333 port [tcp/*] succeeded!</span>
</span></code></pre></td></tr></table></div></figure>


<p>After the tunnel is set up and activated the remaining commands are very similar to the commands executed in the &ldquo;Direct Routing&rdquo; section. The main difference here is that we do not route the traffic directly to other vm, but we are routing it into <code>dev tun1</code> and <code>dev tun2</code> respectively.</p>

<p>With GRE tunnels a point-to-point connection is set up between two hosts, which means that if you have more then two hosts in your network and want to interconnect all of them, then n-1 tunnel endpoint needs to be created on every host, which will be quite challenging to maintain if you have a large cluster.</p>

<p><em>Note: GRE packets are <a href="http://msdn.microsoft.com/en-us/library/azure/dn133803.aspx">filtered out</a> on Azure therefore this solution cannot be used there.</em></p>

<h2>Virtual Private Network (VPN)</h2>

<p>If more secured connections is required between containers then VPNs can be used on VMs. This addiotional security might significantly increase processing overhead. This overhead is highly depends on which VPN solution are you going to use. In this demo we use the VPN capabilities of SSH which is not really suited for production use. In order to enable the VPN capabilites of ssh the  PermitTunnel parameter needs to be switched on in sshd_config. If you are using the Vagranfile provided to this tutorial then nothing needs to be done, since this parameter was already set up for you during provisioning in the bootstrap.sh.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#execute on vm1</span>
</span><span class='line'>sudo ssh -f -N -w 2:1 root@192.168.40.12
</span><span class='line'>sudo ifconfig tun2 up
</span><span class='line'>sudo route add -net 172.17.52.0 netmask 255.255.255.0 dev tun2
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.51.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont1  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont1)</span>
</span><span class='line'>nc -l 3333
</span><span class='line'>
</span><span class='line'><span class="c">#execute on vm2</span>
</span><span class='line'>sudo ifconfig tun1 up
</span><span class='line'>sudo route add -net 172.17.51.0 netmask 255.255.255.0 dev tun1
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.52.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont2  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont2)</span>
</span><span class='line'>nc -w 1 -v 172.17.51.2 3333
</span><span class='line'><span class="c">#Result: Connection to 172.17.51.2 3333 port [tcp/*] succeeded!</span>
</span></code></pre></td></tr></table></div></figure>


<p>The ssh is launched with -w option where the numerical ids of tun devices were specified. After executing the command the tunnel interfaces are created on both VMs. The interfaces needs to be be activated with ifconfig up and after that we need to setup the rooting to direct the traffic to  172.17.51.0/24 and 172.17.52.0/24 to tun2 and tun1.</p>

<p>As mentioned the VPN capabilities of SSH is not recommended in production, but other solutions like  <a href="https://openvpn.net/index.php/open-source.html">OpenVPN</a> would worth a try to secure the communication between the hosts (and also between the containers).</p>

<h2>Conclusion</h2>

<p>The above examples were hand written mainly for demonstration purposes, but there are great tools like <a href="https://github.com/jpetazzo/pipework">Pipework</a> that can make your life simpler and will do the heavy lifting for you.</p>

<p>If you want to check how these methods are working in production environment you are just a few clicks from it, since under the hood these methods are responsible to solve the inter-container communication in our cloud agnostic Hadoop as a Service API called <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create Hadoop clusters in the cloud using a CLI]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/07/clodubreak-shell/"/>
    <updated>2014-08-07T19:43:01+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/07/clodubreak-shell</id>
    <content type="html"><![CDATA[<p>Few weeks back we have announced <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the open source Hadoop as a Service API. Included in the release we open sourced a REST API, REST client, UI and a CLI/shell. In this post we’d like to show you how easy is to use <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak shell</a> in order to create on demand Hadoop clusters on your favorite cloud provider.</p>

<p>While it’s up to everybody&rsquo;s personal preference whether to use a UI, a command line interface or the REST API directly, at SequenceIQ we prefer to use command line tools whenever it’s possible because it’s much faster than interacting with a web UI and it’s a better candidate for automation. Yeah &ndash; we have mentioned this many times &ndash; we are <code>obsessed with automation</code>; any step which is a candidate of doing it twice we script and automate it.</p>

<p>This <code>thing</code> with the automation does not affect the effort and quality standards we put on building the UI &ndash; <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> has an extremely intuitive and clean <strong>responsive</strong> UI and it’s built on the latest and greatest web UI framework &ndash; <a href="https://angularjs.org/">Angular JS</a>. We will have a post about the UI, however we consider it so simple to use that we ask you to go ahead and give it a try. You are a signup and a few clicks away from your Hadoop cluster.</p>

<p>Now back to the CLI. Remember one of our Apache contribution &ndash; the <a href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/">Ambari shell and REST API</a>? Well, the Cloudbreak shell is built on the same technology &ndash; Spring Shell. It’s an interactive shell that can be easily extended using a Spring based programming model and battle tested in various projects like Spring Roo, Spring XD, and Spring REST Shell Combine these two projects to create a powerful tool.</p>

<h2>Cloudbreak Shell</h2>

<p>The goal with the CLI was to provide an interactive command line tool which supports:</p>

<ul>
<li>all functionality available through the REST API or Cloudbreak web UI</li>
<li>makes possible complete automation of management task via <strong>scripts</strong></li>
<li>context aware command availability</li>
<li>tab completion</li>
<li>required/optional parameter support</li>
<li><strong>hint</strong> command to guide you on the usual path</li>
</ul>


<h2>Install Cloudbreak Shell</h2>

<p>You have 3 options to give it a try:</p>

<ul>
<li>use our prepared <a href="https://registry.hub.docker.com/u/sequenceiq/cloudbreak/">docker image</a></li>
<li>download the latest self-containing executable jar form our maven repo</li>
<li>build it from source</li>
</ul>


<p>We will follow up with the first two, in this post we’d like to guide you through the third option.</p>

<h3>Build from source</h3>

<p>If want to use the code or extend it with new commands follow the steps below. You will need:
&ndash; jdk 1.7
&ndash; maven 3.x.x</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/sequenceiq/cloudbreak-shell.git
</span><span class='line'>cd cloudbreak-shell
</span><span class='line'>mvn clean package</span></code></pre></td></tr></table></div></figure>




<!--more-->


<h2>Connect to Cloudbreak</h2>

<p>In order to use the shell you will have to have a Cloudbreak account. You can get one by subscribing to our hosted and free <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> instance. Alternatively you can build your own Cloudbreak and deploy it within your organization &ndash; for that just follow up with the steps in the Cloudbreak <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">documentation</a>. We suggest to try our hosted solution as in case you have any issues we can always help you with. Please feel free to create bugs, ask for enhancements or just give us feedback by either using our <a href="https://github.com/sequenceiq/cloudbreak">GitHub repository</a> or the other channels highlighted in the product documentation (Google Groups, email or social channels).
The shell is built as a single executable jar with the help of <a href="http://projects.spring.io/spring-boot/">Spring Boot</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage:
</span><span class='line'>  java -jar cloudbreak-shell-0.1-SNAPSHOT.jar                  : Starts Cloudbreak Shell in interactive mode.
</span><span class='line'>  java -jar cloudbreak-shell-0.1-SNAPSHOT.jar --cmdfile=&lt;FILE&gt; : Cloudbreak executes commands read from the file.
</span><span class='line'>
</span><span class='line'>Options:
</span><span class='line'>  --cloudbreak.host=&lt;HOSTNAME&gt;       Hostname of the Cloudbreak REST API Server [use:cloudbreak-api.sequenceiq.com].
</span><span class='line'>  --cloudbreak.port=&lt;PORT&gt;           Port of the Cloudbreak REST API Server [use:80].
</span><span class='line'>  --cloudbreak.user=&lt;USER&gt;           Username of the Cloudbreak user [use:your user name ].
</span><span class='line'>  --cloudbreak.password=&lt;PASSWORD&gt;   Password of the Cloudbreak admin [use: your password].
</span><span class='line'>
</span><span class='line'>Note:
</span><span class='line'>  All options are mandatory.</span></code></pre></td></tr></table></div></figure>


<p>Once you are connected you can start to create a cluster. If you are lost and need guidance through the process you can use <code>hint</code>. You can always use <code>TAB</code> for completion. Note that all commands are <code>context aware</code> &ndash; they are available only when it makes sense &ndash; this way you are never confused and guided by the system on the right path.</p>

<h3>Create a cloud credential</h3>

<p>In order to start using Cloudbreak you will need to have a cloud user, for example an Amazon AWS account. Note that Cloudbreak <strong>does not</strong> store you cloud user details &ndash; we work around the concept of <a href="http://aws.amazon.com/iam/">IAM</a> &ndash; on Amazon (or other cloud providers) you will have to create an IAM role, a policy and associate that with your Cloudbreak account &ndash; for further documentation please refer to the <a href="http://sequenceiq.com/cloudbreak/#accounts">documentation</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>credential createEC2 --description “description" --name “myCredentialName" --roleArn "arn:aws:iam::NUMBER:role/cloudbreak-ABC" --sshKeyUrl “URL towards your AWS public key"</span></code></pre></td></tr></table></div></figure>


<p>Alternatively you can upload your public key from a file as well, by using the <code>—sshKeyPath</code> switch. You can check whether the credential was creates successfully by using the <code>credential list</code> command. You can switch between your cloud credential &ndash; when you’d like to use one and act with that you will have to use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>credential select --id #ID of the credential</span></code></pre></td></tr></table></div></figure>


<h3>Create a template</h3>

<p>A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>template createEC2 --description "awstemplate" --name "awstemplate" --region EU_WEST_1 --instanceType M3Large --sshLocation 0.0.0.0/0 </span></code></pre></td></tr></table></div></figure>


<p>You can check whether the template was created successfully by using the <code>template list</code> command. Check the template with or select if you are happy with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>template show --id #ID of the template
</span><span class='line'>
</span><span class='line'>template select --id #ID of the template</span></code></pre></td></tr></table></div></figure>


<h3>Create a stack</h3>

<p>Stacks are template <code>instances</code> &ndash; a running cloud infrastructure created based on a template. Use the following command to create a stack to be used with your Hadoop cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>stack create --name “myStackName" --nodeCount 20 </span></code></pre></td></tr></table></div></figure>


<h3>Select a blueprint</h3>

<p>We ship default Hadoop cluster blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>blueprint list
</span><span class='line'>
</span><span class='line'>blueprint select --id #ID of the blueprint</span></code></pre></td></tr></table></div></figure>


<h3>Create a Hadoop cluster</h3>

<p>You are almost done &ndash; one more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your <code>template</code>, and by using CloudFormation will launch a cloud <code>stack</code> &ndash; once the <code>stack</code> is up and running (cloud provisioning is done) it will use your selected <code>blueprint</code> and install your custom Hadoop cluster with the selected components and services. For the supported list of Hadoop components and services please check the <a href="http://sequenceiq.com/cloudbreak/#supported-components">documentation</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cluster create --description “my cluster desc"</span></code></pre></td></tr></table></div></figure>


<p>You are done &ndash; you can check the progress through the Ambari UI. If you log back to <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak UI</a> you can check the progress over there as well, and learn the IP address of Ambari.</p>

<h3>Automate the process</h3>

<p>Each time you start the shell the executed commands are logged in a file line by line and later either with the <code>script</code> command or specifying an <code>—cmdfile</code> option the same commands can be executed again.</p>

<h2>Commands</h2>

<p>For the full list of available commands please check below. Please note that all commands are context aware, and you can always use <code>TAB</code> for command completion.</p>

<pre><code>* blueprint add - Add a new blueprint with either --url or --file
* blueprint defaults - Adds the default blueprints to Cloudbreak
* blueprint list - Shows the currently available blueprints
* blueprint select - Select the blueprint by its id
* blueprint show - Shows the blueprint by its id
* cluster create - Create a new cluster based on a blueprint and template
* cluster show - Shows the cluster by stack id
* credential createAzure - Create a new Azure credential
* credential createEC2 - Create a new EC2 credential
* credential defaults - Adds the default credentials to Cloudbreak
* credential list - Shows all of your credentials
* credential select - Select the credential by its id
* credential show - Shows the credential by its id
* exit - Exits the shell
* help - List all commands usage
* hint - Shows some hints
* quit - Exits the shell
* script - Parses the specified resource file and executes its commands
* stack create - Create a new stack based on a template
* stack list - Shows all of your stack
* stack select - Select the stack by its id
* stack show - Shows the stack by its id
* stack terminate - Terminate the stack by its id
* template create - Create a new cloud template
* template createEC2 - Create a new EC2 template
* template defaults - Adds the default templates to Cloudbreak
* template list - Shows the currently available cloud templates
* template select - Select the template by its id
* template show - Shows the template by its id
* version - Displays shell version
</code></pre>

<p>As usual for us &ndash; being committed to 100% open source &ndash; we are always open sourcing everything thus you can get the details on our <a href="https://github.com/sequenceiq/cloudbreak-shell">GitHub</a> repository.
Should you have any questions feel free to engage with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Launch Docker containers on Azure]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/04/launch-docker-containers-on-azure/"/>
    <updated>2014-08-04T19:43:01+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/04/launch-docker-containers-on-azure</id>
    <content type="html"><![CDATA[<p>Two weeks ago we have open sourced our cloud agnostic and Docker based Hadoop as a Service API &ndash; called <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a>.
The first public beta version supports Amazon’s AWS and Microsoft’s Azure, while we are already wrapping up a few new cloud provider integrations.</p>

<p>While there is some documentation about running Docker containers on Amazon, there is no detailed description about running Docker on the Azure cloud.
With this blog post we would like to shed some light on it &ndash; recently there have been lots of announcements from Microsoft about Docker support (Azure CLI, Kubernetes, libswarm) but they are either not finished yet or are not ready to build a robust platform on top.
We are eagerly waiting for the <a href="http://azure.microsoft.com/blog/2014/07/10/azure-collaboration-with-google-and-docker/">Kubernetes integration</a>.</p>

<p>In the meantime, if you are interested in running a <code>cluster</code> of Docker container, or do some more complex stuff then read on.</p>

<p>Just to briefly recap &ndash; with Cloudbreak we are launching on demand Hadoop clusters (check our <a href="http://blog.sequenceiq.com/blog/2014/07/25/cloudbreak-technology/">blog</a> for further technical details) in Docker containers. These containers are <code>shipped</code> to different cloud VMsm and dynamically find and join each other &ndash; they form a fully functional Hadoop cluster without the need to do anything manually on the host, or apply any manual pre-configuration.
So how are we doing this?</p>

<!--more-->


<h3>Docker ready base VM image</h3>

<p>First of all you need a base image with Docker installed &ndash; thus for that we have built and made available an Ubuntu 14.04 image with Docker installed. Apart from Docker, to build a fully dynamic and <code>service discovery</code> aware Docker cluster we needed <a href="http://stedolan.github.io/jq/">jq</a> and <a href="http://www.linuxfromscratch.org/blfs/view/svn/basicnet/bridge-utils.html"> bridge-utils</a>.</p>

<p>Once this base image is created you will need to make it public and re-usable. In order to do that the image has to be published in <a href="http://vmdepot.msopentech.com/List/Index">VMdepot</a>. When you are about to use an image from VM depot, and create a VM based on that you will need to copy it in your own storage account &ndash; note that doing it at first time this can be a slow process (20-25 minutes, copying the 30 GB image).</p>

<h3>Dynamic networking</h3>

<p>Now you have an image based on that you can launch your own VMs, and the Docker container inside your VM. While there are a few options to do that, we needed to find a unified way to do so &ndash; note that  <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a> is a cloud agnostic solution &ndash; and we do not want to create init scripts for each and every cloud environment we use. Amazon’s AWS has a feature so called <code>userdata</code> &ndash; an option of passing data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon AWS: shell scripts and cloud-init directives. In order to keep the launch process unified everywhere we are using <a href="https://help.ubuntu.com/community/CloudInit">cloud-init</a> on Azure as well.</p>

<p>You can use/start Docker with different networking setup &ndash; using a bridged network or using the host network. You can check the init scripts in our <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/resources/azure-init.sh">GitHub</a> repository.</p>

<p><strong>Bridged network</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># set bridge0 in docker opts</span>
</span><span class='line'>sh -c <span class="s2">&quot;cat &gt; /etc/default/docker&quot;</span> &lt;&lt;<span class="s2">&quot;EOF&quot;</span>
</span><span class='line'><span class="nv">DOCKER_OPTS</span><span class="o">=</span><span class="s2">&quot;-b bridge0 -H unix:// -H tcp://0.0.0.0:2375&quot;</span>
</span><span class='line'>EOF
</span><span class='line'>
</span><span class='line'><span class="nv">CMD</span><span class="o">=</span><span class="s2">&quot;docker run -d -p SOURCE_PORT:DESTINATION_PORT 0 -e SERF_JOIN_IP=$SERF_JOIN_IP -e SERF_ADVERTISE_IP=$MY_IP --dns 127.0.0.1 --name ${NODE_PREFIX}${INSTANCE_IDX} -h ${NODE_PREFIX}${INSTANCE_IDX}.${MYDOMAIN} --entrypoint /usr/local/serf/bin/start-serf-agent.sh  $IMAGE $AMBARI_ROLE&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p><strong>Host network</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">CMD</span><span class="o">=</span><span class="s2">&quot;docker run -d -e SERF_JOIN_IP=$AMBARI_SERVER_IP --net=host --name ${NODE_PREFIX}${INSTANCE_IDX} --entrypoint /usr/local/serf/bin/start-serf-agent.sh  $IMAGE $AMBARI_ROLE&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p><em>Note: for cloud based clusters we are giving up on the bridged based network &ndash; mostly due to Azure&rsquo;s networking limitations &ndash; and will use the <code>net=host</code> solution in the next release. The bridged network will still be a supported solution, though we are using it mostly with bare metal or multi container/host solutions.</em></p>

<p>Azure has (comparing with Amazon’s AWS or Google’s Cloud compute) an <code>uncommon</code> network setup and supports limited flexibility &ndash; in order to overcome these, and still have a dynamic Hadoop cluster different scenarios / use cases requires different Docker networking &ndash; that is quite a large <strong>undocumented</strong> topic which we will cover in our next blog posts &ndash; in particular the issues, differences and solutions to use Docker on different cloud providers. While we have briefly talked about <a href="http://sequenceiq.com/cloudbreak/#technology">Serf</a> in the <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> documentation, we will enter in deep technical details in one of our next posts as well. Should you be interested in these, make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a> for updates.</p>

<h3>SequenceIQ’s Azure REST API &ndash; open sourced</h3>

<p>At <a href="htp://sequenceiq.com">SequenceIQ</a> we always automate everything &ndash; and in order to launch VM instances, configure networks, start containers, etc we needed a REST client which we can use it from our JAVA and Scala codebase. Since the Microsoft API is XML based &ndash; <em>yo, it’s 2014</em> &ndash; we have created and open sourced a Groovy based <a href="https://github.com/sequenceiq/azure-rest-client">Azure REST API</a> &ndash; wrapping the XML calls into a nice, easy to use and clean REST API. Feel free to use it &ndash; it’s open sourced under an Apache 2 license. Note that <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> does not store your Azure user credential &ndash; whereas with the defulat Azue CLI that would have been possible &ndash; the only thing we need from your side to work is your subscription id. The process is documented here: <a href="http://sequenceiq.com/cloudbreak/#accounts.">http://sequenceiq.com/cloudbreak/#accounts.</a></p>

<h3>Metadata service for Azure</h3>

<p>The another nice feature we have created for Azure VMs is a <code>metadata service</code>. While a service as such does exists on Amazon’s AWS it’s missing from Microsoft Azure &ndash; note that our Cloudbreak solution is a cloud agnostic one, and we always strive to use identical solution on all cloud providers. The instance metadata is data about your instance that you can use to configure or manage the running instances &ndash; and available via a REST call. We have developed a service as such for Azure &ndash; <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/java/com/sequenceiq/cloudbreak/service/stack/connector/azure/AzureMetadataSetup.java">AzureMetadataSetup</a>. As you can see we collect the metadata, and make it available under a <code>unique hash</code> for each cluster by calling the following resource: <code>/metadata/{hash}</code></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">private</span> <span class="n">Set</span><span class="o">&lt;</span><span class="n">CoreInstanceMetaData</span><span class="o">&gt;</span> <span class="nf">collectMetaData</span><span class="o">(</span><span class="n">Stack</span> <span class="n">stack</span><span class="o">,</span> <span class="n">AzureClient</span> <span class="n">azureClient</span><span class="o">,</span> <span class="n">String</span> <span class="n">name</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="o">...</span> <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>                <span class="n">CoreInstanceMetaData</span> <span class="n">instanceMetaData</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CoreInstanceMetaData</span><span class="o">(</span><span class="n">vmName</span><span class="o">,</span>
</span><span class='line'>                        <span class="n">getPrivateIP</span><span class="o">((</span><span class="n">String</span><span class="o">)</span> <span class="n">virtualMachine</span><span class="o">),</span>
</span><span class='line'>                        <span class="n">getVirtualIP</span><span class="o">((</span><span class="n">String</span><span class="o">)</span> <span class="n">virtualMachine</span><span class="o">));</span>
</span><span class='line'>                <span class="n">instanceMetaDatas</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">instanceMetaData</span><span class="o">);</span>
</span><span class='line'>            <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">IOException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span> <span class="o">...</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This service is used in a few cases &ndash; for example to learn different network setups as the hosts are using different network options than the Docker containers.</p>

<p>As usual for us &ndash; being committed to 100% open source &ndash; we are always open sourcing everything thus you can get the details on our <a href="https://github.com/sequenceiq/cloudbreak">GitHub</a> repository.
Should you have any questions feel free to engage with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark - MLlib Introduction]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/31/spark-mllib/"/>
    <updated>2014-07-31T07:47:32+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/31/spark-mllib</id>
    <content type="html"><![CDATA[<h3>Introduction</h3>

<p>In one of our earlier posts we have mentioned that we use Scalding (among others) for writing MR jobs. Scala/Scalding simplifies the implementation of many MR patterns and makes it easy to implement quite complex jobs like machine learning algorithms. Map Reduce is a mature and widely used framework and it is a good choice for processing large amounts of data &ndash; but not as great if you’d like to use it for fast iterative algorithms/processing. This is a use case where <a href="https://spark.apache.org/">Apache Spark</a> can be quite handy. Spark is fit for these kind of algorithms, because it tries to keep everything in memory (in case of you run out of memory, you can switch to another <a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence">storage levels</a>).</p>

<h3>Apache Spark &ndash; MLlib library</h3>

<p><a href="https://spark.apache.org/docs/latest/mllib-guide.html">MLlib</a> is a machine learning library which ships with Apache Spark, and can run on any Hadoop2/YARN cluster without any pre-installation. At SequenceIQ we use MLlib in Scala &ndash; but you could use it from Java and Python as well. Let us quickly show you an MLlib clustering algorithm with code examples.</p>

<h3>KMeans example</h3>

<p>K-Means (Lloyd&rsquo;s algorithm) is a simple NP-hard unsupervised learning algorithm that solve well known clustering problems. The essence of the algorithm is to separate your data into K cluster. In simple terms it needs 4 steps. First of all you have to vectorize your data. (you can do that with text values too). The code looks like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="n">context</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="n">input</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">line</span> <span class="k">=&gt;</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="sc">&#39;,&#39;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toDouble</span><span class="o">))</span>
</span><span class='line'><span class="o">}.</span><span class="n">cache</span><span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>The second step is to choose K center points (centroids). The third one is to assign each vector to the group that has the closest centroid. After all this is done, next thing you will need to do is to recalculate the positions of the centroids. You have to repeat the third and fourth steps until the centroids are not moving (<code>the iterative stuff</code>). The <a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/clustering/KMeans.scala">KMeans</a> MLlib model is doing that for you.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">clusters</span><span class="k">:</span> <span class="kt">KMeansModel</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="o">.</span><span class="n">train</span><span class="o">(</span><span class="n">data</span><span class="o">,</span> <span class="n">K</span><span class="o">,</span> <span class="n">maxIteration</span><span class="o">,</span> <span class="n">runs</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">vectorsAndClusterIdx</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="n">point</span> <span class="k">=&gt;</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">prediction</span> <span class="k">=</span> <span class="n">clusters</span><span class="o">.</span><span class="n">predict</span><span class="o">(</span><span class="n">point</span><span class="o">)</span>
</span><span class='line'>  <span class="o">(</span><span class="n">point</span><span class="o">.</span><span class="n">toString</span><span class="o">,</span> <span class="n">prediction</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>After you have your model result, you can utilize it in your RDD object.</p>

<h3>Running Spark job on YARN</h3>

<p>In order to run this Spark application on YARN first of all you will need a Hadoop YARN cluster. For that you could use our Hadoop as a Service API called <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a> &ndash; using a <code>multi-node-hdfs-yarn</code> blueprint will set you up a Spark ready Hadoop cluster in less than 2 minutes on your favorite cloud provider. Give it a try at our hosted <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> instance.</p>

<p>Once your cluster it’s up and ready you can run the following command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./bin/spark-submit --class com.sequenceiq.spark.Main --master <span class="se">\</span>
</span><span class='line'>yarn-client --driver-memory 1g --executor-memory 1g --executor-cores 1 <span class="se">\</span>
</span><span class='line'>/root/spark-clustering-1.0.jar hdfs://sandbox:9000/input/input.txt /output 10 10 1
</span></code></pre></td></tr></table></div></figure>


<p>Alternatively you can run this in our free Docker based Apache Spark container as well. You can get a Spark container from the official <a href="https://registry.hub.docker.com/u/sequenceiq/spark/">Docker registry</a> or from our <a href="https://github.com/sequenceiq/docker-spark">GitHub</a> repository.
As always we are making the source code available at <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/spark-clustering">SequenceIQ&rsquo;s GitHub repository</a> (check the other interesting examples as well).  You can find 2 simple input datasets for testing purposes.</p>

<p>The result of the clustering looks like this (generated with R):</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/spark-clustering/data/spark-clustering_1.jpeg" alt="" /></p>

<p>While there is a loud buzz about what’s faster than the other and there are huge numbers thrown in as the <em>X</em> multiplier factor we don’t really want to enter that game &ndash; as a fact we’d like to mention that both example performs better than Mahout KMeans (2-3x faster with 20 iterations), but these are really small datasets. We have seen larger datasets in production where the performances are quite the same, or can go the other way (especially that Spark is new and people don’t always get the configuration right).</p>

<p>In one of our next post we will show you metrics for a much larger dataset and other ML algorithms &ndash; follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a> for updates.</p>

<h3>Apache Tez</h3>

<p>We can’t finish this blog post before not talking about <a href="http://tez.apache.org/">Apache Tez</a> &ndash; the project is aimed at building an application framework which allows for a complex directed-acyclic-graph of tasks for processing data &ndash; fast. We (and many others) believe that this can be a good alternative for Spark &ndash; especially for machine learning. The number of frameworks which are adding or moving the MR runtime to Tez is increasing &ndash; among the few to mention are Cascading, Summingbird, Conjecture &ndash; including us as well.</p>

<p>Note that Apache Tez has already showed <strong>awesome</strong> result. Being the key building block of the <a href="http://hortonworks.com/labs/stinger/">Stinger inititive</a> &ndash; led by Hortonworks &ndash; managed to bring near real time queries and speed up Hive with 100x.</p>

<h3>Other promising machine learning frameworks</h3>

<p>If you are interested in machine learning frameworks, you have to check  <a href="https://github.com/etsy/Conjecture">Conjecture</a> or <a href="https://github.com/tresata/ganitha">ganitha</a> &ndash; they both show great fueatures and have promising results.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker ships Hadoop to the cloud]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/25/cloudbreak-technology/"/>
    <updated>2014-07-25T12:56:39+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/25/cloudbreak-technology</id>
    <content type="html"><![CDATA[<p>A week ago we have <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">announced</a> and open sourced <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a>, the first Docker based Hadoop as a Service API. In this post we&rsquo;d like to introduce you into the technical details and the building blocks of the architecture.
Cloudbreak is built on the foundation of cloud providers APIs, Apache Ambari, Docker containers, Serf and dnsmasq. It is a cloud agnostic solution &ndash; as all the Hadoop services and components are running inside Docker containers &ndash; and these containers are shipped across different cloud providers.</p>

<p>Cloudbreak product documentation: <a href="http://sequenceiq.com/cloudbreak">http://sequenceiq.com/cloudbreak</a></p>

<p>Cloudbreak API documentation: <a href="http://docs.cloudbreak.apiary.io/">http://docs.cloudbreak.apiary.io/</a></p>

<h2>How it works</h2>

<p>From Docker containers point of view we have two kind of containers &ndash; based on their Ambari role &ndash; server and agent. There is one Docker container running the Ambari server, and there are many Docker containers running the Ambari agents. The used Docker image is always the same: <code>sequenceiq/ambari</code> and
the Ambari role is decided based on the <code>$AMBARI_ROLE</code> variable.</p>

<p>For example on Amazon EC2 this is how we start the containers:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>docker run -d -p &lt;LIST of ports&gt; -e <span class="nv">SERF_JOIN_IP</span><span class="o">=</span><span class="nv">$SERF_JOIN_IP</span> --dns 127.0.0.1 --name <span class="k">${</span><span class="nv">NODE_PREFIX</span><span class="k">}${</span><span class="nv">INSTANCE_IDX</span><span class="k">}</span> -h <span class="k">${</span><span class="nv">NODE_PREFIX</span><span class="k">}${</span><span class="nv">INSTANCE_IDX</span><span class="k">}</span>.<span class="k">${</span><span class="nv">MYDOMAIN</span><span class="k">}</span> --entrypoint /usr/local/serf/bin/start-serf-agent.sh  <span class="nv">$IMAGE</span> <span class="nv">$AMBARI_ROLE</span>
</span></code></pre></td></tr></table></div></figure>


<p>As we are starting up the instances and the Docker containers on the host, we&rsquo;d like them to join each other and be able to communicate &ndash; though we don&rsquo;t know the IP addresses beforehand. This can be challanging on cloud environments &ndash; where your IP address and DNS name is dynamically allocated &ndash; however you don&rsquo;t want to collect these imformations beforehand launching the Docker containers.
For that we use Serf &ndash; and pass along the IP address <code>SERF_JOIN_IP=$SERF_JOIN_IP</code> of the first container. Using a gossip protocol Serf will automatically discover each other, set the DNS names, and configure the routing between the nodes.
Serf reconfigures the DNS server <code>dnsmasq</code> running inside the container, and keeps it up to date with the joining or leaving nodes information.
As you can see at startup we always pass a <code>--dns 127.0.0.1</code> dns server for the container to use.</p>

<p>As you see there is no cloud specific code at the Docker containers level, the same technology can be used on bare metal as well.
Check our previous blog posts about a <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">multi node Hadoop cluster on any host</a>.</p>

<p>Obliviously there is some configuration on the host as well &ndash; for that and to handle early initialization of a cloud instance we use <a href="https://help.ubuntu.com/community/CloudInit">CloudInit</a>. We will write a blog post about these for every cloud provider we support.</p>

<p>For additional information you can check our slides from the <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">Hadoop Summit 2014</a>.</p>

<p>Once Ambari is started it will install the selected components based on the passed Hadoop blueprint &ndash; and start the desired services.</p>

<!-- more -->


<h2>Used Technologies</h2>

<h3>Apache Ambari</h3>

<p>The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/ambari-overview.png" alt="" /></p>

<p>Ambari enables System Administrators to:</p>

<ol>
<li>Provision a Hadoop Cluster</li>
<li>provides a step-by-step wizard for installing Hadoop services across any number of hosts.</li>
<li><p>handles configuration of Hadoop services for the cluster.</p></li>
<li><p>Manage a Hadoop Cluster</p></li>
<li><p>provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.</p></li>
<li><p>Monitor a Hadoop Cluster</p></li>
<li>provides a dashboard for monitoring health and status of the Hadoop cluster.</li>
<li>leverages Ganglia for metrics collection.</li>
<li>leverages Nagios for system alerting and will send emails when your attention is needed (e.g. a node goes down, remaining disk space is low, etc).</li>
</ol>


<p>Ambari enables to integrate Hadoop provisioning, management and monitoring capabilities into applications with the Ambari REST APIs.
Ambari Blueprints are a declarative definition of a cluster. With a Blueprint, you can specify a Stack, the Component layout and the Configurations to materialize a Hadoop cluster instance (via a REST API) without having to use the Ambari Cluster Install Wizard.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/ambari-create-cluster.png" alt="" /></p>

<h3>Docker</h3>

<p>Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications. Consisting of Docker Engine, a portable, lightweight runtime and packaging tool, and Docker Hub, a cloud service for sharing applications and automating workflows, Docker enables apps to be quickly assembled from components and eliminates the friction between development, QA, and production environments. As a result, IT can ship faster and run the same app, unchanged, on laptops, data center VMs, and any cloud.</p>

<p>The main features of Docker are:</p>

<ol>
<li>Lightweight, portable</li>
<li>Build once, run anywhere</li>
<li>VM &ndash; without the overhead of a VM</li>
<li>Each virtualized application includes not only the application and the necessary binaries and libraries, but also an entire guest operating system</li>
<li><p>The Docker Engine container comprises just the application and its dependencies. It runs as an isolated process in userspace on the host operating system, sharing the kernel with other containers.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/vm.png" alt="" /></p></li>
<li><p>Containers are isolated</p></li>
<li>It can be automated and scripted</li>
</ol>


<h3>Serf</h3>

<p>Serf is a tool for cluster membership, failure detection, and orchestration that is decentralized, fault-tolerant and highly available. Serf runs on every major platform: Linux, Mac OS X, and Windows. It is extremely lightweight.
Serf uses an efficient gossip protocol to solve three major problems:</p>

<ul>
<li><p>Membership: Serf maintains cluster membership lists and is able to execute custom handler scripts when that membership changes. For example, Serf can maintain the list of Hadoop servers of a cluster and notify the members when nodes come online or go offline.</p></li>
<li><p>Failure detection and recovery: Serf automatically detects failed nodes within seconds, notifies the rest of the cluster, and executes handler scripts allowing you to handle these events. Serf will attempt to recover failed nodes by reconnecting to them periodically.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/serf-gossip.png" alt="" /></p></li>
<li><p>Custom event propagation: Serf can broadcast custom events and queries to the cluster. These can be used to trigger deploys, propagate configuration, etc. Events are simple fire-and-forget broadcast, and Serf makes a best effort to deliver messages in the face of offline nodes or network partitions. Queries provide a simple realtime request/response mechanism.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/serf-event.png" alt="" /></p></li>
</ul>


<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YARN Schedulers demystified - Part 1: Capacity]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/22/schedulers-part-1/"/>
    <updated>2014-07-22T12:56:39+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/22/schedulers-part-1</id>
    <content type="html"><![CDATA[<p>After our first <a href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues/">post</a> about re-prioritizing already submitted and running jobs on different queues we have received many questions and feedbacks about the Capacity Scheduler internals. While there is <code>some</code> documentation available, there is no extensive and deep documentation about how it actually works internally. Since it&rsquo;s all event based it&rsquo;s pretty hard to understand the flow &ndash; let alone debugging it. At <a href="http://sequenceiq.com/">SequenceIQ</a> we are working on a heuristic cluster scheduler &ndash; and understanding how YARN schedulers work was essential. This is part of a larger piece of work &ndash; which will lead to a fully dynamic Hadoop cluster &ndash; orchestrating <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the first open source and Docker based <strong>Hadoop as a Service API</strong>. As usual for us, this work and what we have already done around Capacity and Fair schedulers will be open sourced (or already contributed back to Apache YARN project).</p>

<h2>The Capacity Scheduler internals</h2>

<p>The CapacityScheduler is the default scheduler used with Hadoop 2.x. Its purpose is to allow multi-tenancy and share resources between multiple organizations and applications on the same cluster. You can read about the high level abstraction
<a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">here</a>. In this blog entry we&rsquo;ll examine it
from a deep technical point of view (the implementation can be found <a href="https://github.com/apache/hadoop-common/tree/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity">here</a>
as part of the ResourceManager). I&rsquo;ll try to keep it short and deal with the most important aspects, preventing to write a book about it (would be still better than twilight).</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-queue-tests/src/main/resources/event-flow.gif"></p>

<p>The animation shows a basic application submission event flow, but don&rsquo;t worry if you don&rsquo;t understand it yet, hopefully you will when you&rsquo;re done with the reading.</p>

<!-- more -->


<h2>Configuration</h2>

<p>It all begins with the configuration. The scheduler consists of a queue hierarchy, something like this
(except it’s xml ah.. capacity-scheduler.xml):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yarn.scheduler.capacity.maximum-am-resource-percent=0.2
</span><span class='line'>yarn.scheduler.capacity.maximum-applications=10000
</span><span class='line'>yarn.scheduler.capacity.node-locality-delay=40
</span><span class='line'>yarn.scheduler.capacity.root.acl_administer_queue=*
</span><span class='line'>yarn.scheduler.capacity.root.capacity=100
</span><span class='line'>yarn.scheduler.capacity.root.default.acl_administer_jobs=*
</span><span class='line'>yarn.scheduler.capacity.root.default.acl_submit_applications=*
</span><span class='line'>yarn.scheduler.capacity.root.default.capacity=80
</span><span class='line'>yarn.scheduler.capacity.root.default.maximum-capacity=80
</span><span class='line'>yarn.scheduler.capacity.root.default.state=RUNNING
</span><span class='line'>yarn.scheduler.capacity.root.default.user-limit-factor=1
</span><span class='line'>yarn.scheduler.capacity.root.low.acl_administer_jobs=*
</span><span class='line'>yarn.scheduler.capacity.root.low.acl_submit_applications=*
</span><span class='line'>yarn.scheduler.capacity.root.low.capacity=20
</span><span class='line'>yarn.scheduler.capacity.root.low.maximum-capacity=40
</span><span class='line'>yarn.scheduler.capacity.root.low.state=RUNNING
</span><span class='line'>yarn.scheduler.capacity.root.low.user-limit-factor=1
</span><span class='line'>yarn.scheduler.capacity.root.queues=default,low</span></code></pre></td></tr></table></div></figure>


<p><img src="http://yuml.me/9d7e9977"></p>

<p>Generally, queues are for to prevent applications to consume more resources then they should.
Be careful when determining the capacities, because if you mess it up the <code>ResourceManager</code> won&rsquo;t start:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Service RMActiveServices failed in state INITED cause: java.lang.IllegalArgumentException: Illegal capacity of 1.1 for children of queue root.</span></code></pre></td></tr></table></div></figure>


<p>The <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java#L255">initScheduler</a>
will parse the configuration file and create either <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java">parent</a>
or <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java">leaf</a>
queues and compute their capabilities.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>capacity.LeafQueue (LeafQueue.java:setupQueueConfigs(312)) - Initializing default
</span><span class='line'>capacity = 0.8 [= (float) configuredCapacity / 100 ]
</span><span class='line'>asboluteCapacity = 0.8 [= parentAbsoluteCapacity * capacity ]
</span><span class='line'>maxCapacity = 0.8 [= configuredMaxCapacity ]
</span><span class='line'>absoluteMaxCapacity = 0.8 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
</span><span class='line'>userLimit = 100 [= configuredUserLimit ]
</span><span class='line'>userLimitFactor = 1.0 [= configuredUserLimitFactor ]
</span><span class='line'>maxApplications = 8000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
</span><span class='line'>maxApplicationsPerUser = 8000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
</span><span class='line'>maxActiveApplications = 1 [= max((int)ceil((clusterResourceMemory / minimumAllocation) * maxAMResourcePerQueuePercent * absoluteMaxCapacity),1) ]
</span><span class='line'>maxActiveAppsUsingAbsCap = 1 [= max((int)ceil((clusterResourceMemory / minimumAllocation) *maxAMResourcePercent * absoluteCapacity),1) ]
</span><span class='line'>maxActiveApplicationsPerUser = 1 [= max((int)(maxActiveApplications * (userLimit / 100.0f) * userLimitFactor),1) ]
</span><span class='line'>usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
</span><span class='line'>absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
</span><span class='line'>maxAMResourcePerQueuePercent = 0.2 [= configuredMaximumAMResourcePercent ]
</span><span class='line'>minimumAllocationFactor = 0.75 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
</span><span class='line'>numContainers = 0 [= currentNumContainers ]
</span><span class='line'>state = RUNNING [= configuredState ]
</span><span class='line'>acls = ADMINISTER_QUEUE: SUBMIT_APPLICATIONS:* [= configuredAcls ]
</span><span class='line'>nodeLocalityDelay = 40</span></code></pre></td></tr></table></div></figure>


<p>Although it does not imply, but application submission is only allowed to leaf queues. By default all application is submitted to
a queue called <code>default</code>. One interesting property is the <code>schedule-asynchronously</code> about which I&rsquo;ll talk later.</p>

<h2>Messaging</h2>

<p>Once the ResourceManager is up and running, the messaging starts. Mostly everything happens via events. These events are distributed with
a <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java">dispatcher</a>
among the registered event handlers. The down side of the event driven architecture that it&rsquo;s hard to follow the flow because
events can come from everywhere. The CapacityScheduler itself is registered for many events, and act based on these events.
Code snippets are from branch <code>trunk</code> aka <code>3.0.0-SNAPSHOT</code>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'> <span class="nd">@Override</span>
</span><span class='line'>  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">handle</span><span class="o">(</span><span class="n">SchedulerEvent</span> <span class="n">event</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">switch</span><span class="o">(</span><span class="n">event</span><span class="o">.</span><span class="na">getType</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">NODE_ADDED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">NodeAddedSchedulerEvent</span> <span class="n">nodeAddedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">NodeAddedSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">addNode</span><span class="o">(</span><span class="n">nodeAddedEvent</span><span class="o">.</span><span class="na">getAddedRMNode</span><span class="o">());</span>
</span><span class='line'>      <span class="n">recoverContainersOnNode</span><span class="o">(</span><span class="n">nodeAddedEvent</span><span class="o">.</span><span class="na">getContainerReports</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">nodeAddedEvent</span><span class="o">.</span><span class="na">getAddedRMNode</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">NODE_REMOVED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">NodeRemovedSchedulerEvent</span> <span class="n">nodeRemovedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">NodeRemovedSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">removeNode</span><span class="o">(</span><span class="n">nodeRemovedEvent</span><span class="o">.</span><span class="na">getRemovedRMNode</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">NODE_UPDATE:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">NodeUpdateSchedulerEvent</span> <span class="n">nodeUpdatedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">NodeUpdateSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">RMNode</span> <span class="n">node</span> <span class="o">=</span> <span class="n">nodeUpdatedEvent</span><span class="o">.</span><span class="na">getRMNode</span><span class="o">();</span>
</span><span class='line'>      <span class="n">nodeUpdate</span><span class="o">(</span><span class="n">node</span><span class="o">);</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(!</span><span class="n">scheduleAsynchronously</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">allocateContainersToNode</span><span class="o">(</span><span class="n">getNode</span><span class="o">(</span><span class="n">node</span><span class="o">.</span><span class="na">getNodeID</span><span class="o">()));</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_ADDED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">AppAddedSchedulerEvent</span> <span class="n">appAddedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">AppAddedSchedulerEvent</span><span class="o">)</span> <span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">addApplication</span><span class="o">(</span><span class="n">appAddedEvent</span><span class="o">.</span><span class="na">getApplicationId</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAddedEvent</span><span class="o">.</span><span class="na">getQueue</span><span class="o">(),</span> <span class="n">appAddedEvent</span><span class="o">.</span><span class="na">getUser</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_REMOVED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">AppRemovedSchedulerEvent</span> <span class="n">appRemovedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">AppRemovedSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">doneApplication</span><span class="o">(</span><span class="n">appRemovedEvent</span><span class="o">.</span><span class="na">getApplicationID</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appRemovedEvent</span><span class="o">.</span><span class="na">getFinalState</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_ATTEMPT_ADDED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">AppAttemptAddedSchedulerEvent</span> <span class="n">appAttemptAddedEvent</span> <span class="o">=</span>
</span><span class='line'>          <span class="o">(</span><span class="n">AppAttemptAddedSchedulerEvent</span><span class="o">)</span> <span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">addApplicationAttempt</span><span class="o">(</span><span class="n">appAttemptAddedEvent</span><span class="o">.</span><span class="na">getApplicationAttemptId</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAttemptAddedEvent</span><span class="o">.</span><span class="na">getTransferStateFromPreviousAttempt</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAttemptAddedEvent</span><span class="o">.</span><span class="na">getShouldNotifyAttemptAdded</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_ATTEMPT_REMOVED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">AppAttemptRemovedSchedulerEvent</span> <span class="n">appAttemptRemovedEvent</span> <span class="o">=</span>
</span><span class='line'>          <span class="o">(</span><span class="n">AppAttemptRemovedSchedulerEvent</span><span class="o">)</span> <span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">doneApplicationAttempt</span><span class="o">(</span><span class="n">appAttemptRemovedEvent</span><span class="o">.</span><span class="na">getApplicationAttemptID</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAttemptRemovedEvent</span><span class="o">.</span><span class="na">getFinalAttemptState</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAttemptRemovedEvent</span><span class="o">.</span><span class="na">getKeepContainersAcrossAppAttempts</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">CONTAINER_EXPIRED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">ContainerExpiredSchedulerEvent</span> <span class="n">containerExpiredEvent</span> <span class="o">=</span>
</span><span class='line'>          <span class="o">(</span><span class="n">ContainerExpiredSchedulerEvent</span><span class="o">)</span> <span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">ContainerId</span> <span class="n">containerId</span> <span class="o">=</span> <span class="n">containerExpiredEvent</span><span class="o">.</span><span class="na">getContainerId</span><span class="o">();</span>
</span><span class='line'>      <span class="n">completedContainer</span><span class="o">(</span><span class="n">getRMContainer</span><span class="o">(</span><span class="n">containerId</span><span class="o">),</span>
</span><span class='line'>          <span class="n">SchedulerUtils</span><span class="o">.</span><span class="na">createAbnormalContainerStatus</span><span class="o">(</span>
</span><span class='line'>              <span class="n">containerId</span><span class="o">,</span>
</span><span class='line'>              <span class="n">SchedulerUtils</span><span class="o">.</span><span class="na">EXPIRED_CONTAINER</span><span class="o">),</span>
</span><span class='line'>          <span class="n">RMContainerEventType</span><span class="o">.</span><span class="na">EXPIRE</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">default</span><span class="o">:</span>
</span><span class='line'>      <span class="n">LOG</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="s">&quot;Invalid eventtype &quot;</span> <span class="o">+</span> <span class="n">event</span><span class="o">.</span><span class="na">getType</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;. Ignoring!&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>NODE_ADDED</h3>

<p>Each time a node joins the cluster the <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java#L237">ResourceTrackerService</a>
registers the <code>NodeManager</code> and as part of the transition sends a <code>NodeAddedSchedulerEvent</code>. The scheduler keeps track of
the global cluster resources and adds the node&rsquo;s resources to the global.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">Added</span> <span class="n">node</span> <span class="n">amb1</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">:</span><span class="mi">45454</span> <span class="nl">clusterResource:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">5120</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">8</span><span class="o">&gt;</span>
</span><span class='line'><span class="n">Added</span> <span class="n">node</span> <span class="n">amb2</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">:</span><span class="mi">45454</span> <span class="nl">clusterResource:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">10240</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">16</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>It is also needed to update all the queue metrics since the cluster got bigger, thus the queue capacities also change. More likely to happen
that a new application can be scheduled. If the <code>isWorkPreservingRecoveryEnabled</code> is enabled on the <code>ResourceManager</code> it can recover
containers on a re-joining node.</p>

<h3>NODE_REMOVED</h3>

<p>There can be many reasons that a node is being removed from the cluster, but the scenario is almost the same as adding one. A
<code>NodeRemovedSchedulerEvent</code> is sent and the scheduler subtracts the node&rsquo;s resources from the global and updates all the queue metrics.
Things can be a little bit complicated since the node was active part of the resource scheduling and can have running containers and
reserved resources. The scheduler will kill these containers and notify the applications so they can request new containers and
unreserve the resources.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">rmnode</span><span class="o">.</span><span class="na">RMNodeImpl</span> <span class="o">(</span><span class="n">RMNodeImpl</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">transition</span><span class="o">(</span><span class="mi">569</span><span class="o">))</span> <span class="o">-</span> <span class="n">Deactivating</span> <span class="n">Node</span> <span class="n">amb4</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">:</span><span class="mi">45454</span> <span class="n">as</span> <span class="n">it</span> <span class="n">is</span> <span class="n">now</span> <span class="n">DECOMMISSIONED</span>
</span><span class='line'><span class="n">rmnode</span><span class="o">.</span><span class="na">RMNodeImpl</span> <span class="o">(</span><span class="n">RMNodeImpl</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">handle</span><span class="o">(</span><span class="mi">385</span><span class="o">))</span> <span class="o">-</span> <span class="n">amb4</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">:</span><span class="mi">45454</span> <span class="n">Node</span> <span class="n">Transitioned</span> <span class="n">from</span> <span class="n">RUNNING</span> <span class="n">to</span> <span class="n">DECOMMISSIONED</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">CapacityScheduler</span> <span class="o">(</span><span class="n">CapacityScheduler</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">removeNode</span><span class="o">(</span><span class="mi">980</span><span class="o">))</span> <span class="o">-</span> <span class="n">Removed</span> <span class="n">node</span> <span class="n">amb4</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">:</span><span class="mi">45454</span> <span class="nl">clusterResource:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">15360</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">24</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>APP_ADDED</h3>

<p>On application <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java#L266">submission</a>
an <code>AppAddedSchedulerEvent</code> is made and the scheduler will decide to accept the application or not. It depends whether it
was submitted to a leaf queue and the user have the appropriate rights (ACL) to submit to this queue and the queue can have more applications. If
any of these fails the scheduler will reject the application by sending an <code>RMAppRejectedEvent</code>. Otherwise it will register a new
<code>SchedulerApplication</code> and notify the target queue&rsquo;s parents about it and updates the queue metrics.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">ParentQueue</span> <span class="o">(</span><span class="n">ParentQueue</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">addApplication</span><span class="o">(</span><span class="mi">495</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">added</span> <span class="o">-</span> <span class="nl">appId:</span> <span class="n">application_1405323437551_0001</span> <span class="nl">user:</span> <span class="n">hdfs</span> <span class="n">leaf</span><span class="o">-</span><span class="n">queue</span> <span class="n">of</span> <span class="nl">parent:</span> <span class="n">root</span> <span class="err">#</span><span class="nl">applications:</span> <span class="mi">1</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">CapacityScheduler</span> <span class="o">(</span><span class="n">CapacityScheduler</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">addApplication</span><span class="o">(</span><span class="mi">544</span><span class="o">))</span> <span class="o">-</span> <span class="n">Accepted</span> <span class="n">application</span> <span class="n">application_1405323437551_0001</span> <span class="n">from</span> <span class="nl">user:</span> <span class="n">hdfs</span><span class="o">,</span> <span class="n">in</span> <span class="nl">queue:</span> <span class="k">default</span>
</span></code></pre></td></tr></table></div></figure>


<h3>APP_REMOVED</h3>

<p>The analogy is the same as between <code>NODE_ADDED</code> and <code>NODE_REMOVED</code>. Updates the queue metrics and notifies the parent&rsquo;s that an application finished, removes the application and sets its final state.</p>

<h3>APP_ATTEMPT_ADDED</h3>

<p>After the <code>APP_ADDED</code> event the application is in <code>inactive</code> mode. It means it won`t get any resources scheduled for &ndash; only an attempt to run it. One application can have many attempts as it can fail for many reasons.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">rmapp</span><span class="o">.</span><span class="na">RMAppImpl</span> <span class="o">(</span><span class="n">RMAppImpl</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">handle</span><span class="o">(</span><span class="mi">639</span><span class="o">))</span> <span class="o">-</span> <span class="n">application_1405323437551_0001</span> <span class="n">State</span> <span class="n">change</span> <span class="n">from</span> <span class="n">SUBMITTED</span> <span class="n">to</span> <span class="n">ACCEPTED</span>
</span><span class='line'><span class="n">resourcemanager</span><span class="o">.</span><span class="na">ApplicationMasterService</span> <span class="o">(</span><span class="n">ApplicationMasterService</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">registerAppAttempt</span><span class="o">(</span><span class="mi">611</span><span class="o">))</span> <span class="o">-</span> <span class="n">Registering</span> <span class="n">app</span> <span class="n">attempt</span> <span class="o">:</span> <span class="n">appattempt_1405323437551_0001_000001</span>
</span><span class='line'><span class="n">attempt</span><span class="o">.</span><span class="na">RMAppAttemptImpl</span> <span class="o">(</span><span class="n">RMAppAttemptImpl</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">handle</span><span class="o">(</span><span class="mi">659</span><span class="o">))</span> <span class="o">-</span> <span class="n">appattempt_1405323437551_0001_000001</span> <span class="n">State</span> <span class="n">change</span> <span class="n">from</span> <span class="n">NEW</span> <span class="n">to</span> <span class="n">SUBMITTED</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">LeafQueue</span> <span class="o">(</span><span class="n">LeafQueue</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">activateApplications</span><span class="o">(</span><span class="mi">763</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">application_1405323437551_0001</span> <span class="n">from</span> <span class="nl">user:</span> <span class="n">hdfs</span> <span class="n">activated</span> <span class="n">in</span> <span class="nl">queue:</span> <span class="k">default</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">LeafQueue</span> <span class="o">(</span><span class="n">LeafQueue</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">addApplicationAttempt</span><span class="o">(</span><span class="mi">779</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">added</span> <span class="o">-</span> <span class="nl">appId:</span> <span class="n">application_1405323437551_0001</span> <span class="nl">user:</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">yarn</span><span class="o">.</span><span class="na">server</span><span class="o">.</span><span class="na">resourcemanager</span><span class="o">.</span><span class="na">scheduler</span><span class="o">.</span><span class="na">capacity</span><span class="o">.</span><span class="na">LeafQueue</span><span class="n">$User</span><span class="err">@</span><span class="mi">46</span><span class="n">a224a4</span><span class="o">,</span> <span class="n">leaf</span><span class="o">-</span><span class="nl">queue:</span> <span class="k">default</span> <span class="err">#</span><span class="n">user</span><span class="o">-</span><span class="n">pending</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span> <span class="err">#</span><span class="n">user</span><span class="o">-</span><span class="n">active</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">1</span> <span class="err">#</span><span class="n">queue</span><span class="o">-</span><span class="n">pending</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span> <span class="err">#</span><span class="n">queue</span><span class="o">-</span><span class="n">active</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">1</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">CapacityScheduler</span> <span class="o">(</span><span class="n">CapacityScheduler</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">addApplicationAttempt</span><span class="o">(</span><span class="mi">567</span><span class="o">))</span> <span class="o">-</span> <span class="n">Added</span> <span class="n">Application</span> <span class="n">Attempt</span> <span class="n">appattempt_1405323437551_0001_000001</span> <span class="n">to</span> <span class="n">scheduler</span> <span class="n">from</span> <span class="n">user</span> <span class="n">hdfs</span> <span class="n">in</span> <span class="n">queue</span> <span class="k">default</span>
</span></code></pre></td></tr></table></div></figure>


<p>Attempt states are transferred from one to another. By sending an <code>AppAttemptAddedSchedulerEvent</code> the scheduler actually tries to allocate resources. First, the application goes into the pending applications list of the queue and if the queue limits allows it, it goes into the active applications list. This active application list is the one that the queue uses when trying to allocate resources.
It works in FIFO order, but I&rsquo;ll elaborate on it in the <code>NODE_UPDATE</code> part.</p>

<h3>APP_ATTEMPT_REMOVED</h3>

<p>On <code>AppAttemptRemovedSchedulerEvent</code> the scheduler cleans up after the application. Releases all the allocated, acquired, running containers
(in case of <code>ApplicationMaster</code> restart the running containers won&rsquo;t get killed), releases all reserved containers,
cleans up pending requests and informs the queues.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">rmapp</span><span class="o">.</span><span class="na">RMAppImpl</span> <span class="o">(</span><span class="n">RMAppImpl</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">handle</span><span class="o">(</span><span class="mi">639</span><span class="o">))</span> <span class="o">-</span> <span class="n">application_1405323437551_0001</span> <span class="n">State</span> <span class="n">change</span> <span class="n">from</span> <span class="n">FINISHING</span> <span class="n">to</span> <span class="n">FINISHED</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">CapacityScheduler</span> <span class="o">(</span><span class="n">CapacityScheduler</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">doneApplicationAttempt</span><span class="o">(</span><span class="mi">598</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">Attempt</span> <span class="n">appattempt_1405323437551_0001_000001</span> <span class="n">is</span> <span class="n">done</span><span class="o">.</span> <span class="n">finalState</span><span class="o">=</span><span class="n">FINISHED</span>
</span><span class='line'><span class="n">scheduler</span><span class="o">.</span><span class="na">AppSchedulingInfo</span> <span class="o">(</span><span class="n">AppSchedulingInfo</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">clearRequests</span><span class="o">(</span><span class="mi">108</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">application_1405323437551_0001</span> <span class="n">requests</span> <span class="n">cleared</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">LeafQueue</span> <span class="o">(</span><span class="n">LeafQueue</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">removeApplicationAttempt</span><span class="o">(</span><span class="mi">821</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">removed</span> <span class="o">-</span> <span class="nl">appId:</span> <span class="n">application_1405323437551_0001</span> <span class="nl">user:</span> <span class="n">hdfs</span> <span class="nl">queue:</span> <span class="k">default</span> <span class="err">#</span><span class="n">user</span><span class="o">-</span><span class="n">pending</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span> <span class="err">#</span><span class="n">user</span><span class="o">-</span><span class="n">active</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span> <span class="err">#</span><span class="n">queue</span><span class="o">-</span><span class="n">pending</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span> <span class="err">#</span><span class="n">queue</span><span class="o">-</span><span class="n">active</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span>
</span><span class='line'><span class="n">amlauncher</span><span class="o">.</span><span class="na">AMLauncher</span> <span class="o">(</span><span class="n">AMLauncher</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">run</span><span class="o">(</span><span class="mi">262</span><span class="o">))</span> <span class="o">-</span> <span class="n">Cleaning</span> <span class="n">master</span> <span class="n">appattempt_1405323437551_0001_000001</span>
</span></code></pre></td></tr></table></div></figure>


<h3>NODE_UPDATE</h3>

<p>Normally <code>NodeUpdateSchedulerEvents</code> arrive every second from every node. By setting the earlier mentioned <code>schedule-asynchronously</code> to true
the behavior of this event handling can be altered in a way that container allocations happen asynchronously from these events. Meaning
the CapacityScheduler tries to allocate new containers in every 100ms on a different <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java#L361">thread</a>.
Before going into details let&rsquo;s discuss another important aspect.</p>

<h4>Allocate</h4>

<p>The <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java#L675">allocate</a>
method is used as a heartbeat. This is the most important call between the <code>ApplicationMaster</code> and the scheduler. Instead of using a simple empty message, the heartbeat can contain resource requests of the application. The reason it is important here is that the scheduler, more precisely the queue &ndash; when tries to allocate resources &ndash; it will check the active applications in FIFO order and see their resource requests.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">{</span><span class="nl">Priority:</span> <span class="mi">20</span><span class="o">,</span> <span class="nl">Capability:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">1024</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">1</span><span class="o">&gt;,</span> <span class="err">#</span> <span class="nl">Containers:</span> <span class="mi">2</span><span class="o">,</span> <span class="nl">Location:</span> <span class="o">*,</span> <span class="n">Relax</span> <span class="nl">Locality:</span> <span class="kc">true</span><span class="o">}</span>
</span><span class='line'><span class="o">{</span><span class="nl">Priority:</span> <span class="mi">20</span><span class="o">,</span> <span class="nl">Capability:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">1024</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">1</span><span class="o">&gt;,</span> <span class="err">#</span> <span class="nl">Containers:</span> <span class="mi">2</span><span class="o">,</span> <span class="nl">Location:</span> <span class="o">/</span><span class="k">default</span><span class="o">-</span><span class="n">rack</span><span class="o">,</span> <span class="n">Relax</span> <span class="nl">Locality:</span> <span class="kc">true</span><span class="o">}</span>
</span><span class='line'><span class="o">{</span><span class="nl">Priority:</span> <span class="mi">20</span><span class="o">,</span> <span class="nl">Capability:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">1024</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">1</span><span class="o">&gt;,</span> <span class="err">#</span> <span class="nl">Containers:</span> <span class="mi">2</span><span class="o">,</span> <span class="nl">Location:</span> <span class="n">amb1</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">,</span> <span class="n">Relax</span> <span class="nl">Locality:</span> <span class="kc">true</span><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s examine these requests:</p>

<ul>
<li>Priority: requests with higher priority gets served first (mappers got 20, reducer 10, AM 0)</li>
<li>Capability: denotes the required container&rsquo;s size</li>
<li>Location: where the AM would like to get the containers. There are 3 types of locations: node-local, rack-local, off-switch.
The location is based on the location of the blocks of the data. It is the <code>ApplicationMaster</code>&rsquo;s duty to locate them. Off-switch means
that any node in the cluster is good enough. Typically the <code>ApplicationMaster</code>&rsquo;s container request is an off-switch request.</li>
<li>Relax locality: in case it is set to false, only node-local container can be allocated. By default it is set to true.</li>
</ul>


<h4>NODE_UPDATE</h4>

<p>Let&rsquo;s get back to <code>NODE_UPDATE</code>. What happens at every node update and why it happens so frequently? First of all, nodes are running the containers. Containers start and stop all the time thus the scheduler needs an update about the state of the nodes. Also it updates their resource capabilities as well. After the updates are noted, the scheduler tries to allocate containers on the nodes. The same applies to every node in the cluster. At every <code>NODE_UPDATE</code> the scheduler checks if an application reserved a container on this node. If there is reservation then tries to fulfill it. Every node can have only one reserved container. After the reservation it tries to allocate more containers by going through all the queues starting from <code>root</code>. The node can have one more container if it has at least the minimum allocation&rsquo;s resource capability. Going through the child queues of <code>root</code> it checks the queue&rsquo;s active applications and its resource requests by priority order. If the application has request for this node it tries to allocate it. If the relax locality is set to true it could also allocate container even though there is no explicit request for this node, but that&rsquo;s not what&rsquo;s going to happen first. There is another term called delay scheduling. The scheduler tries to delay any non-data-local
request, but cannot delay it for so long. The <code>localityWaitFactor</code> will determine how long to wait until fall back to rack-local then the end to off-switch requests. If everything works out it allocates a container and tracks its resource usage. If there is not enough resource capability one application can reserve a container on this node, and at the next update may can use this reservation. After the allocation is made the <code>ApplicationMaster</code> will submit a request to the node to launch this container and assign a task to it to run.
The scheduler does not need to know what task the AM will run in the container.</p>

<h3>CONTAINER_EXPIRED</h3>

<p>The <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/ContainerAllocationExpirer.java">ContainerAllocationExpirer&rsquo;s</a>
responsibility to check if a container expires and when it does it sends an <code>ContainerExpiredSchedulerEvent</code> and the scheduler will notify the application to remove the container. The value of how long to wait until a container is considered dead can be configured.</p>

<h2>Animation</h2>

<p>The animation on top shows a basic event flow starting from adding 2 nodes and submitting 2 applications with attempts. Eventually the node updates tries to allocate resources to those applications. After reading this post I hope it makes sense now.</p>

<h2>What&rsquo;s next?</h2>

<p>In the next part of this series I&rsquo;ll compare it with FairScheduler, to see the differences.</p>

<p>For updates and further blog posts follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>

<p>Enjoy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudbreak - the Hadoop as a Service API]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/"/>
    <updated>2014-07-18T16:37:37+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak</id>
    <content type="html"><![CDATA[<p><em>Cloudbreak is a powerful left surf that breaks over a coral reef, a mile off southwest the island of Tavarua, Fiji.</em></p>

<p><em>Cloudbreak is a cloud agnostic Hadoop as a Service API. Abstracts the provisioning and ease management and monitoring of on-demand clusters.</em></p>

<p>Today is a big day for us and the Hadoop community &ndash; we are announcing the first <code>public beta</code> version of our open source and cloud agnostic <strong>Hadoop as a Service API</strong>.</p>

<p>During our daily work with large Hadoop clusters in the cloud, <code>dockerized</code> environments and bare metal we were doing the same things over and over again. Although we are automating and <code>dockerizing</code> always everything, we felt that something is missing &ndash; an open source, cloud agnostic Hadoop as a Service API. Welcome <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> &ndash; you are one POST away from your on-demand Hadoop cluster on your favorite cloud provider.</p>

<p>When we have started to work on Cloudbreak &ndash; first of all to solve our internal needs at SequenceIQ &ndash; we have set the following criteria:</p>

<ul>
<li>Use open source software and be <strong>100% open source</strong> under Apache 2 license</li>
<li>Have the ability to quickly launch arbitrary sized Hadoop clusters</li>
<li>Be cloud provider agnostic and create an SDK which allows to quickly add new providers</li>
<li>No more glue code, repeating the same things over and over again</li>
<li>Have a REST API and a CLI in order to be able to automate the whole process</li>
<li>Create an easy to use and responsive UI</li>
<li>Support different Hadoop services and configurations in a declarative way</li>
<li>Elastic and flexible, with the ability to resize running clusters</li>
<li>Secure</li>
</ul>


<!-- more -->


<h2>Docker in the cloud</h2>

<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we are running all our core applications and processes in Docker containers &ndash; and that is true for Hadoop and all of the services as well. During the last few months we have <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">blogged</a> and open sourced all of the <a href="https://hub.docker.com/u/sequenceiq/">building blocks</a> of our <code>dockerized</code> systems and <strong>Cloudbreak</strong> is built on the foundation of these and reusing the same technologies we have released before. While Cloudbreak&rsquo;s primary role is to launch on-demand Hadoop clusters in the cloud, the underlying technology actually does more. It can launch on-demand Hadoop clusters in any environment which supports Docker &ndash; in a dynamic way. There is no predefined configuration needed as all the setup, orchestration, networking and cluster membership is done dynamically.</p>

<ul>
<li><a href="https://hub.docker.com/u/sequenceiq/">Docker containers</a> &ndash; all the Hadoop services are installed and running inside Docker containers, and these containers are <code>shipped</code>  between different cloud vendors, keeping Cloudbreak cloud agnostic</li>
<li><a href="https://github.com/sequenceiq/ambari-rest-client">Apache Ambari</a> &ndash; to declaratively define a Hadoop cluster</li>
<li><a href="https://github.com/sequenceiq/docker-serf">Serf</a> &ndash; for cluster membership, failure detection, and orchestration that is decentralized, fault-tolerant and highly available for dynamic clusters</li>
<li><a href="https://github.com/sequenceiq/docker-dnsmasq">dnsmasq</a> &ndash; to provide resolvable fully qualified domain names between dynamically created Docker containers.</li>
</ul>


<p>The project was presented at the <strong>Hadoop Summit 2014,</strong> in San Jose &ndash; you can get the slides from <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">here</a>.</p>

<p>While there is an extensive list of articles explaining the benefits of using Docker, we would like to highlight our motivations in a few bullet points.</p>

<ul>
<li>Write once, run anywhere &ndash; our solution uses the same Docker containers on different cloud providers, <code>dockerized</code>  environments or bare metal, no difference at all</li>
<li>Reproducible, testable environment &ndash; we are recreating complete config environments in seconds, and being able to work with the same containers on our laptop, QA and production/cloud environments</li>
<li>Isolation &ndash; each container is separated and runs in its own isolated sandbox</li>
<li>Versioning &ndash; we are able to easily version and modify containers, and ship only the changed bits saving bandwidth; essential for large clusters deployed in the cloud</li>
<li>Central repository &ndash; you can build an entire cluster from a trusted and centralised container repository, the Docker Registry/Hub</li>
<li>Smart resource allocation &ndash; containers can be <code>shipped</code> anywhere and resources can be allotted</li>
</ul>


<h2>Cloudbreak &ndash; the project</h2>

<h3>Cloudbreak UI</h3>

<p>The easiest way to start your own Hadoop cluster in your favorite cloud provider is to use our hosted solution. We host, maintain and support <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> for you. Cloudbreak UI is a secure and intuitive way to launch on-demand Hadoop clusters with a few mouse clicks. Please note that Cloudbreak is launching Hadoop clusters on the user&rsquo;s behalf &ndash; on different cloud providers. We do not store your cloud provider account details (such as username, password, keys, private SSL certificates, etc), but work around the concept that Identity and Access Management is fully controlled by you &ndash; the end user.</p>

<h3>Cloudbreak API</h3>

<p>Cloudbreak is a RESTful Hadoop as a Service API. The easiest way to use the API is by using our hosted <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak API</a>.</p>

<p>We have also give you the option to host Cloudbreak within your organization. Once it is deployed in your favourite servlet container exposes a REST API allowing to span up Hadoop clusters of arbitrary sizes on your selected cloud provider. With Cloudbreak you are one POST away from your on-demand Hadoop cluster. You can get the code from our <a href="https://github.com/sequenceiq/cloudbreak">GitHub repository</a>. For further documentation please follow up with the <a href="http://sequenceiq.com/cloudbreak/">general</a> and <a href="http://docs.cloudbreak.apiary.io/">API</a> documentation, or subscribe to one of our social channels in order to receive notifications about further blog posts and releases.</p>

<h3>Cloudbreak REST client</h3>

<p>In order to ease your work with the REST API and embed in your codebase we have created (and also extensively use) a Groovy REST client. The code is available at our <a href="https://github.com/sequenceiq/cloudbreak-rest-client">GitHub</a> repository.</p>

<h3>Cloudbreak CLI</h3>

<p>As we automate everything and we are a very DevOps focused company we are always trying to create easy ways to interact with our systems and API’s. In case of Cloudbreak we have created and released a <a href="https://github.com/sequenceiq/cloudbreak-shell">command line shell</a>, the Cloudbreak CLI. The CLI allows you to use all the REST calls, and it has a large number of easing commands. Interactive help and completion is available.</p>

<h3>Cloudbreak documentation</h3>

<p>We have created two types of documentation. The <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak Product</a> documentation contains an overview, installation, architectural and technical content, whereas the <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak API</a> explains the REST API with examples and a mock server to test your integration.</p>

<h2>Supported Hadoop services</h2>

<p>At high level the supported list of components can be grouped into two main categories: Master and Slave &ndash; and bundling them together form a Hadoop Service. <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> supports the following Hadoop services.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>| Services    | Components                                                              |
</span><span class='line'>| ----------- | ------------------------------------------------------------------------|
</span><span class='line'>| HDFS        | DATANODE, HDFS_CLIENT, JOURNALNODE, NAMENODE, SECONDARY_NAMENODE, ZKFC  |
</span><span class='line'>| YARN        | APP_TIMELINE_SERVER, NODEMANAGER, RESOURCEMANAGER, YARN_CLIENT          |
</span><span class='line'>| MAPREDUCE2  | HISTORYSERVER, MAPREDUCE2_CLIENT                                        |
</span><span class='line'>| GANGLIA     | GANGLIA_MONITOR, GANGLIA_SERVER                                         |
</span><span class='line'>| HBASE       | HBASE_CLIENT, HBASE_MASTER, HBASE_REGIONSERVER                          |
</span><span class='line'>| HIVE        | HIVE_CLIENT, HIVE_METASTORE, HIVE_SERVER, MYSQL_SERVER                  |
</span><span class='line'>| HCATALOG    | HCAT                                                                    |
</span><span class='line'>| WEBHCAT     | WEBHCAT_SERVER                                                          |
</span><span class='line'>| NAGIOS      | NAGIOS_SERVER                                                           |
</span><span class='line'>| OOZIE       | OOZIE_CLIENT, OOZIE_SERVER                                              |
</span><span class='line'>| PIG         | PIG                                                                     |
</span><span class='line'>| SQOOP       | SQOOP                                                                   |
</span><span class='line'>| STORM       | DRPC_SERVER, NIMBUS, STORM_REST_API, STORM_UI_SERVER, SUPERVISOR        |
</span><span class='line'>| TEZ         | TEZ_CLIENT                                                              |
</span><span class='line'>| FALCON      | FALCON_CLIENT, FALCON_SERVER                                            |
</span><span class='line'>| ZOOKEEPER   | ZOOKEEPER_CLIENT, ZOOKEEPER_SERVER                                      |</span></code></pre></td></tr></table></div></figure>


<p>Please note that you can always build your own custom stack beyond these services, using Ambari&rsquo;s custom stack definitions.</p>

<h2>What’s next?</h2>

<p>We will follow up with a few posts to drive you through the technology, API and insights and make it easier for you to learn, understand and use Hadoop in the cloud.</p>

<p>In the meantime we suggest you to go through the <a href="http://sequenceiq.com/cloudbreak/">documentation</a>, try <a href="http://cloudbreak.sequenceiq.com/">Cloudbreak</a> and let us know how it works for you.</p>

<p>Please note that <a href="http://cloudbreak.sequenceiq.com/">Cloudbreak</a> is under development, in public beta &ndash; while we consider the codebase stable for deployments (and use it daily), please let us know if you face any problems through <a href="https://github.com/sequenceiq/cloudbreak">GitHub</a> issues. Also we  welcome your open source contribution &ndash; let it be a bug fix or a new cloud provider <a href="http://sequenceiq.com/cloudbreak/#add-new-cloud-providers">implementation</a>.</p>

<p>Finally, your opinion is important to us &ndash; if you’d like to see your <strong>favourite cloud provider</strong> among the existing ones, please fill this <a href="https://docs.google.com/forms/d/129RVh6VfjRsuuHOcS3VPbFYTdM2SEjANDsGCR5Pul0I/viewform">questionnaire</a>. Make your voice heard!</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Groovy and Java, the runtime bug]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/13/groovy-and-java-runtime-bug/"/>
    <updated>2014-07-13T09:13:53+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/13/groovy-and-java-runtime-bug</id>
    <content type="html"><![CDATA[<p>I can barely count how many languages we use at SequenceIQ <em>[based on our <a href="https://github.com/sequenceiq">GitHub</a> repository it&rsquo;s Java, Scala, Groovy, Go, CoffeeScript, JavaScript, R and Shell (Ansible, Dockerfile, AWS CLI, what not)]</em>. Groovy is one of them.
Coding in Groovy is fast and fun, isn&rsquo;t it? Except when problems come up at runtime. This is one of those.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/groovy-bug/src/main/resources/wtf.png"></p>

<!-- more -->


<p>The Ambari REST Client is written in Groovy and in this case used by a Java application. You can find the sample bug in our
<a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/groovy-bug">repository</a>. The same thing could have been achieved with
reflection as well. Do you know why this can happen? It&rsquo;s a good candidate for an interview question, isn&rsquo;t it &hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Ambari configuration service]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service/"/>
    <updated>2014-07-09T08:20:05+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service</id>
    <content type="html"><![CDATA[<p>At SequenceIQ we use Apache Ambari for provisioning, managing, and monitoring Apache Hadoop clusters on different environments. However Ambari has more useful features than these &ndash; especially for us who automate and frequently build on-demand Hadoop clusters in cloud environments and submit different applications into. These Hadoop clusters carry different components, configurations and services &ndash; think of dev->test->UAT->PROD cluster lifecycles, different settings, SLA&rsquo;s, etc).</p>

<p>Configuration of applications that use dynamically built YARN clusters can be challenging. This is due to the huge amount of configuration properties, some of which needs to be kept in sync on YARN client application side. Think of <em>yarn.resourcemanager.address</em>, <em>fs.defaultFS</em>, <em>yarn.resourcemanager.scheduler.address</em> to name a few. Each time these cluster specific entries change, client applications needs to be reconfigured. Those who ever played with clusters where the default properties are overridden know what this means&hellip;</p>

<p>At SequenceIQ we use Ambari for building on-demand YARN clusters (see the related <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/"> blog post</a>). In our case Ambari not only maintains the configuration of the cluster it manages but also provides access to them through a set of REST resources.</p>

<p>To overcome the configuration maintenance problem in YARN client applications, we implemented an Ambari REST client application that embedded in client applications can dynamically retrieve configuration from an Ambari instance. Thus the only thing needed for an application to have the proper configuration is the access to the Ambari instance.</p>

<p>The Ambari REST client is an open source project we developed and contributed to Apache Ambari &ndash; it&rsquo;s a Groovy REST client used by the <a href="https://github.com/sequenceiq/ambari-shell">Ambari Shell</a> and <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a>.</p>

<!-- more -->


<p>Here is a short example on how to make use of the Ambari client in an arbitrary application:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">AmbariConfigurationService</span> <span class="o">{</span>
</span><span class='line'><span class="o">...</span>
</span><span class='line'><span class="kd">private</span> <span class="n">AmbariClient</span> <span class="n">ambariClient</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kd">public</span> <span class="nf">AmbariConfigurationService</span><span class="o">(){</span>
</span><span class='line'>  <span class="c1">// inject / provide the service with the ambari related properties</span>
</span><span class='line'>  <span class="n">ambariClient</span> <span class="o">=</span> <span class="k">new</span> <span class="n">AmbariClient</span><span class="o">(</span><span class="n">ambariHost</span><span class="o">,</span> <span class="n">ambariPort</span><span class="o">,</span> <span class="n">ambariUser</span><span class="o">,</span> <span class="n">ambariPass</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// list with the properties needed by the application</span>
</span><span class='line'><span class="kd">private</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">configList</span> <span class="o">=</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="s">&quot;mapreduce.framework.name&quot;</span><span class="o">,</span> <span class="s">&quot;yarn.resourcemanager.address&quot;</span><span class="o">,</span> <span class="s">&quot;hbase.zookeeper.quorum&quot;</span> <span class="o">);</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// assembles a Configuration instance with the properties needed by the application</span>
</span><span class='line'><span class="kd">public</span> <span class="n">Configuration</span> <span class="nf">getConfiguration</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>        <span class="c1">//  use this constructor to avoid loading of properties from the classpath!</span>
</span><span class='line'>        <span class="n">Configuration</span> <span class="n">configuration</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">(</span><span class="kc">false</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// Map with service specific configuration. The keys are service names: eg.: yarn-site, hbase-site, global ...</span>
</span><span class='line'>        <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">serviceConfigMap</span> <span class="o">=</span> <span class="n">ambariClient</span><span class="o">.</span><span class="na">getServiceConfigMap</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">for</span> <span class="o">(</span><span class="n">Map</span><span class="o">.</span><span class="na">Entry</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">serviceEntry</span> <span class="o">:</span> <span class="n">serviceConfigMap</span><span class="o">.</span><span class="na">entrySet</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>            <span class="k">for</span> <span class="o">(</span><span class="n">Map</span><span class="o">.</span><span class="na">Entry</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">configEntry</span> <span class="o">:</span> <span class="n">serviceEntry</span><span class="o">.</span><span class="na">getValue</span><span class="o">().</span><span class="na">entrySet</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>                <span class="k">if</span> <span class="o">(</span><span class="n">configList</span><span class="o">.</span><span class="na">contains</span><span class="o">(</span><span class="n">configEntry</span><span class="o">.</span><span class="na">getKey</span><span class="o">()))</span> <span class="o">{</span>
</span><span class='line'>                    <span class="n">configuration</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">configEntry</span><span class="o">.</span><span class="na">getKey</span><span class="o">(),</span> <span class="n">configEntry</span><span class="o">.</span><span class="na">getValue</span><span class="o">());</span>
</span><span class='line'>                <span class="o">}</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// decorate the config with application specific entries, like &quot;dfs.client.use.legacy.blockreader&quot;, &quot;mapreduce.job.user.classpath.first&quot;</span>
</span><span class='line'>        <span class="n">decorateConfiguration</span><span class="o">(</span><span class="n">configuration</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">return</span> <span class="n">configuration</span><span class="o">;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p><em>Note: Apart from the <code>getServiceConfigMap()</code> method you&rsquo;ll find a few interesting and useful operations</em></p>

<p>You can get the Ambari client code from the <a href="https://github.com/sequenceiq/ambari-rest-client">SequenceIQ GitHub repository</a> &ndash; clone it, build it and add it as a dependency to your project.</p>

<p>If you&rsquo;d like to play with a real multi-node Ambari managed cluster check out <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">this</a> older blog post &ndash; this will set you up with a Hadoop cluster in less than 2 minutes / one-click.</p>

<p>Let us know how it works for you &ndash; for updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker debug with nsenter on boot2docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/05/docker-debug-with-nsenter-on-boot2docker/"/>
    <updated>2014-07-05T12:05:41+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/05/docker-debug-with-nsenter-on-boot2docker</id>
    <content type="html"><![CDATA[<p><code>nsenter</code> is a small tool allowing to <code>enter</code> into <code>n</code>ame<code>s</code>paces. Specifically
when you work with docker, it means you can <em>enter</em> any docker container, even
it they don&rsquo;t run any sshd. Running sshd in a docker container for debuging
<a href="http://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil/">considered evil</a>.</p>

<h2>Nsenter with Boot2docker</h2>

<p>Docker doesn&rsquo;t run directly on OS X and on Windows, so you need
<a href="http://boot2docker.io/">boot2docker</a>. To get <code>nsenter</code> working with boot2docker
is a bit trickier.</p>

<p>For the impatient here is a simple function, which lets you enter any docker
container directly from OS X (or any boot2docker host):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker-enter() {
</span><span class='line'>  boot2docker ssh '[ -f /var/lib/boot2docker/nsenter ] || docker run --rm -v /var/lib/boot2docker/:/target jpetazzo/nsenter'
</span><span class='line'>  boot2docker ssh -t sudo /var/lib/boot2docker/docker-enter "$@"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>the first line installs <code>nsenter</code> and <code>docker-enter</code> if missing and the second line
does the actual call.</p>

<p>once you declared the function, you can use it as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker-enter &lt;CONTAINER-ID/CONTAINER-NAME&gt;</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>TL;DR</h2>

<p>If you are interested about the details how this works read on.</p>

<h2>Install nseneter onto boot2docker</h2>

<p>How to install nsenter into boot2docker? Its a bit tricky, as boot2docker isn&rsquo;t
a full-blown linux, it&rsquo;s based on tiny core linux, so compiling on it is not trivial.</p>

<p>But guess what, <strong>jpetazzo</strong> already created a <a href="https://github.com/jpetazzo/nsenter">dockerized nsenter</a>
It suggest to install the binary <code>nsenter</code> as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run --rm -v /usr/local/bin:/target jpetazzo/nsenter</span></code></pre></td></tr></table></div></figure>


<p>This works with boot2docker &hellip; until you restart it. You should store all
changes on the persistent <code>/var/lib/boot2docker</code> directory.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run --rm -v /var/lib/boot2docker/:/target jpetazzo/nsenter</span></code></pre></td></tr></table></div></figure>


<h2>the docker-enter script</h2>

<p><a href="https://github.com/jpetazzo/nsenter/blob/master/docker-enter">docker-enter</a> is
a helper script to do the following 2 steps:</p>

<ul>
<li>gets the <code>PID</code> of the docker container</li>
<li>executes <code>nsenter</code> optionally passing the name of a program to execute inside
the namespace. if no command is specified a shell will be invoked instead.</li>
</ul>


<p>In the previous step, when you have installed <code>nseneter</code> the <code>docker-enter</code> srcipt
<a href="https://github.com/jpetazzo/nsenter/blob/master/installer#L6">got installed</a> into the same directory.</p>

<h2>Nsenter directly from OS X</h2>

<p>Some blogs advise you to first ssh into boot2docker, and use nsenter or docker-enter
inside of the virtual env. But if you are executing a single command via ssh, you
can pass the command to the last argument of: <code>boot2docker ssh &lt;COMMAND&gt;</code></p>

<h2>One-liner</h2>

<p>So combine all the steps into a single <strong>one-liner</strong> function:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker-enter() { boot2docker ssh -t "[ -f /var/lib/boot2docker/nsenter ] || docker run --rm -v /var/lib/boot2docker/:/target jpetazzo/nsenter; sudo /var/lib/boot2docker/docker-enter $@"; }</span></code></pre></td></tr></table></div></figure>


<p>If you want it permanently either copy-paste it into your <code>~/.profile</code> or
<code>~/.bash_profile</code>. Or save it into <code>/usr/local/bin</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Lo /usr/local/bin/docker-enter j.mp/docker-enter && . /usr/local/bin/docker-enter</span></code></pre></td></tr></table></div></figure>


<p>Let us know how <code>nsenter</code> works for you &ndash; and for updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Re-prioritize running jobs with YARN schedulers]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues/"/>
    <updated>2014-07-02T08:20:05+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we run different applications all within the same Hadoop YARN cluster. Often the deployed Hadoop stack is a multi-tenant and multi-application and runtime setup &ndash; and as usual for a scenario as such end users will try to use or book as much cluster capacity as possible. A great help for solving these problems are YARN schedulers &ndash; however in our case due to certain SLA and QoS requirements we needed to step further. We have invested a great effort to build custom YARN schedulers, learn about application insights (check our <a href="http://blog.sequenceiq.com/blog/2014/05/01/mapreduce-job-profiling-with-R/">blog post</a> about how we use R to profile running jobs) and we would like to share our experience with the community. Let&rsquo;s dig into technical details.</p>

<p>In YARN, one of the ResourceManager&rsquo;s most important role is the scheduling (allocating available resources in the cluster) between competing applications. It doesn&rsquo;t care about per-application states nor internal flows and optimizations, but the overall resource requirements of
each application. Currently there are 3 different scheduler implementations available: FIFO, Fair, Capacity.</p>

<p>Going back a few weeks in time we blogged about how to configure the
<a href="http://blog.sequenceiq.com/blog/2014/03/14/yarn-capacity-scheduler/">CapacityScheduler</a> and use different queue
setups. Based on the feedbacks we have received we realized that there is a lack of knowledge about how these schedulers work and many people have asked to fill that gap. Good news that we didn&rsquo;t
forget about you. We&rsquo;re going to start a post series where we&rsquo;ll explain them a little bit detailed with fancy diagrams and code examples.</p>

<p>But before doing that, let&rsquo;s visit a concrete problem we encountered while we were developing our product stack.
We wanted to use the CapacityScheduler, but for different reasons (SLA and QoS) move the submitted applications between different queues to set a priority among them &ndash; at runtime (quick reminder: queues are either a composition of other queues or a collection of applications, forming a tree).
Cross application priorites can&rsquo;t be configured yet, only priorities between tasks within the application. The only problem is if you check the code you&rsquo;ll find this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'>  <span class="kd">public</span> <span class="n">String</span> <span class="nf">moveApplication</span><span class="o">(</span><span class="n">ApplicationId</span> <span class="n">appId</span><span class="o">,</span> <span class="n">String</span> <span class="n">newQueue</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">YarnException</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">throw</span> <span class="k">new</span> <span class="nf">YarnException</span><span class="o">(</span><span class="n">getClass</span><span class="o">().</span><span class="na">getSimpleName</span><span class="o">()</span>
</span><span class='line'>        <span class="o">+</span> <span class="s">&quot; does not support moving apps between queues&quot;</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>Currently this operation is supported only by the FairScheduler. Why is it not implemented? Let us know in a comment and you might receive a surprise present from us :). In the meantime if we&rsquo;d like
to implement it what would be the steps? Lets start with the following queue hierarchy and their capabilities taken from the integration tests:</p>

<p><img src="http://yuml.me/1fe68e90"></p>

<p>Assume we&rsquo;ve submitted 2 applications, <strong>app1</strong> to <code>b2</code> and <strong>app2</strong> to <code>a2</code> (submitting applications is only allowed to leaf queues). What if <strong>app2</strong> is
pending for so long because of the queue capacity and my friend&rsquo;s friend&rsquo;s dog cannot wait anymore to see his data clustering result? We could play with the queue capacities and max capacities, but then other apps might get scheduled as well and we don&rsquo;t want that.
Then we could move the app to a queue where it can get resources with a much higher chance. To move an app to somewhere
else in the hierarchy we have to consider and update a whole bunch of things. Let&rsquo;s move <strong>app1</strong> to queue <code>b1</code>.</p>

<p>Obviously we have to check if the target queue is a leaf queue and moving the app there does not violate any constraints. But how to do that?
The first part is easy (leaf or parent), but what about the other one? It has to do something with queue capacities, but checking only the target
queue&rsquo;s capacity is not enough, we have to go up in the hierarchy (because the parent queues also keep track the number of applications
and resource usages) but for how deep? The lowest common ancestor of the source and target is enough, because above that nothing changes. In our
case it&rsquo;s the <code>b</code> (b1, b2). Finding it is not that hard since the queues are declared like this:</p>

<ul>
<li>root.a.a1</li>
<li>root.a.a2</li>
<li>root.b.b1</li>
<li>root.b.b2</li>
<li>root.b.b3</li>
</ul>


<p>Going back until <code>b</code> and check the capacities:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">CSQueue</span> <span class="n">currentQueue</span> <span class="o">=</span> <span class="n">targetQueue</span><span class="o">;</span>
</span><span class='line'><span class="k">while</span> <span class="o">(</span><span class="n">currentQueue</span> <span class="o">!=</span> <span class="n">lowestCommonAncestor</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// maxApps</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">currentQueue</span><span class="o">.</span><span class="na">getNumApplications</span><span class="o">()</span> <span class="o">==</span> <span class="k">this</span><span class="o">.</span><span class="na">conf</span><span class="o">.</span><span class="na">getMaximumApplicationsPerQueue</span><span class="o">(</span><span class="n">currentQueue</span><span class="o">.</span><span class="na">getQueueName</span><span class="o">()))</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">throw</span> <span class="k">new</span> <span class="nf">YarnException</span><span class="o">(</span><span class="s">&quot;Moving app attempt &quot;</span> <span class="o">+</span> <span class="n">appAttId</span> <span class="o">+</span> <span class="s">&quot; to queue &quot;</span>
</span><span class='line'>      <span class="o">+</span> <span class="n">queueName</span> <span class="o">+</span> <span class="s">&quot; would violate queue maxApps constraints on&quot;</span>
</span><span class='line'>      <span class="o">+</span> <span class="s">&quot; queue &quot;</span> <span class="o">+</span> <span class="n">currentQueue</span><span class="o">.</span><span class="na">getQueueName</span><span class="o">());</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// maxCapacity</span>
</span><span class='line'>  <span class="kt">float</span> <span class="n">potentialNewCapacity</span> <span class="o">=</span> <span class="n">Resources</span><span class="o">.</span><span class="na">divide</span><span class="o">(</span><span class="n">calculator</span><span class="o">,</span> <span class="n">clusterResource</span><span class="o">,</span> <span class="n">Resources</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">currentQueue</span><span class="o">.</span><span class="na">getUsedResources</span><span class="o">(),</span> <span class="n">consumption</span><span class="o">),</span> <span class="n">clusterResource</span><span class="o">);</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">potentialNewCapacity</span> <span class="o">&gt;=</span> <span class="n">currentQueue</span><span class="o">.</span><span class="na">getAbsoluteMaximumCapacity</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">throw</span> <span class="k">new</span> <span class="nf">YarnException</span><span class="o">(</span><span class="s">&quot;Moving app attempt &quot;</span> <span class="o">+</span> <span class="n">appAttId</span> <span class="o">+</span> <span class="s">&quot; to queue &quot;</span>
</span><span class='line'>      <span class="o">+</span> <span class="n">queueName</span> <span class="o">+</span> <span class="s">&quot; would violate queue maxCapacity constraints on&quot;</span>
</span><span class='line'>      <span class="o">+</span> <span class="s">&quot; queue &quot;</span> <span class="o">+</span> <span class="n">currentQueue</span><span class="o">.</span><span class="na">getQueueName</span><span class="o">());</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="n">currentQueue</span> <span class="o">=</span> <span class="n">currentQueue</span><span class="o">.</span><span class="na">getParent</span><span class="o">();</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>If everything is fine we can execute the movement.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">private</span> <span class="kt">void</span> <span class="nf">executeMove</span><span class="o">(</span><span class="n">SchedulerApplication</span> <span class="n">app</span><span class="o">,</span> <span class="n">FiCaSchedulerApp</span> <span class="n">attempt</span><span class="o">,</span> <span class="n">LeafQueue</span> <span class="n">oldQueue</span><span class="o">,</span> <span class="n">LeafQueue</span> <span class="n">newQueue</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">oldQueue</span><span class="o">.</span><span class="na">removeApplicationAttempt</span><span class="o">(</span><span class="n">attempt</span><span class="o">);</span>
</span><span class='line'>    <span class="n">attempt</span><span class="o">.</span><span class="na">move</span><span class="o">(</span><span class="n">newQueue</span><span class="o">);</span> <span class="c1">// This updates all the queue metrics &#39;til the parent</span>
</span><span class='line'>    <span class="n">app</span><span class="o">.</span><span class="na">setQueue</span><span class="o">(</span><span class="n">newQueue</span><span class="o">);</span>
</span><span class='line'>    <span class="n">newQueue</span><span class="o">.</span><span class="na">trackApplications</span><span class="o">(</span><span class="n">attempt</span><span class="o">.</span><span class="na">getApplicationId</span><span class="o">(),</span> <span class="n">attempt</span><span class="o">.</span><span class="na">getUser</span><span class="o">());</span>
</span><span class='line'>    <span class="n">newQueue</span><span class="o">.</span><span class="na">submitApplicationAttempt</span><span class="o">(</span><span class="n">attempt</span><span class="o">,</span> <span class="n">attempt</span><span class="o">.</span><span class="na">getUser</span><span class="o">());</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>There are so many things implemented in these method calls it wouldn&rsquo;t fit here, but it serves the purpose here as pseudo code.</p>

<ul>
<li><p>oldQueue.removeApplicationAttempt(attempt);<br/>
Remove the application from the active and pending list. Notify the parents that an app has been removed.</p></li>
<li><p>attempt.move(newQueue);<br/>
Update the queue metrics upwards to root.</p></li>
<li><p>app.setQueue(newQueue);<br/>
Set the target queue in the app.</p></li>
<li><p>newQueue.trackApplications(attempt.getApplicationId(), attempt.getUser());<br/>
Notify the parents that a new application has been moved here.</p></li>
<li><p>newQueue.submitApplicationAttempt(attempt, attempt.getUser());<br/>
Finally submit the application attempt to the queue.</p></li>
</ul>


<p>As usual we always release the code as well &ndash; you can get the details from our <a href="https://github.com/sequenceiq">GitHub</a> page.</p>

<ul>
<li><p>Move applications between Capacity Scheduler queues <a href="https://github.com/sequenceiq/hadoop-common/blob/branch-2.4.1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/a/ExtendedCapacityScheduler.java#L924">implementation</a>.</p></li>
<li><p>Test case <a href="https://github.com/sequenceiq/hadoop-common/blob/branch-2.4.1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/a/TestExtendedCapacitySchedulerAppMove.java">implementation</a>.</p></li>
</ul>


<p>In case you are interested on the YARN Scheduler series make sure to follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a> or <a href="https://twitter.com/sequenceiq">Twitter</a> for the upcoming posts.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Hadoop 2.4.1 on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/25/hadoop-2-4-0-docker/"/>
    <updated>2014-06-25T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/25/hadoop-2-4-0-docker</id>
    <content type="html"><![CDATA[<p>A few weeks ago we have released an Apache Hadoop 2.3 Docker image &ndash; in a very short time this become the most <a href="https://registry.hub.docker.com/search?q=hadoop&amp;s=downloads">popular</a> Hadoop image in the Docker <a href="https://registry.hub.docker.com/">registry</a>.</p>

<p>Following on the success of our Hadoop 2.3 Docker <a href="https://registry.hub.docker.com/u/sequenceiq/hadoop-docker/">image</a>, the feedbacks and requests we have received and aligning with the Hadoop release cycle, we have released an Apache Hadoop 2.4.1 Docker image &ndash; same as the previous version this is available as a trusted and automated build on the official Docker <a href="https://registry.hub.docker.com/">registry</a>.</p>

<p>Please note that beside this Hadoop image, we have released and maintain a <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/">pseudo-distributed</a> and <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">distributed</a> Hadoop Docker image provisioned with Apache Ambari. As they are provisioned with Ambari you have the option to change, add and remove Hadoop components using cluster blueprints.</p>

<h2>Build the image</h2>

<p>In case you&rsquo;d like to try directly from the <a href="https://github.com/sequenceiq/hadoop-docker">Dockerfile</a> you can build the image as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build  -t sequenceiq/hadoop-docker .</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<h2>Pull the image</h2>

<p>As it is also released as an official Docker image from Docker&rsquo;s automated build repository &ndash; you can always pull or refer the image when launching containers.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/hadoop-docker:2.4.1</span></code></pre></td></tr></table></div></figure>


<h2>Start a container</h2>

<p>In order to use the Docker image you have just build or pulled use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t sequenceiq/hadoop-docker /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<h2>Testing</h2>

<p>You can run one of the stock examples:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd $HADOOP_PREFIX
</span><span class='line'># run the mapreduce
</span><span class='line'>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar grep input output 'dfs[a-z.]+'
</span><span class='line'>
</span><span class='line'># check the output
</span><span class='line'>bin/hdfs dfs -cat output/*</span></code></pre></td></tr></table></div></figure>


<h2>Hadoop native libraries, build, Bintray, etc</h2>

<p>The Hadoop build process is no easy task &ndash; requires lots of libraries and their right version, protobuf, etc and takes some time &ndash; we have simplified all these, made the build and released a 64b version of Hadoop nativelibs on our <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.4.1/view/files">Bintray repo</a>. Enjoy.</p>
]]></content>
  </entry>
  
</feed>
