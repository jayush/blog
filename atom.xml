<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-08-15T07:20:15+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Flet for SALE]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/15/flet-for-sale/"/>
    <updated>2014-08-15T07:17:49+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/15/flet-for-sale</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker intercontainer networking explained]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/12/docker-networking/"/>
    <updated>2014-08-12T08:53:15+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/12/docker-networking</id>
    <content type="html"><![CDATA[<p>The purpose of this blog entry is to cover advanced topics regarding Docker networking and explain different concepts to inter-connect Docker containers when the containers are running on different host machines.
For the demonstration we are using VMs on <a href="https://www.virtualbox.org/">VirtualBox</a> launched with <a href="http://www.vagrantup.com/">Vagrant</a>, but the explained networking concepts work also on Amazon EC2 (with VPC) and Azure unless stated otherwise.</p>

<p>To set up the the test environment clone the <a href="https://github.com/sequenceiq/sequenceiq-samples">SequenceIQ&rsquo;s samples repository</a> and follow the instructions.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone git@github.com:sequenceiq/sequenceiq-samples.git
</span><span class='line'>cd sequenceiq-samples/docker-networking
</span><span class='line'>vagrant up</span></code></pre></td></tr></table></div></figure>


<p>The <code>vagrant up</code> command launches the test setup, which conatins two Ubuntu 14.04 VMs with the network configuration:</p>

<ul>
<li><a href="https://www.virtualbox.org/manual/ch06.html#network_nat">NAT</a></li>
<li><a href="https://docs.vagrantup.com/v2/networking/private_network.html">Private networking</a></li>
</ul>


<p>The NAT (related to eth0 interface on VMs) is used only for access the external network from VMs e.g. download files from debian repository, but it is not used for inter-container communication. The Vagrant sets up a properly configured Host Only Networking in VirtualBox therefore the VMs can communicate with each other on the defined IP addresses:</p>

<ul>
<li>vm1: 192.168.40.11</li>
<li>vm2: 192.168.40.12</li>
</ul>


<p>Let&rsquo;s see how Docker containers running on these VMs can send IP packets to each other.</p>

<!--more-->


<h2>Setting up bridge0</h2>

<p>The Docker attaches all containers to the virtual subnet implemented by docker0, this means that by default on both VMs the Docker containers will be launched with IP addresses from range 172.17.42.1/24. This is a problem for some of the solutions explained below, because if the containers on different hosts have the same IP addresses then we won&rsquo;t be able to properly route the IP packets between them. Therefore on each VMs a network bridge is created with the following subnets:</p>

<ul>
<li>vm1: 172.17.51.1/24</li>
<li>vm2: 172.17.52.1/24</li>
</ul>


<p>This means that every container luanched on vm1 will get an IP address from range 172.17.51.2 &ndash; 172.17.51.255 and containers on vm2 will have an address from range 172.17.52.2 &ndash; 172.17.52.255.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># do not execute, it was already executed on vm1 as root during provision from Vagrant</span>
</span><span class='line'>brctl addbr bridge0
</span><span class='line'>sudo ifconfig bridge0 172.17.51.1 netmask 255.255.255.0
</span><span class='line'>sudo bash -c <span class="s1">&#39;echo DOCKER_OPTS=\&quot;-b=bridge0\&quot; &gt;&gt; /etc/default/docker&#39;</span>
</span><span class='line'>sudo service docker restart
</span><span class='line'>
</span><span class='line'><span class="c"># do not execute, it was already executed on vm1 as root during provision from Vagrant</span>
</span><span class='line'>sudo brctl addbr bridge0
</span><span class='line'>sudo ifconfig bridge0 172.17.52.1 netmask 255.255.255.0
</span><span class='line'>sudo bash -c <span class="s1">&#39;echo DOCKER_OPTS=\&quot;-b=bridge0\&quot; &gt;&gt; /etc/default/docker&#39;</span>
</span><span class='line'>sudo service docker restart
</span></code></pre></td></tr></table></div></figure>


<p>As noted in the comments the above configuration is already executed during the provisioning of VMs and it was copied here just for the sake of clarity and completeness.</p>

<h2>Expose container ports to host</h2>

<p>Probably the simplest way to solve inter-container communication is to expose ports from container to the host. This can be done with the <code>-p</code> switch. E.g. exposing the port 3333 is as simple as:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># execute on vm1</span>
</span><span class='line'>sudo docker run -it --rm --name cont1 -p 3333:3333 ubuntu /bin/bash -c <span class="s2">&quot;nc -l 3333&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="c"># execute on vm2</span>
</span><span class='line'>sudo docker run -it --rm --name cont2 ubuntu /bin/bash -c <span class="s2">&quot;nc -w 1 -v 192.168.40.11 3333&quot;</span>
</span><span class='line'><span class="c">#Result: Connection to 192.168.40.11 3333 port [tcp/*] succeeded!</span>
</span></code></pre></td></tr></table></div></figure>


<p>This might be well suited for cases when the communication ports are defined in advance (e.g. MySQL will run on port 3306), but will not work when the application uses dynamic ports for communication (like Hadoop does with IPC ports).</p>

<h2>Host networking</h2>

<p>If the container is started with <code>--net=host</code> then it avoids placing the container inside of a separate network stack, but as the Docker documentation says this option &ldquo;tells Docker to not containerize the container&rsquo;s networking&rdquo;. The <code>cont1</code> container can bind directly to the network interface of host therefore the <code>nc</code> will be available directly on 192.168.40.11.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># execute on vm1</span>
</span><span class='line'>sudo docker run -it --rm --name cont1 --net<span class="o">=</span>host ubuntu /bin/bash -c <span class="s2">&quot;nc -l 3333&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="c"># execute on vm2</span>
</span><span class='line'>sudo docker run -it --rm --name cont2 ubuntu /bin/bash -c <span class="s2">&quot;nc -w 1 -v 192.168.40.11 3333&quot;</span>
</span><span class='line'><span class="c">#Result: Connection to 192.168.40.11 3333 port [tcp/*] succeeded!</span>
</span></code></pre></td></tr></table></div></figure>


<p>Of course if you want to access cont2 from cont1 then cont2 also needs to be started with <code>--net=host</code> option.
The host networking is very powerful solution for inter-container communication, but it has its drawbacks, since the ports used by the container can collide with the ports used by host or other containers utilising &mdash;net=host option, because all of them are sharing the same network stack.</p>

<h2>Direct Routing</h2>

<p>So far we have seen methods where the containers have used the IP address of host to communicate with each other, but there are solutions to inter-connect the containers by using their own IPs. If we are using the containers own IPs for routing then it is important that we shall be able to distinguish based on IP which container is running on vm1 and which one is running on on vm2, this was the reason why the bridge0 interface was created as explained in &ldquo;Setting up bridge0&rdquo; section.
To make the things a bit easier to understand I have created a simplified diagram of the network interfaces in our current test setup. If I would like to oversimplify the thing then I would say that, we shall setup the routing in that way that the packets from one container are following the red lines shown on the diagram.</p>

<p style="text-align:center;"> <img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/docker-networking/img/routing.png"></p>

<p>To achive this we need to configure the routing table on hosts in that way that every packet which destination is 172.17.51.0/24 is forwarded to vm1 and every IP packet where the destination is 172.17.52.0/24 is forwarded to vm2. To repeat it shortly, the containers running on vm1 are placed to subnet 172.17.51.0/24, containers on vm2 are on subnet 172.17.52.0/24.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># execute on vm1</span>
</span><span class='line'>sudo route add -net 172.17.52.0 netmask 255.255.255.0 gw 192.168.40.12
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.51.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont1  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont1)</span>
</span><span class='line'>nc -l 3333
</span><span class='line'>
</span><span class='line'><span class="c"># execute on vm2</span>
</span><span class='line'>sudo route add -net 172.17.51.0  netmask 255.255.255.0  gw 192.168.40.11
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.52.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont2  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont2)</span>
</span><span class='line'>nc -w 1 -v 172.17.51.2 3333
</span><span class='line'><span class="c">#Result: Connection to 172.17.51.2 3333 port [tcp/*] succeeded!</span>
</span></code></pre></td></tr></table></div></figure>


<p>The <code>route add</code> command adds the desired routing to the route table, but you might wonder why the iptables configuration is necessary. The reason for that the Docker by default sets up a rule to the nat table to masquerade all IP packets that are leaving the machine. In our case we definitely don&rsquo;t want this, therefore we delete all MASQUERADE rules with -F option. At this point we already would be able to make the connection from one container to other and vice verse, but the containers would not be able to communicate with the outside world, therefore an iptables rule needs to be added to masquerade the packets that are going outside of 172.17.0.0/16. I need to mention the another approach would be to use the <a href="https://docs.docker.com/articles/networking/#between-containers">&mdash;iptables=false</a> option of the daemon to avoid any manipulation in the iptables and you can do all the config manually.</p>

<p>Such kind of direct routing from one vm to other vm works great and easy to set up, but cannot be used if the hosts are not on the same subnet. If the host are located the on different subnet the tunneling might be an option as you will see it in the next section.</p>

<p><em>Note: This solution works on Amazon EC2 instances only if the <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_NAT_Instance.html#EIP_Disable_SrcDestCheck">Source/Destionation Check</a> is disabled.</em></p>

<p><em>Note: Due to the packet filtering policy of Azure this method cannot be used there.</em></p>

<h2>Generic Routing Encapsulation (GRE) tunnel</h2>

<p>GRE is a tunneling protocol that can encapsulate a wide variety of network layer protocols inside virtual point-to-point links.
The main idea is to create a GRE tunnel between the VMs and send all traffic through it:</p>

<p style="text-align:center;"> <img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/docker-networking/img/gre.png"></p>

<p>In order to create a tunnel you need to specify the name, the type (which is gre in our case) and the IP address of local and the remote end. Consequently the tun2 name used for the tunnel on on vm1 since from vm1 perspective that is the tunnel endpoint which leads to vm2 and every packet sent to tun2 to will eventually come out on vm2 end.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#GRE tunnel config execute on vm1</span>
</span><span class='line'>sudo iptunnel add tun2 mode gre <span class="nb">local </span>192.168.40.11 remote 192.168.40.12
</span><span class='line'>sudo ifconfig tun2 10.0.201.1
</span><span class='line'>sudo ifconfig tun2 up
</span><span class='line'>sudo route add -net 172.17.52.0 netmask 255.255.255.0 dev tun2
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.51.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont1  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont1)</span>
</span><span class='line'>nc -l 3333
</span><span class='line'>
</span><span class='line'><span class="c">#GRE tunnel config execute on vm2</span>
</span><span class='line'>sudo iptunnel add tun1 mode gre <span class="nb">local </span>192.168.40.12 remote 192.168.40.11
</span><span class='line'>sudo ifconfig tun1 10.0.202.1
</span><span class='line'>sudo ifconfig tun1 up
</span><span class='line'>sudo route add -net 172.17.51.0 netmask 255.255.255.0 dev tun1
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.52.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont2  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont2)</span>
</span><span class='line'>nc -w 1 -v 172.17.51.2 3333
</span><span class='line'><span class="c">#Result: Connection to 172.17.51.2 3333 port [tcp/*] succeeded!</span>
</span></code></pre></td></tr></table></div></figure>


<p>After the tunnel is set up and activated the remaining commands are very similar to the commands executed in the &ldquo;Direct Routing&rdquo; section. The main difference here is that we do not route the traffic directly to other vm, but we are routing it into <code>dev tun1</code> and <code>dev tun2</code> respectively.</p>

<p>With GRE tunnels a point-to-point connection is set up between two hosts, which means that if you have more then two hosts in your network and want to interconnect all of them, then n-1 tunnel endpoint needs to be created on every host, which will be quite challenging to maintain if you have a large cluster.</p>

<p><em>Note: GRE packets are <a href="http://msdn.microsoft.com/en-us/library/azure/dn133803.aspx">filtered out</a> on Azure therefore this solution cannot be used there.</em></p>

<h2>Virtual Private Network (VPN)</h2>

<p>If more secured connections is required between containers then VPNs can be used on VMs. This addiotional security might significantly increase processing overhead. This overhead is highly depends on which VPN solution are you going to use. In this demo we use the VPN capabilities of SSH which is not really suited for production use. In order to enable the VPN capabilites of ssh the  PermitTunnel parameter needs to be switched on in sshd_config. If you are using the Vagranfile provided to this tutorial then nothing needs to be done, since this parameter was already set up for you during provisioning in the bootstrap.sh.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#execute on vm1</span>
</span><span class='line'>sudo ssh -f -N -w 2:1 root@192.168.40.12
</span><span class='line'>sudo ifconfig tun2 up
</span><span class='line'>sudo route add -net 172.17.52.0 netmask 255.255.255.0 dev tun2
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.51.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont1  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont1)</span>
</span><span class='line'>nc -l 3333
</span><span class='line'>
</span><span class='line'><span class="c">#execute on vm2</span>
</span><span class='line'>sudo ifconfig tun1 up
</span><span class='line'>sudo route add -net 172.17.51.0 netmask 255.255.255.0 dev tun1
</span><span class='line'>sudo iptables -t nat -F POSTROUTING
</span><span class='line'>sudo iptables -t nat -A POSTROUTING -s 172.17.52.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
</span><span class='line'>sudo docker run -it --rm --name cont2  ubuntu /bin/bash
</span><span class='line'><span class="c">#Inside the container (cont2)</span>
</span><span class='line'>nc -w 1 -v 172.17.51.2 3333
</span><span class='line'><span class="c">#Result: Connection to 172.17.51.2 3333 port [tcp/*] succeeded!</span>
</span></code></pre></td></tr></table></div></figure>


<p>The ssh is launched with -w option where the numerical ids of tun devices were specified. After executing the command the tunnel interfaces are created on both VMs. The interfaces needs to be be activated with ifconfig up and after that we need to setup the rooting to direct the traffic to  172.17.51.0/24 and 172.17.52.0/24 to tun2 and tun1.</p>

<p>As mentioned the VPN capabilities of SSH is not recommended in production, but other solutions like  <a href="https://openvpn.net/index.php/open-source.html">OpenVPN</a> would worth a try to secure the communication between the hosts (and also between the containers).</p>

<h2>Conclusion</h2>

<p>The above examples were hand written mainly for demonstration purposes, but there are great tools like <a href="https://github.com/jpetazzo/pipework">Pipework</a> that can make your life simpler and will do the heavy lifting for you.</p>

<p>If you want to check how these methods are working in production environment you are just a few clicks from it, since under the hood these methods are responsible to solve the inter-container communication in our cloud agnostic Hadoop as a Service API called <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create Hadoop clusters in the cloud using a CLI]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/07/clodubreak-shell/"/>
    <updated>2014-08-07T19:43:01+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/07/clodubreak-shell</id>
    <content type="html"><![CDATA[<p>Few weeks back we have announced <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the open source Hadoop as a Service API. Included in the release we open sourced a REST API, REST client, UI and a CLI/shell. In this post we’d like to show you how easy is to use <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak shell</a> in order to create on demand Hadoop clusters on your favorite cloud provider.</p>

<p>While it’s up to everybody&rsquo;s personal preference whether to use a UI, a command line interface or the REST API directly, at SequenceIQ we prefer to use command line tools whenever it’s possible because it’s much faster than interacting with a web UI and it’s a better candidate for automation. Yeah &ndash; we have mentioned this many times &ndash; we are <code>obsessed with automation</code>; any step which is a candidate of doing it twice we script and automate it.</p>

<p>This <code>thing</code> with the automation does not affect the effort and quality standards we put on building the UI &ndash; <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> has an extremely intuitive and clean <strong>responsive</strong> UI and it’s built on the latest and greatest web UI framework &ndash; <a href="https://angularjs.org/">Angular JS</a>. We will have a post about the UI, however we consider it so simple to use that we ask you to go ahead and give it a try. You are a signup and a few clicks away from your Hadoop cluster.</p>

<p>Now back to the CLI. Remember one of our Apache contribution &ndash; the <a href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/">Ambari shell and REST API</a>? Well, the Cloudbreak shell is built on the same technology &ndash; Spring Shell. It’s an interactive shell that can be easily extended using a Spring based programming model and battle tested in various projects like Spring Roo, Spring XD, and Spring REST Shell Combine these two projects to create a powerful tool.</p>

<h2>Cloudbreak Shell</h2>

<p>The goal with the CLI was to provide an interactive command line tool which supports:</p>

<ul>
<li>all functionality available through the REST API or Cloudbreak web UI</li>
<li>makes possible complete automation of management task via <strong>scripts</strong></li>
<li>context aware command availability</li>
<li>tab completion</li>
<li>required/optional parameter support</li>
<li><strong>hint</strong> command to guide you on the usual path</li>
</ul>


<h2>Install Cloudbreak Shell</h2>

<p>You have 3 options to give it a try:</p>

<ul>
<li>use our prepared <a href="https://registry.hub.docker.com/u/sequenceiq/cloudbreak/">docker image</a></li>
<li>download the latest self-containing executable jar form our maven repo</li>
<li>build it from source</li>
</ul>


<p>We will follow up with the first two, in this post we’d like to guide you through the third option.</p>

<h3>Build from source</h3>

<p>If want to use the code or extend it with new commands follow the steps below. You will need:
&ndash; jdk 1.7
&ndash; maven 3.x.x</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/sequenceiq/cloudbreak-shell.git
</span><span class='line'>cd cloudbreak-shell
</span><span class='line'>mvn clean package</span></code></pre></td></tr></table></div></figure>




<!--more-->


<h2>Connect to Cloudbreak</h2>

<p>In order to use the shell you will have to have a Cloudbreak account. You can get one by subscribing to our hosted and free <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> instance. Alternatively you can build your own Cloudbreak and deploy it within your organization &ndash; for that just follow up with the steps in the Cloudbreak <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">documentation</a>. We suggest to try our hosted solution as in case you have any issues we can always help you with. Please feel free to create bugs, ask for enhancements or just give us feedback by either using our <a href="https://github.com/sequenceiq/cloudbreak">GitHub repository</a> or the other channels highlighted in the product documentation (Google Groups, email or social channels).
The shell is built as a single executable jar with the help of <a href="http://projects.spring.io/spring-boot/">Spring Boot</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage:
</span><span class='line'>  java -jar cloudbreak-shell-0.1-SNAPSHOT.jar                  : Starts Cloudbreak Shell in interactive mode.
</span><span class='line'>  java -jar cloudbreak-shell-0.1-SNAPSHOT.jar --cmdfile=&lt;FILE&gt; : Cloudbreak executes commands read from the file.
</span><span class='line'>
</span><span class='line'>Options:
</span><span class='line'>  --cloudbreak.host=&lt;HOSTNAME&gt;       Hostname of the Cloudbreak REST API Server [use:cloudbreak-api.sequenceiq.com].
</span><span class='line'>  --cloudbreak.port=&lt;PORT&gt;           Port of the Cloudbreak REST API Server [use:80].
</span><span class='line'>  --cloudbreak.user=&lt;USER&gt;           Username of the Cloudbreak user [use:your user name ].
</span><span class='line'>  --cloudbreak.password=&lt;PASSWORD&gt;   Password of the Cloudbreak admin [use: your password].
</span><span class='line'>
</span><span class='line'>Note:
</span><span class='line'>  All options are mandatory.</span></code></pre></td></tr></table></div></figure>


<p>Once you are connected you can start to create a cluster. If you are lost and need guidance through the process you can use <code>hint</code>. You can always use <code>TAB</code> for completion. Note that all commands are <code>context aware</code> &ndash; they are available only when it makes sense &ndash; this way you are never confused and guided by the system on the right path.</p>

<h3>Create a cloud credential</h3>

<p>In order to start using Cloudbreak you will need to have a cloud user, for example an Amazon AWS account. Note that Cloudbreak <strong>does not</strong> store you cloud user details &ndash; we work around the concept of <a href="http://aws.amazon.com/iam/">IAM</a> &ndash; on Amazon (or other cloud providers) you will have to create an IAM role, a policy and associate that with your Cloudbreak account &ndash; for further documentation please refer to the <a href="http://sequenceiq.com/cloudbreak/#accounts">documentation</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>credential createEC2 --description “description" --name “myCredentialName" --roleArn "arn:aws:iam::NUMBER:role/cloudbreak-ABC" --sshKeyUrl “URL towards your AWS public key"</span></code></pre></td></tr></table></div></figure>


<p>Alternatively you can upload your public key from a file as well, by using the <code>—sshKeyPath</code> switch. You can check whether the credential was creates successfully by using the <code>credential list</code> command. You can switch between your cloud credential &ndash; when you’d like to use one and act with that you will have to use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>credential select --id #ID of the credential</span></code></pre></td></tr></table></div></figure>


<h3>Create a template</h3>

<p>A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>template createEC2 --description "awstemplate" --name "awstemplate" --region EU_WEST_1 --instanceType M3Large --sshLocation 0.0.0.0/0 </span></code></pre></td></tr></table></div></figure>


<p>You can check whether the template was created successfully by using the <code>template list</code> command. Check the template with or select if you are happy with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>template show --id #ID of the template
</span><span class='line'>
</span><span class='line'>template select --id #ID of the template</span></code></pre></td></tr></table></div></figure>


<h3>Create a stack</h3>

<p>Stacks are template <code>instances</code> &ndash; a running cloud infrastructure created based on a template. Use the following command to create a stack to be used with your Hadoop cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>stack create --name “myStackName" --nodeCount 20 </span></code></pre></td></tr></table></div></figure>


<h3>Select a blueprint</h3>

<p>We ship default Hadoop cluster blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>blueprint list
</span><span class='line'>
</span><span class='line'>blueprint select --id #ID of the blueprint</span></code></pre></td></tr></table></div></figure>


<h3>Create a Hadoop cluster</h3>

<p>You are almost done &ndash; one more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your <code>template</code>, and by using CloudFormation will launch a cloud <code>stack</code> &ndash; once the <code>stack</code> is up and running (cloud provisioning is done) it will use your selected <code>blueprint</code> and install your custom Hadoop cluster with the selected components and services. For the supported list of Hadoop components and services please check the <a href="http://sequenceiq.com/cloudbreak/#supported-components">documentation</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cluster create --description “my cluster desc"</span></code></pre></td></tr></table></div></figure>


<p>You are done &ndash; you can check the progress through the Ambari UI. If you log back to <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak UI</a> you can check the progress over there as well, and learn the IP address of Ambari.</p>

<h3>Automate the process</h3>

<p>Each time you start the shell the executed commands are logged in a file line by line and later either with the <code>script</code> command or specifying an <code>—cmdfile</code> option the same commands can be executed again.</p>

<h2>Commands</h2>

<p>For the full list of available commands please check below. Please note that all commands are context aware, and you can always use <code>TAB</code> for command completion.</p>

<pre><code>* blueprint add - Add a new blueprint with either --url or --file
* blueprint defaults - Adds the default blueprints to Cloudbreak
* blueprint list - Shows the currently available blueprints
* blueprint select - Select the blueprint by its id
* blueprint show - Shows the blueprint by its id
* cluster create - Create a new cluster based on a blueprint and template
* cluster show - Shows the cluster by stack id
* credential createAzure - Create a new Azure credential
* credential createEC2 - Create a new EC2 credential
* credential defaults - Adds the default credentials to Cloudbreak
* credential list - Shows all of your credentials
* credential select - Select the credential by its id
* credential show - Shows the credential by its id
* exit - Exits the shell
* help - List all commands usage
* hint - Shows some hints
* quit - Exits the shell
* script - Parses the specified resource file and executes its commands
* stack create - Create a new stack based on a template
* stack list - Shows all of your stack
* stack select - Select the stack by its id
* stack show - Shows the stack by its id
* stack terminate - Terminate the stack by its id
* template create - Create a new cloud template
* template createEC2 - Create a new EC2 template
* template defaults - Adds the default templates to Cloudbreak
* template list - Shows the currently available cloud templates
* template select - Select the template by its id
* template show - Shows the template by its id
* version - Displays shell version
</code></pre>

<p>As usual for us &ndash; being committed to 100% open source &ndash; we are always open sourcing everything thus you can get the details on our <a href="https://github.com/sequenceiq/cloudbreak-shell">GitHub</a> repository.
Should you have any questions feel free to engage with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Launch Docker containers on Azure]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/04/launch-docker-containers-on-azure/"/>
    <updated>2014-08-04T19:43:01+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/04/launch-docker-containers-on-azure</id>
    <content type="html"><![CDATA[<p>Two weeks ago we have open sourced our cloud agnostic and Docker based Hadoop as a Service API &ndash; called <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a>.
The first public beta version supports Amazon’s AWS and Microsoft’s Azure, while we are already wrapping up a few new cloud provider integrations.</p>

<p>While there is some documentation about running Docker containers on Amazon, there is no detailed description about running Docker on the Azure cloud.
With this blog post we would like to shed some light on it &ndash; recently there have been lots of announcements from Microsoft about Docker support (Azure CLI, Kubernetes, libswarm) but they are either not finished yet or are not ready to build a robust platform on top.
We are eagerly waiting for the <a href="http://azure.microsoft.com/blog/2014/07/10/azure-collaboration-with-google-and-docker/">Kubernetes integration</a>.</p>

<p>In the meantime, if you are interested in running a <code>cluster</code> of Docker container, or do some more complex stuff then read on.</p>

<p>Just to briefly recap &ndash; with Cloudbreak we are launching on demand Hadoop clusters (check our <a href="http://blog.sequenceiq.com/blog/2014/07/25/cloudbreak-technology/">blog</a> for further technical details) in Docker containers. These containers are <code>shipped</code> to different cloud VMsm and dynamically find and join each other &ndash; they form a fully functional Hadoop cluster without the need to do anything manually on the host, or apply any manual pre-configuration.
So how are we doing this?</p>

<!--more-->


<h3>Docker ready base VM image</h3>

<p>First of all you need a base image with Docker installed &ndash; thus for that we have built and made available an Ubuntu 14.04 image with Docker installed. Apart from Docker, to build a fully dynamic and <code>service discovery</code> aware Docker cluster we needed <a href="http://stedolan.github.io/jq/">jq</a> and <a href="http://www.linuxfromscratch.org/blfs/view/svn/basicnet/bridge-utils.html"> bridge-utils</a>.</p>

<p>Once this base image is created you will need to make it public and re-usable. In order to do that the image has to be published in <a href="http://vmdepot.msopentech.com/List/Index">VMdepot</a>. When you are about to use an image from VM depot, and create a VM based on that you will need to copy it in your own storage account &ndash; note that doing it at first time this can be a slow process (20-25 minutes, copying the 30 GB image).</p>

<h3>Dynamic networking</h3>

<p>Now you have an image based on that you can launch your own VMs, and the Docker container inside your VM. While there are a few options to do that, we needed to find a unified way to do so &ndash; note that  <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a> is a cloud agnostic solution &ndash; and we do not want to create init scripts for each and every cloud environment we use. Amazon’s AWS has a feature so called <code>userdata</code> &ndash; an option of passing data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon AWS: shell scripts and cloud-init directives. In order to keep the launch process unified everywhere we are using <a href="https://help.ubuntu.com/community/CloudInit">cloud-init</a> on Azure as well.</p>

<p>You can use/start Docker with different networking setup &ndash; using a bridged network or using the host network. You can check the init scripts in our <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/resources/azure-init.sh">GitHub</a> repository.</p>

<p><strong>Bridged network</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># set bridge0 in docker opts</span>
</span><span class='line'>sh -c <span class="s2">&quot;cat &gt; /etc/default/docker&quot;</span> &lt;&lt;<span class="s2">&quot;EOF&quot;</span>
</span><span class='line'><span class="nv">DOCKER_OPTS</span><span class="o">=</span><span class="s2">&quot;-b bridge0 -H unix:// -H tcp://0.0.0.0:2375&quot;</span>
</span><span class='line'>EOF
</span><span class='line'>
</span><span class='line'><span class="nv">CMD</span><span class="o">=</span><span class="s2">&quot;docker run -d -p SOURCE_PORT:DESTINATION_PORT 0 -e SERF_JOIN_IP=$SERF_JOIN_IP -e SERF_ADVERTISE_IP=$MY_IP --dns 127.0.0.1 --name ${NODE_PREFIX}${INSTANCE_IDX} -h ${NODE_PREFIX}${INSTANCE_IDX}.${MYDOMAIN} --entrypoint /usr/local/serf/bin/start-serf-agent.sh  $IMAGE $AMBARI_ROLE&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p><strong>Host network</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">CMD</span><span class="o">=</span><span class="s2">&quot;docker run -d -e SERF_JOIN_IP=$AMBARI_SERVER_IP --net=host --name ${NODE_PREFIX}${INSTANCE_IDX} --entrypoint /usr/local/serf/bin/start-serf-agent.sh  $IMAGE $AMBARI_ROLE&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p><em>Note: for cloud based clusters we are giving up on the bridged based network &ndash; mostly due to Azure&rsquo;s networking limitations &ndash; and will use the <code>net=host</code> solution in the next release. The bridged network will still be a supported solution, though we are using it mostly with bare metal or multi container/host solutions.</em></p>

<p>Azure has (comparing with Amazon’s AWS or Google’s Cloud compute) an <code>uncommon</code> network setup and supports limited flexibility &ndash; in order to overcome these, and still have a dynamic Hadoop cluster different scenarios / use cases requires different Docker networking &ndash; that is quite a large <strong>undocumented</strong> topic which we will cover in our next blog posts &ndash; in particular the issues, differences and solutions to use Docker on different cloud providers. While we have briefly talked about <a href="http://sequenceiq.com/cloudbreak/#technology">Serf</a> in the <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> documentation, we will enter in deep technical details in one of our next posts as well. Should you be interested in these, make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a> for updates.</p>

<h3>SequenceIQ’s Azure REST API &ndash; open sourced</h3>

<p>At <a href="htp://sequenceiq.com">SequenceIQ</a> we always automate everything &ndash; and in order to launch VM instances, configure networks, start containers, etc we needed a REST client which we can use it from our JAVA and Scala codebase. Since the Microsoft API is XML based &ndash; <em>yo, it’s 2014</em> &ndash; we have created and open sourced a Groovy based <a href="https://github.com/sequenceiq/azure-rest-client">Azure REST API</a> &ndash; wrapping the XML calls into a nice, easy to use and clean REST API. Feel free to use it &ndash; it’s open sourced under an Apache 2 license. Note that <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> does not store your Azure user credential &ndash; whereas with the defulat Azue CLI that would have been possible &ndash; the only thing we need from your side to work is your subscription id. The process is documented here: <a href="http://sequenceiq.com/cloudbreak/#accounts.">http://sequenceiq.com/cloudbreak/#accounts.</a></p>

<h3>Metadata service for Azure</h3>

<p>The another nice feature we have created for Azure VMs is a <code>metadata service</code>. While a service as such does exists on Amazon’s AWS it’s missing from Microsoft Azure &ndash; note that our Cloudbreak solution is a cloud agnostic one, and we always strive to use identical solution on all cloud providers. The instance metadata is data about your instance that you can use to configure or manage the running instances &ndash; and available via a REST call. We have developed a service as such for Azure &ndash; <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/java/com/sequenceiq/cloudbreak/service/stack/connector/azure/AzureMetadataSetup.java">AzureMetadataSetup</a>. As you can see we collect the metadata, and make it available under a <code>unique hash</code> for each cluster by calling the following resource: <code>/metadata/{hash}</code></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">private</span> <span class="n">Set</span><span class="o">&lt;</span><span class="n">CoreInstanceMetaData</span><span class="o">&gt;</span> <span class="nf">collectMetaData</span><span class="o">(</span><span class="n">Stack</span> <span class="n">stack</span><span class="o">,</span> <span class="n">AzureClient</span> <span class="n">azureClient</span><span class="o">,</span> <span class="n">String</span> <span class="n">name</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="o">...</span> <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>                <span class="n">CoreInstanceMetaData</span> <span class="n">instanceMetaData</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CoreInstanceMetaData</span><span class="o">(</span><span class="n">vmName</span><span class="o">,</span>
</span><span class='line'>                        <span class="n">getPrivateIP</span><span class="o">((</span><span class="n">String</span><span class="o">)</span> <span class="n">virtualMachine</span><span class="o">),</span>
</span><span class='line'>                        <span class="n">getVirtualIP</span><span class="o">((</span><span class="n">String</span><span class="o">)</span> <span class="n">virtualMachine</span><span class="o">));</span>
</span><span class='line'>                <span class="n">instanceMetaDatas</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">instanceMetaData</span><span class="o">);</span>
</span><span class='line'>            <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">IOException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span> <span class="o">...</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This service is used in a few cases &ndash; for example to learn different network setups as the hosts are using different network options than the Docker containers.</p>

<p>As usual for us &ndash; being committed to 100% open source &ndash; we are always open sourcing everything thus you can get the details on our <a href="https://github.com/sequenceiq/cloudbreak">GitHub</a> repository.
Should you have any questions feel free to engage with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark - MLlib Introduction]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/31/spark-mllib/"/>
    <updated>2014-07-31T07:47:32+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/31/spark-mllib</id>
    <content type="html"><![CDATA[<h3>Introduction</h3>

<p>In one of our earlier posts we have mentioned that we use Scalding (among others) for writing MR jobs. Scala/Scalding simplifies the implementation of many MR patterns and makes it easy to implement quite complex jobs like machine learning algorithms. Map Reduce is a mature and widely used framework and it is a good choice for processing large amounts of data &ndash; but not as great if you’d like to use it for fast iterative algorithms/processing. This is a use case where <a href="https://spark.apache.org/">Apache Spark</a> can be quite handy. Spark is fit for these kind of algorithms, because it tries to keep everything in memory (in case of you run out of memory, you can switch to another <a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence">storage levels</a>).</p>

<h3>Apache Spark &ndash; MLlib library</h3>

<p><a href="https://spark.apache.org/docs/latest/mllib-guide.html">MLlib</a> is a machine learning library which ships with Apache Spark, and can run on any Hadoop2/YARN cluster without any pre-installation. At SequenceIQ we use MLlib in Scala &ndash; but you could use it from Java and Python as well. Let us quickly show you an MLlib clustering algorithm with code examples.</p>

<h3>KMeans example</h3>

<p>K-Means (Lloyd&rsquo;s algorithm) is a simple NP-hard unsupervised learning algorithm that solve well known clustering problems. The essence of the algorithm is to separate your data into K cluster. In simple terms it needs 4 steps. First of all you have to vectorize your data. (you can do that with text values too). The code looks like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="n">context</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="n">input</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">line</span> <span class="k">=&gt;</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="sc">&#39;,&#39;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toDouble</span><span class="o">))</span>
</span><span class='line'><span class="o">}.</span><span class="n">cache</span><span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>The second step is to choose K center points (centroids). The third one is to assign each vector to the group that has the closest centroid. After all this is done, next thing you will need to do is to recalculate the positions of the centroids. You have to repeat the third and fourth steps until the centroids are not moving (<code>the iterative stuff</code>). The <a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/clustering/KMeans.scala">KMeans</a> MLlib model is doing that for you (2-3-4 steps centroid delta checks).</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">clusters</span><span class="k">:</span> <span class="kt">KMeansModel</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="o">.</span><span class="n">train</span><span class="o">(</span><span class="n">data</span><span class="o">,</span> <span class="n">K</span><span class="o">,</span> <span class="n">maxIteration</span><span class="o">,</span> <span class="n">runs</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">vectorsAndClusterIdx</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="n">point</span> <span class="k">=&gt;</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">prediction</span> <span class="k">=</span> <span class="n">clusters</span><span class="o">.</span><span class="n">predict</span><span class="o">(</span><span class="n">point</span><span class="o">)</span>
</span><span class='line'>  <span class="o">(</span><span class="n">point</span><span class="o">.</span><span class="n">toString</span><span class="o">,</span> <span class="n">prediction</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>After you have your model result, you can utilize it in your RDD object.</p>

<h3>Running Spark job on YARN</h3>

<p>In order to run this Spark application on YARN first of all you will need a Hadoop YARN cluster. For that you could use our Hadoop as a Service API called <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a> &ndash; using a <code>multi-node-hdfs-yarn</code> blueprint will set you up a Spark ready Hadoop cluster in less than 2 minutes on your favorite cloud provider. Give it a try at our hosted <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> instance.</p>

<p>Once your cluster it’s up and ready you can run the following command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./bin/spark-submit --class com.sequenceiq.spark.Main --master <span class="se">\</span>
</span><span class='line'>yarn-client --driver-memory 1g --executor-memory 1g --executor-cores 1 <span class="se">\</span>
</span><span class='line'>/root/spark-clustering-1.0.jar hdfs://sandbox:9000/input/input.txt /output 10 10 1
</span></code></pre></td></tr></table></div></figure>


<p>Alternatively you can run this in our free Docker based Apache Spark container as well. You can get a Spark container from the official <a href="https://registry.hub.docker.com/u/sequenceiq/spark/">Docker registry</a> or from our <a href="https://github.com/sequenceiq/docker-spark">GitHub</a> repository.
As always we are making the source code available at <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/spark-clustering">SequenceIQ&rsquo;s GitHub repository</a> (check the other interesting examples as well).  You can find 2 simple input datasets for testing purposes.</p>

<p>The result of the clustering looks like this (generated with R):</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/spark-clustering/data/spark-clustering_1.jpeg" alt="" /></p>

<p>While there is a loud buzz about what’s faster than the other and there are huge numbers thrown in as the <em>X</em> multiplier factor we don’t really want to enter that game &ndash; as a fact we’d like to mention that both example performs better than Mahout KMeans (2-3x faster with 20 iterations), but these are really small datasets. We have seen larger datasets in production where the performances are quite the same, or can go the other way (especially that Spark is new and people don’t always get the configuration right).</p>

<p>In one of our next post we will show you metrics for a much larger dataset and other ML algorithms &ndash; follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a> for updates.</p>

<h3>Apache Tez</h3>

<p>We can’t finish this blog post before not talking about <a href="http://tez.apache.org/">Apache Tez</a> &ndash; the project is aimed at building an application framework which allows for a complex directed-acyclic-graph of tasks for processing data &ndash; fast. We (and many others) believe that this can be a good alternative for Spark &ndash; especially for machine learning. The number of frameworks which are adding or moving the MR runtime to Tez is increasing &ndash; among the few to mention are Cascading, Summingbird, Conjecture &ndash; including us as well.</p>

<p>Note that Apache Tez has already showed <strong>awesome</strong> result. Being the key building block of the <a href="http://hortonworks.com/labs/stinger/">Stinger inititive</a> &ndash; led by Hortonworks &ndash; managed to bring near real time queries and speed up Hive with 100x.</p>

<h3>Other promising machine learning frameworks</h3>

<p>If you are interested in machine learning frameworks, you have to check  <a href="https://github.com/etsy/Conjecture">Conjecture</a> or <a href="https://github.com/tresata/ganitha">ganitha</a> &ndash; they both show great fueatures and have promising results.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker ships Hadoop to the cloud]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/25/cloudbreak-technology/"/>
    <updated>2014-07-25T12:56:39+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/25/cloudbreak-technology</id>
    <content type="html"><![CDATA[<p>A week ago we have <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">announced</a> and open sourced <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a>, the first Docker based Hadoop as a Service API. In this post we&rsquo;d like to introduce you into the technical details and the building blocks of the architecture.
Cloudbreak is built on the foundation of cloud providers APIs, Apache Ambari, Docker containers, Serf and dnsmasq. It is a cloud agnostic solution &ndash; as all the Hadoop services and components are running inside Docker containers &ndash; and these containers are shipped across different cloud providers.</p>

<p>Cloudbreak product documentation: <a href="http://sequenceiq.com/cloudbreak">http://sequenceiq.com/cloudbreak</a></p>

<p>Cloudbreak API documentation: <a href="http://docs.cloudbreak.apiary.io/">http://docs.cloudbreak.apiary.io/</a></p>

<h2>How it works</h2>

<p>From Docker containers point of view we have two kind of containers &ndash; based on their Ambari role &ndash; server and agent. There is one Docker container running the Ambari server, and there are many Docker containers running the Ambari agents. The used Docker image is always the same: <code>sequenceiq/ambari</code> and
the Ambari role is decided based on the <code>$AMBARI_ROLE</code> variable.</p>

<p>For example on Amazon EC2 this is how we start the containers:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>docker run -d -p &lt;LIST of ports&gt; -e <span class="nv">SERF_JOIN_IP</span><span class="o">=</span><span class="nv">$SERF_JOIN_IP</span> --dns 127.0.0.1 --name <span class="k">${</span><span class="nv">NODE_PREFIX</span><span class="k">}${</span><span class="nv">INSTANCE_IDX</span><span class="k">}</span> -h <span class="k">${</span><span class="nv">NODE_PREFIX</span><span class="k">}${</span><span class="nv">INSTANCE_IDX</span><span class="k">}</span>.<span class="k">${</span><span class="nv">MYDOMAIN</span><span class="k">}</span> --entrypoint /usr/local/serf/bin/start-serf-agent.sh  <span class="nv">$IMAGE</span> <span class="nv">$AMBARI_ROLE</span>
</span></code></pre></td></tr></table></div></figure>


<p>As we are starting up the instances and the Docker containers on the host, we&rsquo;d like them to join each other and be able to communicate &ndash; though we don&rsquo;t know the IP addresses beforehand. This can be challanging on cloud environments &ndash; where your IP address and DNS name is dynamically allocated &ndash; however you don&rsquo;t want to collect these imformations beforehand launching the Docker containers.
For that we use Serf &ndash; and pass along the IP address <code>SERF_JOIN_IP=$SERF_JOIN_IP</code> of the first container. Using a gossip protocol Serf will automatically discover each other, set the DNS names, and configure the routing between the nodes.
Serf reconfigures the DNS server <code>dnsmasq</code> running inside the container, and keeps it up to date with the joining or leaving nodes information.
As you can see at startup we always pass a <code>--dns 127.0.0.1</code> dns server for the container to use.</p>

<p>As you see there is no cloud specific code at the Docker containers level, the same technology can be used on bare metal as well.
Check our previous blog posts about a <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">multi node Hadoop cluster on any host</a>.</p>

<p>Obliviously there is some configuration on the host as well &ndash; for that and to handle early initialization of a cloud instance we use <a href="https://help.ubuntu.com/community/CloudInit">CloudInit</a>. We will write a blog post about these for every cloud provider we support.</p>

<p>For additional information you can check our slides from the <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">Hadoop Summit 2014</a>.</p>

<p>Once Ambari is started it will install the selected components based on the passed Hadoop blueprint &ndash; and start the desired services.</p>

<!-- more -->


<h2>Used Technologies</h2>

<h3>Apache Ambari</h3>

<p>The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/ambari-overview.png" alt="" /></p>

<p>Ambari enables System Administrators to:</p>

<ol>
<li>Provision a Hadoop Cluster</li>
<li>provides a step-by-step wizard for installing Hadoop services across any number of hosts.</li>
<li><p>handles configuration of Hadoop services for the cluster.</p></li>
<li><p>Manage a Hadoop Cluster</p></li>
<li><p>provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.</p></li>
<li><p>Monitor a Hadoop Cluster</p></li>
<li>provides a dashboard for monitoring health and status of the Hadoop cluster.</li>
<li>leverages Ganglia for metrics collection.</li>
<li>leverages Nagios for system alerting and will send emails when your attention is needed (e.g. a node goes down, remaining disk space is low, etc).</li>
</ol>


<p>Ambari enables to integrate Hadoop provisioning, management and monitoring capabilities into applications with the Ambari REST APIs.
Ambari Blueprints are a declarative definition of a cluster. With a Blueprint, you can specify a Stack, the Component layout and the Configurations to materialize a Hadoop cluster instance (via a REST API) without having to use the Ambari Cluster Install Wizard.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/ambari-create-cluster.png" alt="" /></p>

<h3>Docker</h3>

<p>Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications. Consisting of Docker Engine, a portable, lightweight runtime and packaging tool, and Docker Hub, a cloud service for sharing applications and automating workflows, Docker enables apps to be quickly assembled from components and eliminates the friction between development, QA, and production environments. As a result, IT can ship faster and run the same app, unchanged, on laptops, data center VMs, and any cloud.</p>

<p>The main features of Docker are:</p>

<ol>
<li>Lightweight, portable</li>
<li>Build once, run anywhere</li>
<li>VM &ndash; without the overhead of a VM</li>
<li>Each virtualized application includes not only the application and the necessary binaries and libraries, but also an entire guest operating system</li>
<li><p>The Docker Engine container comprises just the application and its dependencies. It runs as an isolated process in userspace on the host operating system, sharing the kernel with other containers.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/vm.png" alt="" /></p></li>
<li><p>Containers are isolated</p></li>
<li>It can be automated and scripted</li>
</ol>


<h3>Serf</h3>

<p>Serf is a tool for cluster membership, failure detection, and orchestration that is decentralized, fault-tolerant and highly available. Serf runs on every major platform: Linux, Mac OS X, and Windows. It is extremely lightweight.
Serf uses an efficient gossip protocol to solve three major problems:</p>

<ul>
<li><p>Membership: Serf maintains cluster membership lists and is able to execute custom handler scripts when that membership changes. For example, Serf can maintain the list of Hadoop servers of a cluster and notify the members when nodes come online or go offline.</p></li>
<li><p>Failure detection and recovery: Serf automatically detects failed nodes within seconds, notifies the rest of the cluster, and executes handler scripts allowing you to handle these events. Serf will attempt to recover failed nodes by reconnecting to them periodically.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/serf-gossip.png" alt="" /></p></li>
<li><p>Custom event propagation: Serf can broadcast custom events and queries to the cluster. These can be used to trigger deploys, propagate configuration, etc. Events are simple fire-and-forget broadcast, and Serf makes a best effort to deliver messages in the face of offline nodes or network partitions. Queries provide a simple realtime request/response mechanism.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/serf-event.png" alt="" /></p></li>
</ul>


<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YARN Schedulers demystified - Part 1: Capacity]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/22/schedulers-part-1/"/>
    <updated>2014-07-22T12:56:39+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/22/schedulers-part-1</id>
    <content type="html"><![CDATA[<p>After our first <a href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues/">post</a> about re-prioritizing already submitted and running jobs on different queues we have received many questions and feedbacks about the Capacity Scheduler internals. While there is <code>some</code> documentation available, there is no extensive and deep documentation about how it actually works internally. Since it&rsquo;s all event based it&rsquo;s pretty hard to understand the flow &ndash; let alone debugging it. At <a href="http://sequenceiq.com/">SequenceIQ</a> we are working on a heuristic cluster scheduler &ndash; and understanding how YARN schedulers work was essential. This is part of a larger piece of work &ndash; which will lead to a fully dynamic Hadoop cluster &ndash; orchestrating <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the first open source and Docker based <strong>Hadoop as a Service API</strong>. As usual for us, this work and what we have already done around Capacity and Fair schedulers will be open sourced (or already contributed back to Apache YARN project).</p>

<h2>The Capacity Scheduler internals</h2>

<p>The CapacityScheduler is the default scheduler used with Hadoop 2.x. Its purpose is to allow multi-tenancy and share resources between multiple organizations and applications on the same cluster. You can read about the high level abstraction
<a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">here</a>. In this blog entry we&rsquo;ll examine it
from a deep technical point of view (the implementation can be found <a href="https://github.com/apache/hadoop-common/tree/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity">here</a>
as part of the ResourceManager). I&rsquo;ll try to keep it short and deal with the most important aspects, preventing to write a book about it (would be still better than twilight).</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-queue-tests/src/main/resources/event-flow.gif"></p>

<p>The animation shows a basic application submission event flow, but don&rsquo;t worry if you don&rsquo;t understand it yet, hopefully you will when you&rsquo;re done with the reading.</p>

<!-- more -->


<h2>Configuration</h2>

<p>It all begins with the configuration. The scheduler consists of a queue hierarchy, something like this
(except it’s xml ah.. capacity-scheduler.xml):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yarn.scheduler.capacity.maximum-am-resource-percent=0.2
</span><span class='line'>yarn.scheduler.capacity.maximum-applications=10000
</span><span class='line'>yarn.scheduler.capacity.node-locality-delay=40
</span><span class='line'>yarn.scheduler.capacity.root.acl_administer_queue=*
</span><span class='line'>yarn.scheduler.capacity.root.capacity=100
</span><span class='line'>yarn.scheduler.capacity.root.default.acl_administer_jobs=*
</span><span class='line'>yarn.scheduler.capacity.root.default.acl_submit_applications=*
</span><span class='line'>yarn.scheduler.capacity.root.default.capacity=80
</span><span class='line'>yarn.scheduler.capacity.root.default.maximum-capacity=80
</span><span class='line'>yarn.scheduler.capacity.root.default.state=RUNNING
</span><span class='line'>yarn.scheduler.capacity.root.default.user-limit-factor=1
</span><span class='line'>yarn.scheduler.capacity.root.low.acl_administer_jobs=*
</span><span class='line'>yarn.scheduler.capacity.root.low.acl_submit_applications=*
</span><span class='line'>yarn.scheduler.capacity.root.low.capacity=20
</span><span class='line'>yarn.scheduler.capacity.root.low.maximum-capacity=40
</span><span class='line'>yarn.scheduler.capacity.root.low.state=RUNNING
</span><span class='line'>yarn.scheduler.capacity.root.low.user-limit-factor=1
</span><span class='line'>yarn.scheduler.capacity.root.queues=default,low</span></code></pre></td></tr></table></div></figure>


<p><img src="http://yuml.me/9d7e9977"></p>

<p>Generally, queues are for to prevent applications to consume more resources then they should.
Be careful when determining the capacities, because if you mess it up the <code>ResourceManager</code> won&rsquo;t start:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Service RMActiveServices failed in state INITED cause: java.lang.IllegalArgumentException: Illegal capacity of 1.1 for children of queue root.</span></code></pre></td></tr></table></div></figure>


<p>The <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java#L255">initScheduler</a>
will parse the configuration file and create either <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java">parent</a>
or <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java">leaf</a>
queues and compute their capabilities.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>capacity.LeafQueue (LeafQueue.java:setupQueueConfigs(312)) - Initializing default
</span><span class='line'>capacity = 0.8 [= (float) configuredCapacity / 100 ]
</span><span class='line'>asboluteCapacity = 0.8 [= parentAbsoluteCapacity * capacity ]
</span><span class='line'>maxCapacity = 0.8 [= configuredMaxCapacity ]
</span><span class='line'>absoluteMaxCapacity = 0.8 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
</span><span class='line'>userLimit = 100 [= configuredUserLimit ]
</span><span class='line'>userLimitFactor = 1.0 [= configuredUserLimitFactor ]
</span><span class='line'>maxApplications = 8000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
</span><span class='line'>maxApplicationsPerUser = 8000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
</span><span class='line'>maxActiveApplications = 1 [= max((int)ceil((clusterResourceMemory / minimumAllocation) * maxAMResourcePerQueuePercent * absoluteMaxCapacity),1) ]
</span><span class='line'>maxActiveAppsUsingAbsCap = 1 [= max((int)ceil((clusterResourceMemory / minimumAllocation) *maxAMResourcePercent * absoluteCapacity),1) ]
</span><span class='line'>maxActiveApplicationsPerUser = 1 [= max((int)(maxActiveApplications * (userLimit / 100.0f) * userLimitFactor),1) ]
</span><span class='line'>usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
</span><span class='line'>absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
</span><span class='line'>maxAMResourcePerQueuePercent = 0.2 [= configuredMaximumAMResourcePercent ]
</span><span class='line'>minimumAllocationFactor = 0.75 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
</span><span class='line'>numContainers = 0 [= currentNumContainers ]
</span><span class='line'>state = RUNNING [= configuredState ]
</span><span class='line'>acls = ADMINISTER_QUEUE: SUBMIT_APPLICATIONS:* [= configuredAcls ]
</span><span class='line'>nodeLocalityDelay = 40</span></code></pre></td></tr></table></div></figure>


<p>Although it does not imply, but application submission is only allowed to leaf queues. By default all application is submitted to
a queue called <code>default</code>. One interesting property is the <code>schedule-asynchronously</code> about which I&rsquo;ll talk later.</p>

<h2>Messaging</h2>

<p>Once the ResourceManager is up and running, the messaging starts. Mostly everything happens via events. These events are distributed with
a <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java">dispatcher</a>
among the registered event handlers. The down side of the event driven architecture that it&rsquo;s hard to follow the flow because
events can come from everywhere. The CapacityScheduler itself is registered for many events, and act based on these events.
Code snippets are from branch <code>trunk</code> aka <code>3.0.0-SNAPSHOT</code>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'> <span class="nd">@Override</span>
</span><span class='line'>  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">handle</span><span class="o">(</span><span class="n">SchedulerEvent</span> <span class="n">event</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">switch</span><span class="o">(</span><span class="n">event</span><span class="o">.</span><span class="na">getType</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">NODE_ADDED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">NodeAddedSchedulerEvent</span> <span class="n">nodeAddedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">NodeAddedSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">addNode</span><span class="o">(</span><span class="n">nodeAddedEvent</span><span class="o">.</span><span class="na">getAddedRMNode</span><span class="o">());</span>
</span><span class='line'>      <span class="n">recoverContainersOnNode</span><span class="o">(</span><span class="n">nodeAddedEvent</span><span class="o">.</span><span class="na">getContainerReports</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">nodeAddedEvent</span><span class="o">.</span><span class="na">getAddedRMNode</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">NODE_REMOVED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">NodeRemovedSchedulerEvent</span> <span class="n">nodeRemovedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">NodeRemovedSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">removeNode</span><span class="o">(</span><span class="n">nodeRemovedEvent</span><span class="o">.</span><span class="na">getRemovedRMNode</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">NODE_UPDATE:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">NodeUpdateSchedulerEvent</span> <span class="n">nodeUpdatedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">NodeUpdateSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">RMNode</span> <span class="n">node</span> <span class="o">=</span> <span class="n">nodeUpdatedEvent</span><span class="o">.</span><span class="na">getRMNode</span><span class="o">();</span>
</span><span class='line'>      <span class="n">nodeUpdate</span><span class="o">(</span><span class="n">node</span><span class="o">);</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(!</span><span class="n">scheduleAsynchronously</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">allocateContainersToNode</span><span class="o">(</span><span class="n">getNode</span><span class="o">(</span><span class="n">node</span><span class="o">.</span><span class="na">getNodeID</span><span class="o">()));</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_ADDED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">AppAddedSchedulerEvent</span> <span class="n">appAddedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">AppAddedSchedulerEvent</span><span class="o">)</span> <span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">addApplication</span><span class="o">(</span><span class="n">appAddedEvent</span><span class="o">.</span><span class="na">getApplicationId</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAddedEvent</span><span class="o">.</span><span class="na">getQueue</span><span class="o">(),</span> <span class="n">appAddedEvent</span><span class="o">.</span><span class="na">getUser</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_REMOVED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">AppRemovedSchedulerEvent</span> <span class="n">appRemovedEvent</span> <span class="o">=</span> <span class="o">(</span><span class="n">AppRemovedSchedulerEvent</span><span class="o">)</span><span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">doneApplication</span><span class="o">(</span><span class="n">appRemovedEvent</span><span class="o">.</span><span class="na">getApplicationID</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appRemovedEvent</span><span class="o">.</span><span class="na">getFinalState</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_ATTEMPT_ADDED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">AppAttemptAddedSchedulerEvent</span> <span class="n">appAttemptAddedEvent</span> <span class="o">=</span>
</span><span class='line'>          <span class="o">(</span><span class="n">AppAttemptAddedSchedulerEvent</span><span class="o">)</span> <span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">addApplicationAttempt</span><span class="o">(</span><span class="n">appAttemptAddedEvent</span><span class="o">.</span><span class="na">getApplicationAttemptId</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAttemptAddedEvent</span><span class="o">.</span><span class="na">getTransferStateFromPreviousAttempt</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAttemptAddedEvent</span><span class="o">.</span><span class="na">getShouldNotifyAttemptAdded</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">APP_ATTEMPT_REMOVED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">AppAttemptRemovedSchedulerEvent</span> <span class="n">appAttemptRemovedEvent</span> <span class="o">=</span>
</span><span class='line'>          <span class="o">(</span><span class="n">AppAttemptRemovedSchedulerEvent</span><span class="o">)</span> <span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">doneApplicationAttempt</span><span class="o">(</span><span class="n">appAttemptRemovedEvent</span><span class="o">.</span><span class="na">getApplicationAttemptID</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAttemptRemovedEvent</span><span class="o">.</span><span class="na">getFinalAttemptState</span><span class="o">(),</span>
</span><span class='line'>        <span class="n">appAttemptRemovedEvent</span><span class="o">.</span><span class="na">getKeepContainersAcrossAppAttempts</span><span class="o">());</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">case</span> <span class="nl">CONTAINER_EXPIRED:</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="n">ContainerExpiredSchedulerEvent</span> <span class="n">containerExpiredEvent</span> <span class="o">=</span>
</span><span class='line'>          <span class="o">(</span><span class="n">ContainerExpiredSchedulerEvent</span><span class="o">)</span> <span class="n">event</span><span class="o">;</span>
</span><span class='line'>      <span class="n">ContainerId</span> <span class="n">containerId</span> <span class="o">=</span> <span class="n">containerExpiredEvent</span><span class="o">.</span><span class="na">getContainerId</span><span class="o">();</span>
</span><span class='line'>      <span class="n">completedContainer</span><span class="o">(</span><span class="n">getRMContainer</span><span class="o">(</span><span class="n">containerId</span><span class="o">),</span>
</span><span class='line'>          <span class="n">SchedulerUtils</span><span class="o">.</span><span class="na">createAbnormalContainerStatus</span><span class="o">(</span>
</span><span class='line'>              <span class="n">containerId</span><span class="o">,</span>
</span><span class='line'>              <span class="n">SchedulerUtils</span><span class="o">.</span><span class="na">EXPIRED_CONTAINER</span><span class="o">),</span>
</span><span class='line'>          <span class="n">RMContainerEventType</span><span class="o">.</span><span class="na">EXPIRE</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="k">break</span><span class="o">;</span>
</span><span class='line'>    <span class="k">default</span><span class="o">:</span>
</span><span class='line'>      <span class="n">LOG</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="s">&quot;Invalid eventtype &quot;</span> <span class="o">+</span> <span class="n">event</span><span class="o">.</span><span class="na">getType</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;. Ignoring!&quot;</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>NODE_ADDED</h3>

<p>Each time a node joins the cluster the <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java#L237">ResourceTrackerService</a>
registers the <code>NodeManager</code> and as part of the transition sends a <code>NodeAddedSchedulerEvent</code>. The scheduler keeps track of
the global cluster resources and adds the node&rsquo;s resources to the global.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">Added</span> <span class="n">node</span> <span class="n">amb1</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">:</span><span class="mi">45454</span> <span class="nl">clusterResource:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">5120</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">8</span><span class="o">&gt;</span>
</span><span class='line'><span class="n">Added</span> <span class="n">node</span> <span class="n">amb2</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">:</span><span class="mi">45454</span> <span class="nl">clusterResource:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">10240</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">16</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>It is also needed to update all the queue metrics since the cluster got bigger, thus the queue capacities also change. More likely to happen
that a new application can be scheduled. If the <code>isWorkPreservingRecoveryEnabled</code> is enabled on the <code>ResourceManager</code> it can recover
containers on a re-joining node.</p>

<h3>NODE_REMOVED</h3>

<p>There can be many reasons that a node is being removed from the cluster, but the scenario is almost the same as adding one. A
<code>NodeRemovedSchedulerEvent</code> is sent and the scheduler subtracts the node&rsquo;s resources from the global and updates all the queue metrics.
Things can be a little bit complicated since the node was active part of the resource scheduling and can have running containers and
reserved resources. The scheduler will kill these containers and notify the applications so they can request new containers and
unreserve the resources.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">rmnode</span><span class="o">.</span><span class="na">RMNodeImpl</span> <span class="o">(</span><span class="n">RMNodeImpl</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">transition</span><span class="o">(</span><span class="mi">569</span><span class="o">))</span> <span class="o">-</span> <span class="n">Deactivating</span> <span class="n">Node</span> <span class="n">amb4</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">:</span><span class="mi">45454</span> <span class="n">as</span> <span class="n">it</span> <span class="n">is</span> <span class="n">now</span> <span class="n">DECOMMISSIONED</span>
</span><span class='line'><span class="n">rmnode</span><span class="o">.</span><span class="na">RMNodeImpl</span> <span class="o">(</span><span class="n">RMNodeImpl</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">handle</span><span class="o">(</span><span class="mi">385</span><span class="o">))</span> <span class="o">-</span> <span class="n">amb4</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">:</span><span class="mi">45454</span> <span class="n">Node</span> <span class="n">Transitioned</span> <span class="n">from</span> <span class="n">RUNNING</span> <span class="n">to</span> <span class="n">DECOMMISSIONED</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">CapacityScheduler</span> <span class="o">(</span><span class="n">CapacityScheduler</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">removeNode</span><span class="o">(</span><span class="mi">980</span><span class="o">))</span> <span class="o">-</span> <span class="n">Removed</span> <span class="n">node</span> <span class="n">amb4</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">:</span><span class="mi">45454</span> <span class="nl">clusterResource:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">15360</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">24</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>APP_ADDED</h3>

<p>On application <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java#L266">submission</a>
an <code>AppAddedSchedulerEvent</code> is made and the scheduler will decide to accept the application or not. It depends whether it
was submitted to a leaf queue and the user have the appropriate rights (ACL) to submit to this queue and the queue can have more applications. If
any of these fails the scheduler will reject the application by sending an <code>RMAppRejectedEvent</code>. Otherwise it will register a new
<code>SchedulerApplication</code> and notify the target queue&rsquo;s parents about it and updates the queue metrics.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">ParentQueue</span> <span class="o">(</span><span class="n">ParentQueue</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">addApplication</span><span class="o">(</span><span class="mi">495</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">added</span> <span class="o">-</span> <span class="nl">appId:</span> <span class="n">application_1405323437551_0001</span> <span class="nl">user:</span> <span class="n">hdfs</span> <span class="n">leaf</span><span class="o">-</span><span class="n">queue</span> <span class="n">of</span> <span class="nl">parent:</span> <span class="n">root</span> <span class="err">#</span><span class="nl">applications:</span> <span class="mi">1</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">CapacityScheduler</span> <span class="o">(</span><span class="n">CapacityScheduler</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">addApplication</span><span class="o">(</span><span class="mi">544</span><span class="o">))</span> <span class="o">-</span> <span class="n">Accepted</span> <span class="n">application</span> <span class="n">application_1405323437551_0001</span> <span class="n">from</span> <span class="nl">user:</span> <span class="n">hdfs</span><span class="o">,</span> <span class="n">in</span> <span class="nl">queue:</span> <span class="k">default</span>
</span></code></pre></td></tr></table></div></figure>


<h3>APP_REMOVED</h3>

<p>The analogy is the same as between <code>NODE_ADDED</code> and <code>NODE_REMOVED</code>. Updates the queue metrics and notifies the parent&rsquo;s that an application finished, removes the application and sets its final state.</p>

<h3>APP_ATTEMPT_ADDED</h3>

<p>After the <code>APP_ADDED</code> event the application is in <code>inactive</code> mode. It means it won`t get any resources scheduled for &ndash; only an attempt to run it. One application can have many attempts as it can fail for many reasons.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">rmapp</span><span class="o">.</span><span class="na">RMAppImpl</span> <span class="o">(</span><span class="n">RMAppImpl</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">handle</span><span class="o">(</span><span class="mi">639</span><span class="o">))</span> <span class="o">-</span> <span class="n">application_1405323437551_0001</span> <span class="n">State</span> <span class="n">change</span> <span class="n">from</span> <span class="n">SUBMITTED</span> <span class="n">to</span> <span class="n">ACCEPTED</span>
</span><span class='line'><span class="n">resourcemanager</span><span class="o">.</span><span class="na">ApplicationMasterService</span> <span class="o">(</span><span class="n">ApplicationMasterService</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">registerAppAttempt</span><span class="o">(</span><span class="mi">611</span><span class="o">))</span> <span class="o">-</span> <span class="n">Registering</span> <span class="n">app</span> <span class="n">attempt</span> <span class="o">:</span> <span class="n">appattempt_1405323437551_0001_000001</span>
</span><span class='line'><span class="n">attempt</span><span class="o">.</span><span class="na">RMAppAttemptImpl</span> <span class="o">(</span><span class="n">RMAppAttemptImpl</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">handle</span><span class="o">(</span><span class="mi">659</span><span class="o">))</span> <span class="o">-</span> <span class="n">appattempt_1405323437551_0001_000001</span> <span class="n">State</span> <span class="n">change</span> <span class="n">from</span> <span class="n">NEW</span> <span class="n">to</span> <span class="n">SUBMITTED</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">LeafQueue</span> <span class="o">(</span><span class="n">LeafQueue</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">activateApplications</span><span class="o">(</span><span class="mi">763</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">application_1405323437551_0001</span> <span class="n">from</span> <span class="nl">user:</span> <span class="n">hdfs</span> <span class="n">activated</span> <span class="n">in</span> <span class="nl">queue:</span> <span class="k">default</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">LeafQueue</span> <span class="o">(</span><span class="n">LeafQueue</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">addApplicationAttempt</span><span class="o">(</span><span class="mi">779</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">added</span> <span class="o">-</span> <span class="nl">appId:</span> <span class="n">application_1405323437551_0001</span> <span class="nl">user:</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">hadoop</span><span class="o">.</span><span class="na">yarn</span><span class="o">.</span><span class="na">server</span><span class="o">.</span><span class="na">resourcemanager</span><span class="o">.</span><span class="na">scheduler</span><span class="o">.</span><span class="na">capacity</span><span class="o">.</span><span class="na">LeafQueue</span><span class="n">$User</span><span class="err">@</span><span class="mi">46</span><span class="n">a224a4</span><span class="o">,</span> <span class="n">leaf</span><span class="o">-</span><span class="nl">queue:</span> <span class="k">default</span> <span class="err">#</span><span class="n">user</span><span class="o">-</span><span class="n">pending</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span> <span class="err">#</span><span class="n">user</span><span class="o">-</span><span class="n">active</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">1</span> <span class="err">#</span><span class="n">queue</span><span class="o">-</span><span class="n">pending</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span> <span class="err">#</span><span class="n">queue</span><span class="o">-</span><span class="n">active</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">1</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">CapacityScheduler</span> <span class="o">(</span><span class="n">CapacityScheduler</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">addApplicationAttempt</span><span class="o">(</span><span class="mi">567</span><span class="o">))</span> <span class="o">-</span> <span class="n">Added</span> <span class="n">Application</span> <span class="n">Attempt</span> <span class="n">appattempt_1405323437551_0001_000001</span> <span class="n">to</span> <span class="n">scheduler</span> <span class="n">from</span> <span class="n">user</span> <span class="n">hdfs</span> <span class="n">in</span> <span class="n">queue</span> <span class="k">default</span>
</span></code></pre></td></tr></table></div></figure>


<p>Attempt states are transferred from one to another. By sending an <code>AppAttemptAddedSchedulerEvent</code> the scheduler actually tries to allocate resources. First, the application goes into the pending applications list of the queue and if the queue limits allows it, it goes into the active applications list. This active application list is the one that the queue uses when trying to allocate resources.
It works in FIFO order, but I&rsquo;ll elaborate on it in the <code>NODE_UPDATE</code> part.</p>

<h3>APP_ATTEMPT_REMOVED</h3>

<p>On <code>AppAttemptRemovedSchedulerEvent</code> the scheduler cleans up after the application. Releases all the allocated, acquired, running containers
(in case of <code>ApplicationMaster</code> restart the running containers won&rsquo;t get killed), releases all reserved containers,
cleans up pending requests and informs the queues.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">rmapp</span><span class="o">.</span><span class="na">RMAppImpl</span> <span class="o">(</span><span class="n">RMAppImpl</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">handle</span><span class="o">(</span><span class="mi">639</span><span class="o">))</span> <span class="o">-</span> <span class="n">application_1405323437551_0001</span> <span class="n">State</span> <span class="n">change</span> <span class="n">from</span> <span class="n">FINISHING</span> <span class="n">to</span> <span class="n">FINISHED</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">CapacityScheduler</span> <span class="o">(</span><span class="n">CapacityScheduler</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">doneApplicationAttempt</span><span class="o">(</span><span class="mi">598</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">Attempt</span> <span class="n">appattempt_1405323437551_0001_000001</span> <span class="n">is</span> <span class="n">done</span><span class="o">.</span> <span class="n">finalState</span><span class="o">=</span><span class="n">FINISHED</span>
</span><span class='line'><span class="n">scheduler</span><span class="o">.</span><span class="na">AppSchedulingInfo</span> <span class="o">(</span><span class="n">AppSchedulingInfo</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">clearRequests</span><span class="o">(</span><span class="mi">108</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">application_1405323437551_0001</span> <span class="n">requests</span> <span class="n">cleared</span>
</span><span class='line'><span class="n">capacity</span><span class="o">.</span><span class="na">LeafQueue</span> <span class="o">(</span><span class="n">LeafQueue</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">removeApplicationAttempt</span><span class="o">(</span><span class="mi">821</span><span class="o">))</span> <span class="o">-</span> <span class="n">Application</span> <span class="n">removed</span> <span class="o">-</span> <span class="nl">appId:</span> <span class="n">application_1405323437551_0001</span> <span class="nl">user:</span> <span class="n">hdfs</span> <span class="nl">queue:</span> <span class="k">default</span> <span class="err">#</span><span class="n">user</span><span class="o">-</span><span class="n">pending</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span> <span class="err">#</span><span class="n">user</span><span class="o">-</span><span class="n">active</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span> <span class="err">#</span><span class="n">queue</span><span class="o">-</span><span class="n">pending</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span> <span class="err">#</span><span class="n">queue</span><span class="o">-</span><span class="n">active</span><span class="o">-</span><span class="nl">applications:</span> <span class="mi">0</span>
</span><span class='line'><span class="n">amlauncher</span><span class="o">.</span><span class="na">AMLauncher</span> <span class="o">(</span><span class="n">AMLauncher</span><span class="o">.</span><span class="na">java</span><span class="o">:</span><span class="n">run</span><span class="o">(</span><span class="mi">262</span><span class="o">))</span> <span class="o">-</span> <span class="n">Cleaning</span> <span class="n">master</span> <span class="n">appattempt_1405323437551_0001_000001</span>
</span></code></pre></td></tr></table></div></figure>


<h3>NODE_UPDATE</h3>

<p>Normally <code>NodeUpdateSchedulerEvents</code> arrive every second from every node. By setting the earlier mentioned <code>schedule-asynchronously</code> to true
the behavior of this event handling can be altered in a way that container allocations happen asynchronously from these events. Meaning
the CapacityScheduler tries to allocate new containers in every 100ms on a different <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java#L361">thread</a>.
Before going into details let&rsquo;s discuss another important aspect.</p>

<h4>Allocate</h4>

<p>The <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java#L675">allocate</a>
method is used as a heartbeat. This is the most important call between the <code>ApplicationMaster</code> and the scheduler. Instead of using a simple empty message, the heartbeat can contain resource requests of the application. The reason it is important here is that the scheduler, more precisely the queue &ndash; when tries to allocate resources &ndash; it will check the active applications in FIFO order and see their resource requests.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">{</span><span class="nl">Priority:</span> <span class="mi">20</span><span class="o">,</span> <span class="nl">Capability:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">1024</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">1</span><span class="o">&gt;,</span> <span class="err">#</span> <span class="nl">Containers:</span> <span class="mi">2</span><span class="o">,</span> <span class="nl">Location:</span> <span class="o">*,</span> <span class="n">Relax</span> <span class="nl">Locality:</span> <span class="kc">true</span><span class="o">}</span>
</span><span class='line'><span class="o">{</span><span class="nl">Priority:</span> <span class="mi">20</span><span class="o">,</span> <span class="nl">Capability:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">1024</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">1</span><span class="o">&gt;,</span> <span class="err">#</span> <span class="nl">Containers:</span> <span class="mi">2</span><span class="o">,</span> <span class="nl">Location:</span> <span class="o">/</span><span class="k">default</span><span class="o">-</span><span class="n">rack</span><span class="o">,</span> <span class="n">Relax</span> <span class="nl">Locality:</span> <span class="kc">true</span><span class="o">}</span>
</span><span class='line'><span class="o">{</span><span class="nl">Priority:</span> <span class="mi">20</span><span class="o">,</span> <span class="nl">Capability:</span> <span class="o">&lt;</span><span class="nl">memory:</span><span class="mi">1024</span><span class="o">,</span> <span class="nl">vCores:</span><span class="mi">1</span><span class="o">&gt;,</span> <span class="err">#</span> <span class="nl">Containers:</span> <span class="mi">2</span><span class="o">,</span> <span class="nl">Location:</span> <span class="n">amb1</span><span class="o">.</span><span class="na">mycorp</span><span class="o">.</span><span class="na">kom</span><span class="o">,</span> <span class="n">Relax</span> <span class="nl">Locality:</span> <span class="kc">true</span><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s examine these requests:</p>

<ul>
<li>Priority: requests with higher priority gets served first (mappers got 20, reducer 10, AM 0)</li>
<li>Capability: denotes the required container&rsquo;s size</li>
<li>Location: where the AM would like to get the containers. There are 3 types of locations: node-local, rack-local, off-switch.
The location is based on the location of the blocks of the data. It is the <code>ApplicationMaster</code>&rsquo;s duty to locate them. Off-switch means
that any node in the cluster is good enough. Typically the <code>ApplicationMaster</code>&rsquo;s container request is an off-switch request.</li>
<li>Relax locality: in case it is set to false, only node-local container can be allocated. By default it is set to true.</li>
</ul>


<h4>NODE_UPDATE</h4>

<p>Let&rsquo;s get back to <code>NODE_UPDATE</code>. What happens at every node update and why it happens so frequently? First of all, nodes are running the containers. Containers start and stop all the time thus the scheduler needs an update about the state of the nodes. Also it updates their resource capabilities as well. After the updates are noted, the scheduler tries to allocate containers on the nodes. The same applies to every node in the cluster. At every <code>NODE_UPDATE</code> the scheduler checks if an application reserved a container on this node. If there is reservation then tries to fulfill it. Every node can have only one reserved container. After the reservation it tries to allocate more containers by going through all the queues starting from <code>root</code>. The node can have one more container if it has at least the minimum allocation&rsquo;s resource capability. Going through the child queues of <code>root</code> it checks the queue&rsquo;s active applications and its resource requests by priority order. If the application has request for this node it tries to allocate it. If the relax locality is set to true it could also allocate container even though there is no explicit request for this node, but that&rsquo;s not what&rsquo;s going to happen first. There is another term called delay scheduling. The scheduler tries to delay any non-data-local
request, but cannot delay it for so long. The <code>localityWaitFactor</code> will determine how long to wait until fall back to rack-local then the end to off-switch requests. If everything works out it allocates a container and tracks its resource usage. If there is not enough resource capability one application can reserve a container on this node, and at the next update may can use this reservation. After the allocation is made the <code>ApplicationMaster</code> will submit a request to the node to launch this container and assign a task to it to run.
The scheduler does not need to know what task the AM will run in the container.</p>

<h3>CONTAINER_EXPIRED</h3>

<p>The <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/ContainerAllocationExpirer.java">ContainerAllocationExpirer&rsquo;s</a>
responsibility to check if a container expires and when it does it sends an <code>ContainerExpiredSchedulerEvent</code> and the scheduler will notify the application to remove the container. The value of how long to wait until a container is considered dead can be configured.</p>

<h2>Animation</h2>

<p>The animation on top shows a basic event flow starting from adding 2 nodes and submitting 2 applications with attempts. Eventually the node updates tries to allocate resources to those applications. After reading this post I hope it makes sense now.</p>

<h2>What&rsquo;s next?</h2>

<p>In the next part of this series I&rsquo;ll compare it with FairScheduler, to see the differences.</p>

<p>For updates and further blog posts follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>

<p>Enjoy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloudbreak - the Hadoop as a Service API]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/"/>
    <updated>2014-07-18T16:37:37+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak</id>
    <content type="html"><![CDATA[<p><em>Cloudbreak is a powerful left surf that breaks over a coral reef, a mile off southwest the island of Tavarua, Fiji.</em></p>

<p><em>Cloudbreak is a cloud agnostic Hadoop as a Service API. Abstracts the provisioning and ease management and monitoring of on-demand clusters.</em></p>

<p>Today is a big day for us and the Hadoop community &ndash; we are announcing the first <code>public beta</code> version of our open source and cloud agnostic <strong>Hadoop as a Service API</strong>.</p>

<p>During our daily work with large Hadoop clusters in the cloud, <code>dockerized</code> environments and bare metal we were doing the same things over and over again. Although we are automating and <code>dockerizing</code> always everything, we felt that something is missing &ndash; an open source, cloud agnostic Hadoop as a Service API. Welcome <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> &ndash; you are one POST away from your on-demand Hadoop cluster on your favorite cloud provider.</p>

<p>When we have started to work on Cloudbreak &ndash; first of all to solve our internal needs at SequenceIQ &ndash; we have set the following criteria:</p>

<ul>
<li>Use open source software and be <strong>100% open source</strong> under Apache 2 license</li>
<li>Have the ability to quickly launch arbitrary sized Hadoop clusters</li>
<li>Be cloud provider agnostic and create an SDK which allows to quickly add new providers</li>
<li>No more glue code, repeating the same things over and over again</li>
<li>Have a REST API and a CLI in order to be able to automate the whole process</li>
<li>Create an easy to use and responsive UI</li>
<li>Support different Hadoop services and configurations in a declarative way</li>
<li>Elastic and flexible, with the ability to resize running clusters</li>
<li>Secure</li>
</ul>


<!-- more -->


<h2>Docker in the cloud</h2>

<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we are running all our core applications and processes in Docker containers &ndash; and that is true for Hadoop and all of the services as well. During the last few months we have <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">blogged</a> and open sourced all of the <a href="https://hub.docker.com/u/sequenceiq/">building blocks</a> of our <code>dockerized</code> systems and <strong>Cloudbreak</strong> is built on the foundation of these and reusing the same technologies we have released before. While Cloudbreak&rsquo;s primary role is to launch on-demand Hadoop clusters in the cloud, the underlying technology actually does more. It can launch on-demand Hadoop clusters in any environment which supports Docker &ndash; in a dynamic way. There is no predefined configuration needed as all the setup, orchestration, networking and cluster membership is done dynamically.</p>

<ul>
<li><a href="https://hub.docker.com/u/sequenceiq/">Docker containers</a> &ndash; all the Hadoop services are installed and running inside Docker containers, and these containers are <code>shipped</code>  between different cloud vendors, keeping Cloudbreak cloud agnostic</li>
<li><a href="https://github.com/sequenceiq/ambari-rest-client">Apache Ambari</a> &ndash; to declaratively define a Hadoop cluster</li>
<li><a href="https://github.com/sequenceiq/docker-serf">Serf</a> &ndash; for cluster membership, failure detection, and orchestration that is decentralized, fault-tolerant and highly available for dynamic clusters</li>
<li><a href="https://github.com/sequenceiq/docker-dnsmasq">dnsmasq</a> &ndash; to provide resolvable fully qualified domain names between dynamically created Docker containers.</li>
</ul>


<p>The project was presented at the <strong>Hadoop Summit 2014,</strong> in San Jose &ndash; you can get the slides from <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">here</a>.</p>

<p>While there is an extensive list of articles explaining the benefits of using Docker, we would like to highlight our motivations in a few bullet points.</p>

<ul>
<li>Write once, run anywhere &ndash; our solution uses the same Docker containers on different cloud providers, <code>dockerized</code>  environments or bare metal, no difference at all</li>
<li>Reproducible, testable environment &ndash; we are recreating complete config environments in seconds, and being able to work with the same containers on our laptop, QA and production/cloud environments</li>
<li>Isolation &ndash; each container is separated and runs in its own isolated sandbox</li>
<li>Versioning &ndash; we are able to easily version and modify containers, and ship only the changed bits saving bandwidth; essential for large clusters deployed in the cloud</li>
<li>Central repository &ndash; you can build an entire cluster from a trusted and centralised container repository, the Docker Registry/Hub</li>
<li>Smart resource allocation &ndash; containers can be <code>shipped</code> anywhere and resources can be allotted</li>
</ul>


<h2>Cloudbreak &ndash; the project</h2>

<h3>Cloudbreak UI</h3>

<p>The easiest way to start your own Hadoop cluster in your favorite cloud provider is to use our hosted solution. We host, maintain and support <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> for you. Cloudbreak UI is a secure and intuitive way to launch on-demand Hadoop clusters with a few mouse clicks. Please note that Cloudbreak is launching Hadoop clusters on the user&rsquo;s behalf &ndash; on different cloud providers. We do not store your cloud provider account details (such as username, password, keys, private SSL certificates, etc), but work around the concept that Identity and Access Management is fully controlled by you &ndash; the end user.</p>

<h3>Cloudbreak API</h3>

<p>Cloudbreak is a RESTful Hadoop as a Service API. The easiest way to use the API is by using our hosted <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak API</a>.</p>

<p>We have also give you the option to host Cloudbreak within your organization. Once it is deployed in your favourite servlet container exposes a REST API allowing to span up Hadoop clusters of arbitrary sizes on your selected cloud provider. With Cloudbreak you are one POST away from your on-demand Hadoop cluster. You can get the code from our <a href="https://github.com/sequenceiq/cloudbreak">GitHub repository</a>. For further documentation please follow up with the <a href="http://sequenceiq.com/cloudbreak/">general</a> and <a href="http://docs.cloudbreak.apiary.io/">API</a> documentation, or subscribe to one of our social channels in order to receive notifications about further blog posts and releases.</p>

<h3>Cloudbreak REST client</h3>

<p>In order to ease your work with the REST API and embed in your codebase we have created (and also extensively use) a Groovy REST client. The code is available at our <a href="https://github.com/sequenceiq/cloudbreak-rest-client">GitHub</a> repository.</p>

<h3>Cloudbreak CLI</h3>

<p>As we automate everything and we are a very DevOps focused company we are always trying to create easy ways to interact with our systems and API’s. In case of Cloudbreak we have created and released a <a href="https://github.com/sequenceiq/cloudbreak-shell">command line shell</a>, the Cloudbreak CLI. The CLI allows you to use all the REST calls, and it has a large number of easing commands. Interactive help and completion is available.</p>

<h3>Cloudbreak documentation</h3>

<p>We have created two types of documentation. The <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak Product</a> documentation contains an overview, installation, architectural and technical content, whereas the <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak API</a> explains the REST API with examples and a mock server to test your integration.</p>

<h2>Supported Hadoop services</h2>

<p>At high level the supported list of components can be grouped into two main categories: Master and Slave &ndash; and bundling them together form a Hadoop Service. <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> supports the following Hadoop services.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>| Services    | Components                                                              |
</span><span class='line'>| ----------- | ------------------------------------------------------------------------|
</span><span class='line'>| HDFS        | DATANODE, HDFS_CLIENT, JOURNALNODE, NAMENODE, SECONDARY_NAMENODE, ZKFC  |
</span><span class='line'>| YARN        | APP_TIMELINE_SERVER, NODEMANAGER, RESOURCEMANAGER, YARN_CLIENT          |
</span><span class='line'>| MAPREDUCE2  | HISTORYSERVER, MAPREDUCE2_CLIENT                                        |
</span><span class='line'>| GANGLIA     | GANGLIA_MONITOR, GANGLIA_SERVER                                         |
</span><span class='line'>| HBASE       | HBASE_CLIENT, HBASE_MASTER, HBASE_REGIONSERVER                          |
</span><span class='line'>| HIVE        | HIVE_CLIENT, HIVE_METASTORE, HIVE_SERVER, MYSQL_SERVER                  |
</span><span class='line'>| HCATALOG    | HCAT                                                                    |
</span><span class='line'>| WEBHCAT     | WEBHCAT_SERVER                                                          |
</span><span class='line'>| NAGIOS      | NAGIOS_SERVER                                                           |
</span><span class='line'>| OOZIE       | OOZIE_CLIENT, OOZIE_SERVER                                              |
</span><span class='line'>| PIG         | PIG                                                                     |
</span><span class='line'>| SQOOP       | SQOOP                                                                   |
</span><span class='line'>| STORM       | DRPC_SERVER, NIMBUS, STORM_REST_API, STORM_UI_SERVER, SUPERVISOR        |
</span><span class='line'>| TEZ         | TEZ_CLIENT                                                              |
</span><span class='line'>| FALCON      | FALCON_CLIENT, FALCON_SERVER                                            |
</span><span class='line'>| ZOOKEEPER   | ZOOKEEPER_CLIENT, ZOOKEEPER_SERVER                                      |</span></code></pre></td></tr></table></div></figure>


<p>Please note that you can always build your own custom stack beyond these services, using Ambari&rsquo;s custom stack definitions.</p>

<h2>What’s next?</h2>

<p>We will follow up with a few posts to drive you through the technology, API and insights and make it easier for you to learn, understand and use Hadoop in the cloud.</p>

<p>In the meantime we suggest you to go through the <a href="http://sequenceiq.com/cloudbreak/">documentation</a>, try <a href="http://cloudbreak.sequenceiq.com/">Cloudbreak</a> and let us know how it works for you.</p>

<p>Please note that <a href="http://cloudbreak.sequenceiq.com/">Cloudbreak</a> is under development, in public beta &ndash; while we consider the codebase stable for deployments (and use it daily), please let us know if you face any problems through <a href="https://github.com/sequenceiq/cloudbreak">GitHub</a> issues. Also we  welcome your open source contribution &ndash; let it be a bug fix or a new cloud provider <a href="http://sequenceiq.com/cloudbreak/#add-new-cloud-providers">implementation</a>.</p>

<p>Finally, your opinion is important to us &ndash; if you’d like to see your <strong>favourite cloud provider</strong> among the existing ones, please fill this <a href="https://docs.google.com/forms/d/129RVh6VfjRsuuHOcS3VPbFYTdM2SEjANDsGCR5Pul0I/viewform">questionnaire</a>. Make your voice heard!</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Groovy and Java, the runtime bug]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/13/groovy-and-java-runtime-bug/"/>
    <updated>2014-07-13T09:13:53+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/13/groovy-and-java-runtime-bug</id>
    <content type="html"><![CDATA[<p>I can barely count how many languages we use at SequenceIQ <em>[based on our <a href="https://github.com/sequenceiq">GitHub</a> repository it&rsquo;s Java, Scala, Groovy, Go, CoffeeScript, JavaScript, R and Shell (Ansible, Dockerfile, AWS CLI, what not)]</em>. Groovy is one of them.
Coding in Groovy is fast and fun, isn&rsquo;t it? Except when problems come up at runtime. This is one of those.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/groovy-bug/src/main/resources/wtf.png"></p>

<!-- more -->


<p>The Ambari REST Client is written in Groovy and in this case used by a Java application. You can find the sample bug in our
<a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/groovy-bug">repository</a>. The same thing could have been achieved with
reflection as well. Do you know why this can happen? It&rsquo;s a good candidate for an interview question, isn&rsquo;t it &hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Ambari configuration service]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service/"/>
    <updated>2014-07-09T08:20:05+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service</id>
    <content type="html"><![CDATA[<p>At SequenceIQ we use Apache Ambari for provisioning, managing, and monitoring Apache Hadoop clusters on different environments. However Ambari has more useful features than these &ndash; especially for us who automate and frequently build on-demand Hadoop clusters in cloud environments and submit different applications into. These Hadoop clusters carry different components, configurations and services &ndash; think of dev->test->UAT->PROD cluster lifecycles, different settings, SLA&rsquo;s, etc).</p>

<p>Configuration of applications that use dynamically built YARN clusters can be challenging. This is due to the huge amount of configuration properties, some of which needs to be kept in sync on YARN client application side. Think of <em>yarn.resourcemanager.address</em>, <em>fs.defaultFS</em>, <em>yarn.resourcemanager.scheduler.address</em> to name a few. Each time these cluster specific entries change, client applications needs to be reconfigured. Those who ever played with clusters where the default properties are overridden know what this means&hellip;</p>

<p>At Sequenceiq we use Ambari for building on-demand YARN clusters (see the related <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/"> blog post</a>). In our case Ambari not only maintains the configuration of the cluster it manages but also provides access to them through a set of REST resources.</p>

<p>To overcome the configuration maintenance problem in YARN client applications, we implemented an Ambari REST client application that embedded in client applications can dynamically retrieve configuration from an Ambari instance. Thus the only thing needed for an application to have the proper configuration is the access to the Ambari instance.</p>

<p>The Ambari REST client is an open source project we developed and contributed to Apache Ambari &ndash; it&rsquo;s a Groovy REST client used by the <a href="https://github.com/sequenceiq/ambari-shell">Ambari Shell</a> and <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a>.</p>

<!-- more -->


<p>Here is a short example on how to make use of the Ambari client in an arbitrary application:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">AmbariConfigurationService</span> <span class="o">{</span>
</span><span class='line'><span class="o">...</span>
</span><span class='line'><span class="kd">private</span> <span class="n">AmbariClient</span> <span class="n">ambariClient</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kd">public</span> <span class="nf">AmbariConfigurationService</span><span class="o">(){</span>
</span><span class='line'>  <span class="c1">// inject / provide the service with the ambari related properties</span>
</span><span class='line'>  <span class="n">ambariClient</span> <span class="o">=</span> <span class="k">new</span> <span class="n">AmbariClient</span><span class="o">(</span><span class="n">ambariHost</span><span class="o">,</span> <span class="n">ambariPort</span><span class="o">,</span> <span class="n">ambariUser</span><span class="o">,</span> <span class="n">ambariPass</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// list with the properties needed by the application</span>
</span><span class='line'><span class="kd">private</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">configList</span> <span class="o">=</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="s">&quot;mapreduce.framework.name&quot;</span><span class="o">,</span> <span class="s">&quot;yarn.resourcemanager.address&quot;</span><span class="o">,</span> <span class="s">&quot;hbase.zookeeper.quorum&quot;</span> <span class="o">);</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// assembles a Configuration instance with the properties needed by the application</span>
</span><span class='line'><span class="kd">public</span> <span class="n">Configuration</span> <span class="nf">getConfiguration</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>        <span class="c1">//  use this constructor to avoid loading of properties from the classpath!</span>
</span><span class='line'>        <span class="n">Configuration</span> <span class="n">configuration</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">(</span><span class="kc">false</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// Map with service specific configuration. The keys are service names: eg.: yarn-site, hbase-site, global ...</span>
</span><span class='line'>        <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">serviceConfigMap</span> <span class="o">=</span> <span class="n">ambariClient</span><span class="o">.</span><span class="na">getServiceConfigMap</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">for</span> <span class="o">(</span><span class="n">Map</span><span class="o">.</span><span class="na">Entry</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">serviceEntry</span> <span class="o">:</span> <span class="n">serviceConfigMap</span><span class="o">.</span><span class="na">entrySet</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>            <span class="k">for</span> <span class="o">(</span><span class="n">Map</span><span class="o">.</span><span class="na">Entry</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">configEntry</span> <span class="o">:</span> <span class="n">serviceEntry</span><span class="o">.</span><span class="na">getValue</span><span class="o">().</span><span class="na">entrySet</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>                <span class="k">if</span> <span class="o">(</span><span class="n">configList</span><span class="o">.</span><span class="na">contains</span><span class="o">(</span><span class="n">configEntry</span><span class="o">.</span><span class="na">getKey</span><span class="o">()))</span> <span class="o">{</span>
</span><span class='line'>                    <span class="n">configuration</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">configEntry</span><span class="o">.</span><span class="na">getKey</span><span class="o">(),</span> <span class="n">configEntry</span><span class="o">.</span><span class="na">getValue</span><span class="o">());</span>
</span><span class='line'>                <span class="o">}</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>        <span class="c1">// decorate the config with application specific entries, like &quot;dfs.client.use.legacy.blockreader&quot;, &quot;mapreduce.job.user.classpath.first&quot;</span>
</span><span class='line'>        <span class="n">decorateConfiguration</span><span class="o">(</span><span class="n">configuration</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">return</span> <span class="n">configuration</span><span class="o">;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p><em>Note: Apart from the <code>getServiceConfigMap()</code> method you&rsquo;ll find a few interesting and useful operations</em></p>

<p>You can get the Ambari client code from the <a href="https://github.com/sequenceiq/ambari-rest-client">SequenceIQ GitHub repository</a> &ndash; clone it, build it and add it as a dependency to your project.</p>

<p>If you&rsquo;d like to play with a real multi-node Ambari managed cluster check out <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">this</a> older blog post &ndash; this will set you up with a Hadoop cluster in less than 2 minutes / one-click.</p>

<p>Let us know how it works for you &ndash; for updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker debug with nsenter on boot2docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/05/docker-debug-with-nsenter-on-boot2docker/"/>
    <updated>2014-07-05T12:05:41+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/05/docker-debug-with-nsenter-on-boot2docker</id>
    <content type="html"><![CDATA[<p><code>nsenter</code> is a small tool allowing to <code>enter</code> into <code>n</code>ame<code>s</code>paces. Specifically
when you work with docker, it means you can <em>enter</em> any docker container, even
it they don&rsquo;t run any sshd. Running sshd in a docker container for debuging
<a href="http://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil/">considered evil</a>.</p>

<h2>Nsenter with Boot2docker</h2>

<p>Docker doesn&rsquo;t run directly on OS X and on Windows, so you need
<a href="http://boot2docker.io/">boot2docker</a>. To get <code>nsenter</code> working with boot2docker
is a bit trickier.</p>

<p>For the impatient here is a simple function, which lets you enter any docker
container directly from OS X (or any boot2docker host):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker-enter() {
</span><span class='line'>  boot2docker ssh '[ -f /var/lib/boot2docker/nsenter ] || docker run --rm -v /var/lib/boot2docker/:/target jpetazzo/nsenter'
</span><span class='line'>  boot2docker ssh -t sudo /var/lib/boot2docker/docker-enter "$@"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>the first line installs <code>nsenter</code> and <code>docker-enter</code> if missing and the second line
does the actual call.</p>

<p>once you declared the function, you can use it as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker-enter &lt;CONTAINER-ID/CONTAINER-NAME&gt;</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>TL;DR</h2>

<p>If you are interested about the details how this works read on.</p>

<h2>Install nseneter onto boot2docker</h2>

<p>How to install nsenter into boot2docker? Its a bit tricky, as boot2docker isn&rsquo;t
a full-blown linux, it&rsquo;s based on tiny core linux, so compiling on it is not trivial.</p>

<p>But guess what, <strong>jpetazzo</strong> already created a <a href="https://github.com/jpetazzo/nsenter">dockerized nsenter</a>
It suggest to install the binary <code>nsenter</code> as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run --rm -v /usr/local/bin:/target jpetazzo/nsenter</span></code></pre></td></tr></table></div></figure>


<p>This works with boot2docker &hellip; until you restart it. You should store all
changes on the persistent <code>/var/lib/boot2docker</code> directory.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run --rm -v /var/lib/boot2docker/:/target jpetazzo/nsenter</span></code></pre></td></tr></table></div></figure>


<h2>the docker-enter script</h2>

<p><a href="https://github.com/jpetazzo/nsenter/blob/master/docker-enter">docker-enter</a> is
a helper script to do the following 2 steps:</p>

<ul>
<li>gets the <code>PID</code> of the docker container</li>
<li>executes <code>nsenter</code> optionally passing the name of a program to execute inside
the namespace. if no command is specified a shell will be invoked instead.</li>
</ul>


<p>In the previous step, when you have installed <code>nseneter</code> the <code>docker-enter</code> srcipt
<a href="https://github.com/jpetazzo/nsenter/blob/master/installer#L6">got installed</a> into the same directory.</p>

<h2>Nsenter directly from OS X</h2>

<p>Some blogs advise you to first ssh into boot2docker, and use nsenter or docker-enter
inside of the virtual env. But if you are executing a single command via ssh, you
can pass the command to the last argument of: <code>boot2docker ssh &lt;COMMAND&gt;</code></p>

<h2>One-liner</h2>

<p>So combine all the steps into a single <strong>one-liner</strong> function:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker-enter() { boot2docker ssh -t "[ -f /var/lib/boot2docker/nsenter ] || docker run --rm -v /var/lib/boot2docker/:/target jpetazzo/nsenter; sudo /var/lib/boot2docker/docker-enter $@"; }</span></code></pre></td></tr></table></div></figure>


<p>If you want it permanently either copy-paste it into your <code>~/.profile</code> or
<code>~/.bash_profile</code>. Or save it into <code>/usr/local/bin</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Lo /usr/local/bin/docker-enter j.mp/docker-enter && . /usr/local/bin/docker-enter</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Re-prioritize running jobs with YARN schedulers]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues/"/>
    <updated>2014-07-02T08:20:05+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we run different applications all within the same Hadoop YARN cluster. Often the deployed Hadoop stack is a multi-tenant and multi-application and runtime setup &ndash; and as usual for a scenario as such end users will try to use or book as much cluster capacity as possible. A great help for solving these problems are YARN schedulers &ndash; however in our case due to certain SLA and QoS requirements we needed to step further. We have invested a great effort to build custom YARN schedulers, learn about application insights (check our <a href="http://blog.sequenceiq.com/blog/2014/05/01/mapreduce-job-profiling-with-R/">blog post</a> about how we use R to profile running jobs) and we would like to share our experience with the community. Let&rsquo;s dig into technical details.</p>

<p>In YARN, one of the ResourceManager&rsquo;s most important role is the scheduling (allocating available resources in the cluster) between competing applications. It doesn&rsquo;t care about per-application states nor internal flows and optimizations, but the overall resource requirements of
each application. Currently there are 3 different scheduler implementations available: FIFO, Fair, Capacity.</p>

<p>Going back a few weeks in time we blogged about how to configure the
<a href="http://blog.sequenceiq.com/blog/2014/03/14/yarn-capacity-scheduler/">CapacityScheduler</a> and use different queue
setups. Based on the feedbacks we have received we realized that there is a lack of knowledge about how these schedulers work and many people have asked to fill that gap. Good news that we didn&rsquo;t
forget about you. We&rsquo;re going to start a post series where we&rsquo;ll explain them a little bit detailed with fancy diagrams and code examples.</p>

<p>But before doing that, let&rsquo;s visit a concrete problem we encountered while we were developing our product stack.
We wanted to use the CapacityScheduler, but for different reasons (SLA and QoS) move the submitted applications between different queues to set a priority among them &ndash; at runtime (quick reminder: queues are either a composition of other queues or a collection of applications, forming a tree).
Cross application priorites can&rsquo;t be configured yet, only priorities between tasks within the application. The only problem is if you check the code you&rsquo;ll find this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'>  <span class="kd">public</span> <span class="n">String</span> <span class="nf">moveApplication</span><span class="o">(</span><span class="n">ApplicationId</span> <span class="n">appId</span><span class="o">,</span> <span class="n">String</span> <span class="n">newQueue</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">YarnException</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">throw</span> <span class="k">new</span> <span class="nf">YarnException</span><span class="o">(</span><span class="n">getClass</span><span class="o">().</span><span class="na">getSimpleName</span><span class="o">()</span>
</span><span class='line'>        <span class="o">+</span> <span class="s">&quot; does not support moving apps between queues&quot;</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>Currently this operation is supported only by the FairScheduler. Why is it not implemented? Let us know in a comment and you might receive a surprise present from us :). In the meantime if we&rsquo;d like
to implement it what would be the steps? Lets start with the following queue hierarchy and their capabilities taken from the integration tests:</p>

<p><img src="http://yuml.me/1fe68e90"></p>

<p>Assume we&rsquo;ve submitted 2 applications, <strong>app1</strong> to <code>b2</code> and <strong>app2</strong> to <code>a2</code> (submitting applications is only allowed to leaf queues). What if <strong>app2</strong> is
pending for so long because of the queue capacity and my friend&rsquo;s friend&rsquo;s dog cannot wait anymore to see his data clustering result? We could play with the queue capacities and max capacities, but then other apps might get scheduled as well and we don&rsquo;t want that.
Then we could move the app to a queue where it can get resources with a much higher chance. To move an app to somewhere
else in the hierarchy we have to consider and update a whole bunch of things. Let&rsquo;s move <strong>app1</strong> to queue <code>b1</code>.</p>

<p>Obviously we have to check if the target queue is a leaf queue and moving the app there does not violate any constraints. But how to do that?
The first part is easy (leaf or parent), but what about the other one? It has to do something with queue capacities, but checking only the target
queue&rsquo;s capacity is not enough, we have to go up in the hierarchy (because the parent queues also keep track the number of applications
and resource usages) but for how deep? The lowest common ancestor of the source and target is enough, because above that nothing changes. In our
case it&rsquo;s the <code>b</code> (b1, b2). Finding it is not that hard since the queues are declared like this:</p>

<ul>
<li>root.a.a1</li>
<li>root.a.a2</li>
<li>root.b.b1</li>
<li>root.b.b2</li>
<li>root.b.b3</li>
</ul>


<p>Going back until <code>b</code> and check the capacities:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">CSQueue</span> <span class="n">currentQueue</span> <span class="o">=</span> <span class="n">targetQueue</span><span class="o">;</span>
</span><span class='line'><span class="k">while</span> <span class="o">(</span><span class="n">currentQueue</span> <span class="o">!=</span> <span class="n">lowestCommonAncestor</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// maxApps</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">currentQueue</span><span class="o">.</span><span class="na">getNumApplications</span><span class="o">()</span> <span class="o">==</span> <span class="k">this</span><span class="o">.</span><span class="na">conf</span><span class="o">.</span><span class="na">getMaximumApplicationsPerQueue</span><span class="o">(</span><span class="n">currentQueue</span><span class="o">.</span><span class="na">getQueueName</span><span class="o">()))</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">throw</span> <span class="k">new</span> <span class="nf">YarnException</span><span class="o">(</span><span class="s">&quot;Moving app attempt &quot;</span> <span class="o">+</span> <span class="n">appAttId</span> <span class="o">+</span> <span class="s">&quot; to queue &quot;</span>
</span><span class='line'>      <span class="o">+</span> <span class="n">queueName</span> <span class="o">+</span> <span class="s">&quot; would violate queue maxApps constraints on&quot;</span>
</span><span class='line'>      <span class="o">+</span> <span class="s">&quot; queue &quot;</span> <span class="o">+</span> <span class="n">currentQueue</span><span class="o">.</span><span class="na">getQueueName</span><span class="o">());</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// maxCapacity</span>
</span><span class='line'>  <span class="kt">float</span> <span class="n">potentialNewCapacity</span> <span class="o">=</span> <span class="n">Resources</span><span class="o">.</span><span class="na">divide</span><span class="o">(</span><span class="n">calculator</span><span class="o">,</span> <span class="n">clusterResource</span><span class="o">,</span> <span class="n">Resources</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">currentQueue</span><span class="o">.</span><span class="na">getUsedResources</span><span class="o">(),</span> <span class="n">consumption</span><span class="o">),</span> <span class="n">clusterResource</span><span class="o">);</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">potentialNewCapacity</span> <span class="o">&gt;=</span> <span class="n">currentQueue</span><span class="o">.</span><span class="na">getAbsoluteMaximumCapacity</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">throw</span> <span class="k">new</span> <span class="nf">YarnException</span><span class="o">(</span><span class="s">&quot;Moving app attempt &quot;</span> <span class="o">+</span> <span class="n">appAttId</span> <span class="o">+</span> <span class="s">&quot; to queue &quot;</span>
</span><span class='line'>      <span class="o">+</span> <span class="n">queueName</span> <span class="o">+</span> <span class="s">&quot; would violate queue maxCapacity constraints on&quot;</span>
</span><span class='line'>      <span class="o">+</span> <span class="s">&quot; queue &quot;</span> <span class="o">+</span> <span class="n">currentQueue</span><span class="o">.</span><span class="na">getQueueName</span><span class="o">());</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="n">currentQueue</span> <span class="o">=</span> <span class="n">currentQueue</span><span class="o">.</span><span class="na">getParent</span><span class="o">();</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>If everything is fine we can execute the movement.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">private</span> <span class="kt">void</span> <span class="nf">executeMove</span><span class="o">(</span><span class="n">SchedulerApplication</span> <span class="n">app</span><span class="o">,</span> <span class="n">FiCaSchedulerApp</span> <span class="n">attempt</span><span class="o">,</span> <span class="n">LeafQueue</span> <span class="n">oldQueue</span><span class="o">,</span> <span class="n">LeafQueue</span> <span class="n">newQueue</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">oldQueue</span><span class="o">.</span><span class="na">removeApplicationAttempt</span><span class="o">(</span><span class="n">attempt</span><span class="o">);</span>
</span><span class='line'>    <span class="n">attempt</span><span class="o">.</span><span class="na">move</span><span class="o">(</span><span class="n">newQueue</span><span class="o">);</span> <span class="c1">// This updates all the queue metrics &#39;til the parent</span>
</span><span class='line'>    <span class="n">app</span><span class="o">.</span><span class="na">setQueue</span><span class="o">(</span><span class="n">newQueue</span><span class="o">);</span>
</span><span class='line'>    <span class="n">newQueue</span><span class="o">.</span><span class="na">trackApplications</span><span class="o">(</span><span class="n">attempt</span><span class="o">.</span><span class="na">getApplicationId</span><span class="o">(),</span> <span class="n">attempt</span><span class="o">.</span><span class="na">getUser</span><span class="o">());</span>
</span><span class='line'>    <span class="n">newQueue</span><span class="o">.</span><span class="na">submitApplicationAttempt</span><span class="o">(</span><span class="n">attempt</span><span class="o">,</span> <span class="n">attempt</span><span class="o">.</span><span class="na">getUser</span><span class="o">());</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>There are so many things implemented in these method calls it wouldn&rsquo;t fit here, but it serves the purpose here as pseudo code.</p>

<ul>
<li><p>oldQueue.removeApplicationAttempt(attempt);<br/>
Remove the application from the active and pending list. Notify the parents that an app has been removed.</p></li>
<li><p>attempt.move(newQueue);<br/>
Update the queue metrics upwards to root.</p></li>
<li><p>app.setQueue(newQueue);<br/>
Set the target queue in the app.</p></li>
<li><p>newQueue.trackApplications(attempt.getApplicationId(), attempt.getUser());<br/>
Notify the parents that a new application has been moved here.</p></li>
<li><p>newQueue.submitApplicationAttempt(attempt, attempt.getUser());<br/>
Finally submit the application attempt to the queue.</p></li>
</ul>


<p>As usual we always release the code as well &ndash; you can get the details from our <a href="https://github.com/sequenceiq">GitHub</a> page.</p>

<ul>
<li><p>Move applications between Capacity Scheduler queues <a href="https://github.com/sequenceiq/hadoop-common/blob/branch-2.4.1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/a/ExtendedCapacityScheduler.java#L924">implementation</a>.</p></li>
<li><p>Test case <a href="https://github.com/sequenceiq/hadoop-common/blob/branch-2.4.1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/a/TestExtendedCapacitySchedulerAppMove.java">implementation</a>.</p></li>
</ul>


<p>In case you are interested on the YARN Scheduler series make sure to follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a> or <a href="https://twitter.com/sequenceiq">Twitter</a> for the upcoming posts.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Hadoop 2.4.1 on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/25/hadoop-2-4-0-docker/"/>
    <updated>2014-06-25T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/25/hadoop-2-4-0-docker</id>
    <content type="html"><![CDATA[<p>A few weeks ago we have released an Apache Hadoop 2.3 Docker image &ndash; in a very short time this become the most <a href="https://registry.hub.docker.com/search?q=hadoop&amp;s=downloads">popular</a> Hadoop image in the Docker <a href="https://registry.hub.docker.com/">registry</a>.</p>

<p>Following on the success of our Hadoop 2.3 Docker <a href="https://registry.hub.docker.com/u/sequenceiq/hadoop-docker/">image</a>, the feedbacks and requests we have received and aligning with the Hadoop release cycle, we have released an Apache Hadoop 2.4.1 Docker image &ndash; same as the previous version this is available as a trusted and automated build on the official Docker <a href="https://registry.hub.docker.com/">registry</a>.</p>

<p>Please note that beside this Hadoop image, we have released and maintain a <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/">pseudo-distributed</a> and <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">distributed</a> Hadoop Docker image provisioned with Apache Ambari. As they are provisioned with Ambari you have the option to change, add and remove Hadoop components using cluster blueprints.</p>

<h2>Build the image</h2>

<p>In case you&rsquo;d like to try directly from the <a href="https://github.com/sequenceiq/hadoop-docker">Dockerfile</a> you can build the image as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build  -t sequenceiq/hadoop-docker .</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<h2>Pull the image</h2>

<p>As it is also released as an official Docker image from Docker&rsquo;s automated build repository &ndash; you can always pull or refer the image when launching containers.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/hadoop-docker:2.4.1</span></code></pre></td></tr></table></div></figure>


<h2>Start a container</h2>

<p>In order to use the Docker image you have just build or pulled use:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t sequenceiq/hadoop-docker /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<h2>Testing</h2>

<p>You can run one of the stock examples:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd $HADOOP_PREFIX
</span><span class='line'># run the mapreduce
</span><span class='line'>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar grep input output 'dfs[a-z.]+'
</span><span class='line'>
</span><span class='line'># check the output
</span><span class='line'>bin/hdfs dfs -cat output/*</span></code></pre></td></tr></table></div></figure>


<h2>Hadoop native libraries, build, Bintray, etc</h2>

<p>The Hadoop build process is no easy task &ndash; requires lots of libraries and their right version, protobuf, etc and takes some time &ndash; we have simplified all these, made the build and released a 64b version of Hadoop nativelibs on our <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.4.1/view/files">Bintray repo</a>. Enjoy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pearson correlation with Scalding]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/23/scalding-correlation-example/"/>
    <updated>2014-06-23T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/23/scalding-correlation-example</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>At SequenceIQ we are processing data in batch and streaming &ndash; for both we use Scala as our prefered language; for batch processing in particular we use Scalding to build our job and data pipelines. Actually there is <code>Babylon</code> at SequenceIQ as we use Java, Scala, Go, R, Groovy, Ansible, shell, JavaScript and what not &ndash; follow up with us for a post talking about the language heterogeneity.</p>

<p>Scalding is a powerful tool and great choice to simplify the writing and abstracting MapReduce jobs &ndash; an open source project originally developed by Twitter and recently the community.
In the following detailed example we&rsquo;d like show you an example of how to write and test Scalding jobs, running on Hadoop.</p>

<h2>Writing a Pearson correlation job</h2>

<p>In this example, we&rsquo;d like to calculate a Pearson&rsquo;s product-moment coefficient on 2 columns of a given <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/scalding-correlation/data">input</a>.
This is a simple computation and the easiest way to find any dependency between two datasets.
First of all we need all the parameters for the given <a href="http://www.statisticshowto.com/what-is-the-correlation-coefficient-formula/">formula</a>.
In Scala the code would look like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">trait</span> <span class="nc">CorrelationOp</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">calculateCorrelation</span><span class="o">(</span><span class="n">size</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">su1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">su2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">dotProd</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">dividend</span> <span class="k">=</span> <span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">dotProd</span><span class="o">)</span> <span class="o">-</span> <span class="o">(</span><span class="n">su1</span> <span class="o">*</span> <span class="n">su2</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">divisor</span> <span class="k">=</span> <span class="n">scala</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">sq1</span> <span class="o">-</span> <span class="n">su1</span> <span class="o">*</span> <span class="n">su1</span><span class="o">)</span> <span class="o">*</span> <span class="n">scala</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">sq2</span> <span class="o">-</span> <span class="n">su2</span> <span class="o">*</span> <span class="n">su2</span><span class="o">)</span>
</span><span class='line'>    <span class="n">dividend</span> <span class="o">/</span> <span class="n">divisor</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>In this example we compute all the required parameters for the correlation formula using the <a href="https://github.com/twitter/scalding/wiki/Fields-based-API-Reference">Field API</a> of Scala.
First we obtain the input/output and the two comparable column arguments which comes from command line parameters (usage : &mdash;key value) and provide the schema for the CSV input.
After the input is read we map the two selected fields (product and squares); with the underlined informations, we are able to produce the required parameters (grouping part).
At the end we just need to use the formula on the given fields (second map) and write the results into a TSV file.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="k">val</span> <span class="n">comparableColumn1</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="s">&quot;column1&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">comparableColumn2</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="s">&quot;column2&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">samplePercent</span> <span class="k">=</span> <span class="n">args</span><span class="o">.</span><span class="n">getOrElse</span><span class="o">(</span><span class="s">&quot;samplePercent&quot;</span><span class="o">,</span><span class="s">&quot;1.00&quot;</span><span class="o">).</span><span class="n">toDouble</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">scheme</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Fields</span><span class="o">(</span><span class="s">&quot;id&quot;</span><span class="o">,</span> <span class="s">&quot;num1&quot;</span><span class="o">,</span> <span class="s">&quot;num2&quot;</span><span class="o">,</span> <span class="s">&quot;num3&quot;</span><span class="o">,</span> <span class="s">&quot;num4&quot;</span><span class="o">,</span> <span class="s">&quot;num5&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="nc">Csv</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="s">&quot;input&quot;</span><span class="o">),</span> <span class="n">fields</span> <span class="k">=</span> <span class="n">scheme</span><span class="o">,</span> <span class="n">skipHeader</span> <span class="k">=</span> <span class="kc">true</span><span class="o">).</span><span class="n">read</span>
</span><span class='line'>  <span class="o">.</span><span class="n">sample</span><span class="o">(</span><span class="n">samplePercent</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="n">comparableColumn1</span><span class="o">,</span><span class="n">comparableColumn2</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="-Symbol">&#39;prod</span><span class="o">,</span> <span class="-Symbol">&#39;compSq1</span><span class="o">,</span> <span class="-Symbol">&#39;compSq2</span><span class="o">)){</span>
</span><span class='line'>    <span class="n">values</span> <span class="k">:</span> <span class="o">(</span><span class="kt">Double</span><span class="o">,</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="o">(</span><span class="n">values</span><span class="o">.</span><span class="n">_1</span> <span class="o">*</span> <span class="n">values</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="n">values</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="n">values</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">groupAll</span><span class="o">{</span>
</span><span class='line'>    <span class="k">_</span><span class="o">.</span><span class="n">size</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sum</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">comparableColumn1</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;compSum1</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sum</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">comparableColumn2</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;compSum2</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sum</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="-Symbol">&#39;compSq1</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;normSq1</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sum</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="-Symbol">&#39;compSq2</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;normSq2</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sum</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="-Symbol">&#39;prod</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;dotProduct</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">limit</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">project</span><span class="o">(</span><span class="-Symbol">&#39;size</span><span class="o">,</span><span class="-Symbol">&#39;compSum1</span><span class="o">,</span> <span class="-Symbol">&#39;compSum2</span><span class="o">,</span> <span class="-Symbol">&#39;normSq1</span><span class="o">,</span> <span class="-Symbol">&#39;normSq2</span><span class="o">,</span> <span class="-Symbol">&#39;dotProduct</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="-Symbol">&#39;size</span><span class="o">,</span> <span class="-Symbol">&#39;compSum1</span><span class="o">,</span> <span class="-Symbol">&#39;compSum2</span><span class="o">,</span><span class="-Symbol">&#39;normSq1</span><span class="o">,</span> <span class="-Symbol">&#39;normSq2</span><span class="o">,</span> <span class="-Symbol">&#39;dotProduct</span><span class="o">)</span>
</span><span class='line'>    <span class="o">-&gt;</span> <span class="o">(</span><span class="-Symbol">&#39;key</span><span class="o">,</span> <span class="-Symbol">&#39;correlation</span><span class="o">)){</span>
</span><span class='line'>    <span class="n">fields</span> <span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="k">val</span> <span class="o">(</span><span class="n">size</span><span class="o">,</span> <span class="n">sum1</span><span class="o">,</span> <span class="n">sum2</span><span class="o">,</span> <span class="n">normSq1</span><span class="o">,</span> <span class="n">normSq2</span><span class="o">,</span> <span class="n">dotProduct</span><span class="o">)</span> <span class="k">=</span> <span class="n">fields</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">corr</span> <span class="k">=</span> <span class="n">calculateCorrelation</span><span class="o">(</span><span class="n">size</span><span class="o">,</span> <span class="n">sum1</span><span class="o">,</span> <span class="n">sum2</span><span class="o">,</span> <span class="n">normSq1</span><span class="o">,</span> <span class="n">normSq2</span><span class="o">,</span> <span class="n">dotProduct</span><span class="o">)</span>
</span><span class='line'>      <span class="o">(</span><span class="n">comparableColumn1</span> <span class="o">+</span> <span class="s">&quot;-&quot;</span> <span class="o">+</span> <span class="n">comparableColumn2</span><span class="o">,</span> <span class="n">corr</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">project</span><span class="o">(</span><span class="-Symbol">&#39;key</span><span class="o">,</span> <span class="-Symbol">&#39;correlation</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">write</span><span class="o">(</span><span class="nc">Tsv</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="s">&quot;output&quot;</span><span class="o">)))</span>
</span></code></pre></td></tr></table></div></figure>


<p>For running the example you will have to run the following command: (<em>you can use &mdash;hdfs instead of &mdash;local</em>)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>yarn jar scalding-correalation-1.0.jar com.sequenceiq.scalding.correlation.SimpleCorrelationJob --local --input data/data.csv --output data/corr-out.tsv --column1 num1 --column2 num2 --samplePercent 0.1
</span></code></pre></td></tr></table></div></figure>


<h2>Testing Scalding jobs</h2>

<p>In order to test that your data transformations are correct, you can use the
<a href="http://twitter.github.io/scalding/com/twitter/scalding/JobTest.html">JobTest</a> class for unit testing.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="nd">@RunWith</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">JUnitRunner</span><span class="o">])</span>
</span><span class='line'><span class="k">class</span> <span class="nc">SimpleCorrelationJobTest</span>  <span class="k">extends</span> <span class="nc">Specification</span> <span class="o">{</span>
</span><span class='line'>  <span class="s">&quot;A SimpleCorrelation Job&quot;</span> <span class="n">should</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">input</span> <span class="k">=</span> <span class="nc">List</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">),(</span><span class="mi">2</span><span class="o">,</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">),(</span><span class="mi">3</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">))</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">correctOutputLimit</span> <span class="k">=</span> <span class="mf">0.8</span>
</span><span class='line'>
</span><span class='line'>    <span class="nc">JobTest</span><span class="o">(</span><span class="s">&quot;com.sequenceiq.scalding.correlation.SimpleCorrelationJob&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">arg</span><span class="o">(</span><span class="s">&quot;input&quot;</span><span class="o">,</span> <span class="s">&quot;fakeInput&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">arg</span><span class="o">(</span><span class="s">&quot;output&quot;</span><span class="o">,</span> <span class="s">&quot;fakeOutput&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">arg</span><span class="o">(</span><span class="s">&quot;column1&quot;</span><span class="o">,</span> <span class="s">&quot;num1&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">arg</span><span class="o">(</span><span class="s">&quot;column2&quot;</span><span class="o">,</span> <span class="s">&quot;num2&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">arg</span><span class="o">(</span><span class="s">&quot;correlationThreshold&quot;</span><span class="o">,</span> <span class="s">&quot;0.8&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">source</span><span class="o">(</span><span class="nc">Csv</span><span class="o">(</span><span class="s">&quot;fakeInput&quot;</span><span class="o">,</span> <span class="s">&quot;,&quot;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">Fields</span><span class="o">(</span><span class="s">&quot;id&quot;</span><span class="o">,</span><span class="s">&quot;num1&quot;</span><span class="o">,</span><span class="s">&quot;num2&quot;</span><span class="o">,</span><span class="s">&quot;num3&quot;</span><span class="o">,</span><span class="s">&quot;num4&quot;</span><span class="o">,</span><span class="s">&quot;num5&quot;</span><span class="o">),</span><span class="n">skipHeader</span> <span class="k">=</span> <span class="kc">true</span><span class="o">),</span> <span class="n">input</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sink</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)](</span><span class="nc">Tsv</span><span class="o">(</span><span class="s">&quot;fakeOutput&quot;</span><span class="o">,</span> <span class="n">fields</span> <span class="k">=</span> <span class="nc">Fields</span><span class="o">.</span><span class="nc">ALL</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">outputBuf</span> <span class="k">=&gt;</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">actualOutput</span> <span class="k">=</span> <span class="n">outputBuf</span><span class="o">.</span><span class="n">toList</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">_2</span>
</span><span class='line'>        <span class="s">&quot;return greater correlation result than 0.8&quot;</span> <span class="n">in</span> <span class="o">{</span>
</span><span class='line'>          <span class="n">correctOutputLimit</span> <span class="n">must</span> <span class="n">be_&lt;</span> <span class="o">(</span><span class="n">actualOutput</span><span class="o">)</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>      <span class="o">.</span><span class="n">run</span>
</span><span class='line'>      <span class="o">.</span><span class="n">finish</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Writing results to HBase</h2>

<p>In case we&rsquo;d like to store our data in a database (at SequenceIQ we use HBase) we can use a special Cascading Tap for it.
In this example we used <a href="https://github.com/ParallelAI/SpyGlass">Spyglass</a> to store the correlation results in HBase.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="k">val</span> <span class="n">tableName</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="s">&quot;tableName&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">quorum_name</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="s">&quot;quorum&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">quorum_port</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="s">&quot;quorumPort&quot;</span><span class="o">).</span><span class="n">toInt</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">scheme</span> <span class="k">=</span> <span class="nc">List</span><span class="o">(</span><span class="-Symbol">&#39;key</span><span class="o">,</span> <span class="-Symbol">&#39;correlation</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">familyNames</span> <span class="k">=</span> <span class="nc">List</span><span class="o">(</span><span class="s">&quot;corrCf&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="nc">Tsv</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="s">&quot;input&quot;</span><span class="o">)).</span><span class="n">read</span>
</span><span class='line'>    <span class="o">.</span><span class="n">toBytesWritable</span><span class="o">(</span><span class="n">scheme</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="n">write</span><span class="o">(</span>
</span><span class='line'>      <span class="k">new</span> <span class="nc">HBaseSource</span><span class="o">(</span>
</span><span class='line'>        <span class="n">tableName</span><span class="o">,</span>
</span><span class='line'>        <span class="n">quorum_name</span> <span class="o">+</span> <span class="s">&quot;:&quot;</span> <span class="o">+</span> <span class="n">quorum_port</span><span class="o">,</span>
</span><span class='line'>        <span class="n">scheme</span><span class="o">.</span><span class="n">head</span><span class="o">,</span>
</span><span class='line'>        <span class="n">familyNames</span><span class="o">,</span>
</span><span class='line'>        <span class="n">scheme</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="n">x</span><span class="k">:</span> <span class="kt">Symbol</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="k">new</span> <span class="nc">Fields</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="o">)).</span><span class="n">toList</span><span class="o">,</span>
</span><span class='line'>        <span class="n">timestamp</span> <span class="k">=</span> <span class="nc">Platform</span><span class="o">.</span><span class="n">currentTime</span>
</span><span class='line'>      <span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Build the application</h2>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./gradlew clean jar
</span></code></pre></td></tr></table></div></figure>


<p>or</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">export </span><span class="nv">GRADLE_OPTS</span><span class="o">=</span><span class="s2">&quot;-XX:MaxPermSize=2048m&quot;</span> <span class="c"># for tests</span>
</span><span class='line'>./gradlew clean build
</span></code></pre></td></tr></table></div></figure>


<h2>Running the example and persisting to HBase</h2>

<p>In order to run the example you&rsquo;ll have to run the following command: (you can use &mdash;hdfs instead of &mdash;local)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>yarn jar scalding-correalation-1.0.jar com.sequenceiq.scalding.hbase.HBaseWriterJob --local --input data/corr-out.tsv --tableName corrTable --quorum localhost --quorumPort 2181
</span></code></pre></td></tr></table></div></figure>


<p>Hope this correlation example and introduction into Scalding was useful &ndash; you can get the example project from our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/scalding-correlation">GitHub</a> repository.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Multi-node Hadoop cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/"/>
    <updated>2014-06-19T20:29:10+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker</id>
    <content type="html"><![CDATA[<p>In the <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/">previous post</a>
you saw how easy is to create a single-node Hadoop <em>cluster</em> on your devbox.</p>

<p>Now lets raise the bar and create a multinode Hadoop cluster on Docker. Before we
start, make sure you have the latest Ambari image:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/ambari:latest</span></code></pre></td></tr></table></div></figure>


<h2>One-liner</h2>

<p>Once you have the latest image, you can start running Docker containers.
But instead of typing long commands like <code>docker run [options] image [command]</code>,
we have created a couple of <a href="https://github.com/sequenceiq/docker-ambari/blob/master/ambari-functions">shell functions</a> to help you with Docker commands.</p>

<p>Using these functions the impatient can provision a 3 node Hadoop cluster with this one-liner:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Lo .amb j.mp/docker-ambari && . .amb && amb-deploy-cluster</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>Note that you can always alter the default parameters as the blueprint, cluster size, etc &hellip; check the shell <code>j.mp/docker-ambari</code> function&rsquo;s head for the parameters list.</p>

<p>It does the following steps:</p>

<ul>
<li>runs <code>ambari-server start</code> in a daemon Docker (background) container (and also an <code>ambari-agent start</code>)</li>
<li>runs <code>n-1</code> daemon containers with <code>ambari-agent start</code> connecting to the server</li>
<li>runs AmbariShell with attached terminal (to see provision progress)

<ul>
<li>AmbariShell will post the built-in multi-node blueprint to <code>/api/v1/blueprints</code> REST API</li>
<li>AmbariShell auto-assign hosts to host_groups defined in the blueprint</li>
<li>creates a cluster, by posting to the <code>/api/v1/clusters</code> REST API</li>
</ul>
</li>
</ul>


<h2>Custom blueprint</h2>

<p>If you have your own blueprint, put it into a <a href="https://gist.github.com/">gist</a>
and you can use it from AmbariShell. First start AmbariShell:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>amb-start-cluster 2
</span><span class='line'>amb-shell</span></code></pre></td></tr></table></div></figure>


<p>AmbariShell will wait for:</p>

<ul>
<li>Ambari REST API
Below you will see a happy path to create a multi node Hadoop cluster using the AmbariShell.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>host list
</span><span class='line'>blueprint add --url https://gist.githubusercontent.com/lalyos/xxx/raw/custum-blueprint.json
</span><span class='line'>cluster build --blueprint custom-blueprint
</span><span class='line'>cluster assign --hostGroup host_group_1 --host amb0.mycorp.kom
</span><span class='line'>cluster assign --hostGroup host_group_2 --host amb1.mycorp.kom
</span><span class='line'>cluster assign --hostGroup host_group_2 --host amb1.mycorp.kom
</span><span class='line'>cluster create</span></code></pre></td></tr></table></div></figure>


<p>In AmbariShell the <code>hint</code> command will always guide you on the happy path,
and remember that devops are lazy, so instead of typing press <code>&lt;TAB&gt;</code> for autocomplete or suggestions.</p>

<p>Autocomplete will help you to:</p>

<ul>
<li>complete the command in the given context (e.g. without any blueprint, cluster commands are not available)</li>
<li>add required parameters</li>
<li>add optional parameters: press tab after double dash <code>--&lt;TAB&gt;</code></li>
<li>complete parameter arguments, such as blueprint names, hostnames &hellip;</li>
</ul>


<h2>Summary</h2>

<p>Ever since we started to use Docker we are always developing against a multi-node
Hadoop cluster &ndash; as running a 3-4 node cluster in a laptop actually has less overhead
than working on a Sandbox VM.</p>

<p>We are <em>Dockerizing</em> the Hadoop ecosystem and simplifying the provisioning
process &ndash; watch this space or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>
for the latest news about <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> &ndash; the
open source cloud agnostic <em>Hadoop as a Service</em> API built on Docker.</p>

<p>Hope this helps and simplifies your development process &ndash; let us know how it goes
for you or if you need any help with Hadoop on Docker.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ambari provisioned Hadoop cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/"/>
    <updated>2014-06-17T08:51:14+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker</id>
    <content type="html"><![CDATA[<p>We are getting close to release and open source our <strong>Docker-based Hadoop Provisioning</strong> project.
The <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">slides</a>
were presented recently at the <a href="http://hadoopsummit.org/san-jose/">Hadoop Summit</a>, and
there is an interest from the community to learn the technical details.</p>

<p>The project &ndash; called <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> &ndash; will provide a REST API to provision a Hadoop cluster &ndash; anywhere. The cluster can be hosted
on AWS EC2, Azure, physical servers or even your laptop &ndash; we are adding more providers &ndash; but always based on the same concept:
<a href="http://ambari.apache.org/">Apache Ambari</a> managed <a href="http://www.docker.com/">Docker</a>
containers.</p>

<p>This blog entry is the first in a series, where we describe the Docker layer step-by-step:</p>

<ul>
<li>Single-node Docker based Hadoop &ldquo;cluster&rdquo; locally</li>
<li>Multi-node Docker based Hadoop cluster</li>
<li>Multi-node Docker based Hadoop cluster on EC2</li>
<li>Cloudbreak</li>
</ul>


<h2>Get Docker</h2>

<p>The only required software is Docker, so if you don&rsquo;t have it yet, jump to the
installation section of the <a href="https://docs.docker.com/installation/">official documentation</a>.</p>

<p>The very basic you need to work with Docker containers, is described in the
<a href="https://docs.docker.com/userguide/dockerizing/">users guide</a>.</p>

<h2>Single-node Cluster</h2>

<p>All setup is based on <a href="https://hub.docker.com/u/sequenceiq/">Docker images</a> only
the glue-code is different. Let&rsquo;s start with the most simple setup:</p>

<ul>
<li>start the first Docker container in the background that runs <strong>ambari-server</strong> and <strong>ambari-agent</strong>.</li>
<li>start the second Docker container which:

<ul>
<li>waits for the agent connecting to the server</li>
<li>starts an <a href="https://github.com/sequenceiq/ambari-shell">ambari-shell</a>, which will instruct ambari-server on its REST API:

<ul>
<li>define an <strong><a href="https://cwiki.apache.org/confluence/display/AMBARI/Blueprints">Ambari Blueprint</a></strong> by posting a JSON to <code>&lt;AMBARI_URL&gt;/api/v1/blueprints</code></li>
<li>create a Hadoop cluster by posting a JSON to <code>&lt;AMBARI_URL&gt;/api/v1/clusters</code> using the blueprint created in the previous step</li>
</ul>
</li>
</ul>
</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d -p 8080 -h amb0.mycorp.kom --name ambari-singlenode sequenceiq/ambari --tag ambari-server=true
</span><span class='line'>docker run -e BLUEPRINT=single-node-hdfs-yarn --link ambari-singlenode:ambariserver -t --rm --entrypoint /bin/sh sequenceiq/ambari -c /tmp/install-cluster.sh</span></code></pre></td></tr></table></div></figure>


<p>or if you want a <strong>twitter-sized</strong> one-liner to start with Hadoop in less than a minute:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -LOs j.mp/ambari-singlenode && . ambari-singlenode</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>When you pull the <code>sequenceiq/ambari</code> image first it will take a couple of minutes (for me it was 4 minutes).
Meanwhile you have started and running the download let&rsquo;s explain all those parameters.</p>

<h2>First container: ambari-server and ambari-agent</h2>

<p>Let&rsquo;s break down the parameters of the first container:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d -p 8080 -h amb0.mycorp.kom --name ambari-singlenode sequenceiq/ambari --tag ambari-server=true</span></code></pre></td></tr></table></div></figure>


<ul>
<li><strong>-d</strong> : Detached mode, container runs in the background</li>
<li><strong>-p 8080</strong> : Publish ambari web and REST API port</li>
<li><strong>-h amb0.mycorp.kom</strong> : hostname</li>
<li><strong>&mdash;name ambari-singlenode</strong> : assign a name to the container</li>
<li><strong>sequenceiq/ambari</strong> : the name of the image</li>
<li><strong>&mdash;tag ambari-server=true</strong> : the <em>command</em> but please note that this is appended to the <em>entrypoint</em>.</li>
</ul>


<p>The default <em>entrypoint</em> of the image is <code>start-serf-agent.sh</code>
<a href="https://github.com/sequenceiq/docker-ambari/blob/master/ambari-server/Dockerfile#L24">see the Dockerfile</a>
so the <code>--tag ambari-server=true</code> command is actually an argument of the <a href="http://www.serfdom.io/">serf agent</a>.</p>

<h3>Serf</h3>

<p>What is <a href="http://www.serfdom.io/">Serf</a>? The definition goes like:</p>

<blockquote><p>Serf is a decentralized solution for cluster membership, failure detection, and orchestration. Lightweight and highly available.</p></blockquote>

<p>Right now it doesn&rsquo;t seem to make any sense to talk about membership and cluster, but remember we want to
have the exact same process/tools for dev env and production.</p>

<p>The only Serf feature we use at this point is that you can define shell scripts based <strong>event-handlers</strong> for
each membership events:</p>

<ul>
<li>member-join</li>
<li>member-failed</li>
<li>member-leave</li>
<li>member-xxx</li>
</ul>


<p>The <strong>member-join</strong> event-handler script will check the Serf tags, defined by <code>--tag name=value</code>
and will start:
 &ndash; ambari-server java process: if the <strong>ambari-server</strong> tag is <strong>true</strong>
 &ndash; ambari-agent python process: if the <strong>ambari-agent</strong> tag is <strong>true</strong></p>

<p>You might noted that only the <strong>ambari-server</strong> tag is defined. The reason is that <strong>ambari-agent</strong> is defined as <strong>true</strong> by default.</p>

<h2>Second container: ambari-shell</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -e BLUEPRINT=single-node-hdfs-yarn --link ambari-singlenode:ambariserver -t --rm --entrypoint /bin/sh sequenceiq/ambari -c /tmp/install-cluster.sh</span></code></pre></td></tr></table></div></figure>


<ul>
<li><strong>-e BLUEPRINT=single-node-hdfs-yarn</strong> : the template to use for the cluster (single-node-hdfs-yarn/multi-node-hdfs-yarn/lambda-architecture) <a href="https://github.com/sequenceiq/ambari-rest-client/tree/master/src/main/resources/blueprints">see the blueprint JSON on GitHub</a></li>
<li><strong>&mdash;link ambari-singlenode:ambariserver </strong> :  it will make all exposed ports and the private IP of <code>ambari-singlenode</code> available as <code>AMBARISERVER_xxx</code> env variables</li>
<li><strong>-t</strong> : pseudo terminal, to see the progress</li>
<li><strong>&mdash;rm</strong> : remove the container once it&rsquo;s finished</li>
<li><strong>&mdash;entrypoint /bin/sh</strong> : the default entrypoint runs the shell in interactive mode, we want to overwrite it with a script specified as <code>/tmp/install-cluster.sh</code></li>
</ul>


<h1>Install completed</h1>

<p>Once Ambari-shell completed with the installation, you are ready to use it.
To find out the IP of the Ambari server run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker inspect -f "" ambari-singlenode</span></code></pre></td></tr></table></div></figure>


<p>To start with you can browse ambari web ui on <code>port 8080</code>. The default username/password is admin/admin.</p>

<p>or if you can&rsquo;t reach directly the private IP of the container (windows users), use the port exposed to the host:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker port ambari-singlenode 8080</span></code></pre></td></tr></table></div></figure>


<h1>Next steps</h1>

<p>In the upcomming blog posts we will do a multinode Hadoop cluster with the same toolset, so stay tuned &hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Summit 2014 - SequenceIQ slides]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/06/hadoop-summit-slides/"/>
    <updated>2014-06-06T13:42:11+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/06/hadoop-summit-slides</id>
    <content type="html"><![CDATA[<p>These are the slides of our presentation from the Hadoop Summit 2014, San Jose. We would like to thank all who have joined the session and the positive feedbacks we have received. This gives us a great confidence and validates our efforts that there is a great need to an easy and seamless Hadoop provisionig &ndash; let it be bare metal, cloud or other virtualizations.</p>

<p>Watch this space as <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> will be open sourced in the coming weeks.</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/35573123" width="640" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning" title="Docker based Hadoop provisioning - Hadoop Summit 2014 " target="_blank">Docker based Hadoop provisioning &ndash; Hadoop Summit 2014 </a> </strong> from <strong><a href="http://www.slideshare.net/JanosMatyas" target="_blank">Janos Matyas</a></strong> </div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Ambari + Spring Shell = Ambari Shell]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/"/>
    <updated>2014-05-26T13:42:11+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p><a href="http://ambari.apache.org/">Apache Ambari&rsquo;s</a> goal is to make Hadoop cluster management
as simple as possible. It provides an intuitive easy-to-use web UI backed by its RESTful API.
With only a few clicks you&rsquo;re able to install Hadoop services across any number of hosts
and Ambari will take care of the configurations as well. After the installation is complete
you can monitor your cluster taking leverage of <a href="http://ganglia.sourceforge.net/">Ganglia</a>
and <a href="http://www.nagios.org/">Nagios</a>.</p>

<p>At SequenceIQ we prefer to use command line tools whenever it&rsquo;s possible,
because it&rsquo;s much faster than interacting with a web UI and it&rsquo;s a better candidate for automation.
Here comes <a href="https://github.com/spring-projects/spring-shell#readme">Spring Shell</a> to our rescue.</p>

<p>It&rsquo;s an interactive shell that can be easily extended using a Spring based programming model and battle
tested in various projects like <a href="http://projects.spring.io/spring-roo/">Spring Roo</a>,
<a href="http://docs.spring.io/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/">Spring XD</a>, and
<a href="https://github.com/spring-projects/rest-shell">Spring REST Shell</a> Combine these two projects
and a really powerful tool will come to light.</p>

<h2>Ambari Shell</h2>

<p>The goal is to provide an interactive command line tool which supports:</p>

<ul>
<li>all functionality available through the REST API or Ambari web UI</li>
<li>makes possible complete automation of management task via <strong>scripts</strong></li>
<li>context aware command availability</li>
<li>tab completion</li>
<li>required/optional parameter support</li>
<li><strong>hint</strong> command to guide you on the usual path</li>
</ul>


<p>Since we&rsquo;re open sourcing the project, it should be available and part of the official Ambari repository
<a href="https://issues.apache.org/jira/browse/AMBARI-5482">soon</a>.</p>

<h2>Install Ambari Shell</h2>

<p>For now you have 3 options to give it a try:</p>

<ul>
<li>use our prepared <a href="https://index.docker.io/u/sequenceiq/ambari-shell/">docker image</a></li>
<li>download the latest self-containing executable jar form our maven repo</li>
<li>build it from source</li>
</ul>


<h3>Run via docker</h3>

<p>As we love to dockerize everything, we prepared a <a href="https://index.docker.io/u/sequenceiq/ambari-shell/">docker image</a>
with jdk1.7 on latest ubuntu, ambari-shell preinstalled. Detailed description is available on <a href="https://github.com/sequenceiq/ambari-shell-docker">github</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -it --rm sequenceiq/ambari-shell --ambari.host=&lt;HOSTNAME&gt; --ambari.port=&lt;PORT&gt;</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h3>Run latest snapshot</h3>

<p>You need only jdk 1.7. The script below will download the latest ambari-shell.jar into a
temporary folder, and give you instruction on how to use.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Ls https://raw.githubusercontent.com/sequenceiq/ambari-shell/master/latest-snap.sh | bash</span></code></pre></td></tr></table></div></figure>


<h3>Build from source</h3>

<p>If want to check the code, or extend it with new commands, Follow the steps below. You will need:
&ndash; jdk 1.7
&ndash; maven 3.x.x</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/sequenceiq/ambari-shell.git
</span><span class='line'>cd ambari-shell
</span><span class='line'>mvn package</span></code></pre></td></tr></table></div></figure>


<p>The shell is built as a single executable jar with the help of <a href="http://projects.spring.io/spring-boot/">Spring Boot</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>java -jar target/ambari-shell-1.0-SNAPSHOT.jar --ambari.host=&lt;HOSTNAME&gt; --ambari.port=&lt;PORT&gt;</span></code></pre></td></tr></table></div></figure>


<h2>Start a &ldquo;sandbox&rdquo; Ambari Server</h2>

<p>The image is available at the Docker repository, which means you only need to run the following to get a running Ambari server:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d -P -h server.ambari.com --name ambari-singlenode sequenceiq/ambari</span></code></pre></td></tr></table></div></figure>


<h2>Connect Ambari Shell to the server</h2>

<p>Once the server is up and running (10-20 sec) you can connect to it with the shell:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage:
</span><span class='line'>  java -jar ambari-shell.jar                  : Starts Ambari Shell in interactive mode.
</span><span class='line'>  java -jar ambari-shell.jar --cmdfile=&lt;FILE&gt; : Ambari Shell executes commands read from the file.
</span><span class='line'>
</span><span class='line'>Options:
</span><span class='line'>  --ambari.host=&lt;HOSTNAME&gt;       Hostname of the Ambari Server [default: localhost].
</span><span class='line'>  --ambari.port=&lt;PORT&gt;           Port of the Ambari Server [default: 8080].
</span><span class='line'>  --ambari.user=&lt;USER&gt;           Username of the Ambari admin [default: admin].
</span><span class='line'>  --ambari.password=&lt;PASSWORD&gt;   Password of the Ambari admin [default: admin].
</span><span class='line'>
</span><span class='line'>Note:
</span><span class='line'>  At least one option is mandatory.</span></code></pre></td></tr></table></div></figure>


<h2>Create a cluster</h2>

<p>All commands are context aware and are available only when it makes sense. For example the <code>cluster create</code> command is not available
until a blueprint hasn&rsquo;t been added or selected. A good approach is to use the <code>hint</code> command &ndash; as the Ambari UI, this will give
you hints about the available commands and the flow of creating or configuring a cluster. You can always use TAB for completion
or available parameters.</p>

<p>Initially there are no blueprints available &ndash; you can add blueprints from file or URL. For your convenience we&rsquo;ve added two
blueprints as defaults. You can get these blueprints by using the <code>blueprint defaults</code> command. The result is the following:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ambari-shell&gt; blueprint defaults
</span><span class='line'>ambari-shell&gt; blueprint list</span></code></pre></td></tr></table></div></figure>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  BLUEPRINT              STACK
</span><span class='line'>  ---------------------  -------
</span><span class='line'>  multi-node-hdfs-yarn   HDP:2.0
</span><span class='line'>  single-node-hdfs-yarn  HDP:2.0</span></code></pre></td></tr></table></div></figure>


<p>Once the blueprints are added you can use them to create a cluster by typing <code>cluster build --blueprint single-node-hdfs-yarn</code>.
Now that the blueprint is selected you have to assign the hosts to the available host groups. Use</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ambari-shell&gt; cluster build --blueprint single-node-hdfs-yarn
</span><span class='line'>CLUSTER_BUILD:single-node-hdfs-yarn&gt; cluster assign --hostGroup host_group_1 --host server.ambari.com
</span><span class='line'>
</span><span class='line'>  HOSTGROUP     HOST
</span><span class='line'>  ------------  -----------------
</span><span class='line'>  host_group_1  server.ambari.com</span></code></pre></td></tr></table></div></figure>


<p>Once you are happy with the host &ndash; host group associations you can choose <code>cluster create</code> to start building the cluster.
Progress can be checked either at the Amabri UI or using the <code>tasks</code> command.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>CLUSTER_BUILD:single-node-hdfs-yarn&gt; cluster create
</span><span class='line'>Successfully created the cluster
</span><span class='line'>CLUSTER:single-node-hdfs-yarn&gt; tasks
</span><span class='line'>
</span><span class='line'>  TASK                        STATUS
</span><span class='line'>  --------------------------  -------
</span><span class='line'>  HISTORYSERVER INSTALL       QUEUED
</span><span class='line'>  ZOOKEEPER_SERVER START      PENDING
</span><span class='line'>  ZOOKEEPER_CLIENT INSTALL    PENDING
</span><span class='line'>  HDFS_CLIENT INSTALL         PENDING
</span><span class='line'>  HISTORYSERVER START         PENDING
</span><span class='line'>  NODEMANAGER INSTALL         QUEUED
</span><span class='line'>  NODEMANAGER START           PENDING
</span><span class='line'>  ZOOKEEPER_SERVER INSTALL    QUEUED
</span><span class='line'>  YARN_CLIENT INSTALL         PENDING
</span><span class='line'>  NAMENODE INSTALL            QUEUED
</span><span class='line'>  RESOURCEMANAGER INSTALL     QUEUED
</span><span class='line'>  NAMENODE START              PENDING
</span><span class='line'>  RESOURCEMANAGER START       PENDING
</span><span class='line'>  DATANODE START              PENDING
</span><span class='line'>  SECONDARY_NAMENODE START    PENDING
</span><span class='line'>  DATANODE INSTALL            QUEUED
</span><span class='line'>  MAPREDUCE2_CLIENT INSTALL   PENDING
</span><span class='line'>  SECONDARY_NAMENODE INSTALL  QUEUED</span></code></pre></td></tr></table></div></figure>


<p>Each time you start the shell the executed commands are logged in a file line by line and later either with the <code>script</code> command
or specifying an <code>--cmdfile</code> option the same commands can be executed.</p>

<h2>Commands</h2>

<p>The currently supported commands are:</p>

<ul>
<li><code>blueprint add</code> &ndash; Add a new blueprint with either &mdash;url or &mdash;file</li>
<li><code>blueprint defaults</code> &ndash; Adds the default blueprints to Ambari</li>
<li><code>blueprint list</code> &ndash; Lists all known blueprints</li>
<li><code>blueprint show</code> &ndash; Shows the blueprint by its id</li>
<li><code>cluster assign</code> &ndash; Assign host to host group</li>
<li><code>cluster build</code> &ndash; Starts to build a cluster</li>
<li><code>cluster create</code> &ndash; Create a cluster based on current blueprint and assigned hosts</li>
<li><code>cluster delete</code> &ndash; Delete the cluster</li>
<li><code>cluster preview</code> &ndash; Shows the currently assigned hosts</li>
<li><code>cluster reset</code> &ndash; Clears the host &ndash; host group assignments</li>
<li><code>debug off</code> &ndash; Stops showing the URL of the API calls</li>
<li><code>debug on</code> &ndash; Shows the URL of the API calls</li>
<li><code>exit</code> &ndash; Exits the shell</li>
<li><code>hello</code> &ndash; Prints a simple elephant to the console</li>
<li><code>help</code> &ndash; List all commands usage</li>
<li><code>hint</code> &ndash; Shows some hints</li>
<li><code>host components</code> &ndash; Lists the components assigned to the selected host</li>
<li><code>host focus</code> &ndash; Sets the useHost to the specified host</li>
<li><code>host list</code> &ndash; Lists the available hosts</li>
<li><code>quit</code> &ndash; Exits the shell</li>
<li><code>script</code> &ndash; Parses the specified resource file and executes its commands</li>
<li><code>service components</code> &ndash; Lists all services with their components</li>
<li><code>service list</code> &ndash; Lists the available services</li>
<li><code>tasks</code> &ndash; Lists the Ambari tasks</li>
<li><code>version</code> &ndash; Displays shell version</li>
</ul>


<h2>Summary</h2>

<p>To sum it up in less than two minutes watch this video:</p>

<script type="text/javascript" src="https://asciinema.org/a/9783.js" id="asciicast-9783" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building the build environment with Ansible and Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker/"/>
    <updated>2014-05-09T11:51:57+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker</id>
    <content type="html"><![CDATA[<p>At SequenceIQ we put a strong emphasis on automating everything we can and this automation starts with our continuous integration &amp; delivery process.</p>

<h2>Introduction</h2>

<p>Lately there is a lot of buzz around continuous integration, development and deployment. More and more companies are moving away from long release cycles towards the &ldquo;release early, release often&rdquo; approach. The advantages of this approach are well known: lower overhead, earlier bug discovery and bug fixing, fewer context switches for the developers to name just a few. There are very good resources to learn about these concepts &ndash; blog posts by different companies (e.g.: by <a href="http://techblog.netflix.com/2013/08/deploying-netflix-api.html">Netflix</a>) and of course the <a href="http://www.amazon.com/dp/0321601912">book</a> &lsquo;Continuous Delivery&rsquo; by Jez Humble and David Farley &ndash; we&rsquo;ll now try to add our own experiences as well.</p>

<p>We&rsquo;ll share two blog posts about our continuous delivery at SequenceIQ: the first one being an introductory post about some tools we use to make the whole process easier and more robust, the second one explains the <a href="http://scottchacon.com/2011/08/31/github-flow.html">flow</a> we use from committing changes to being the changes available in our different environments.</p>

<h2>Tools</h2>

<p>Our CI and CD process at SequenceIQ is based on Ansible, Jenkins and of course Docker.
When we started to build our own process, we decided that we don&rsquo;t want to commit the same mistake that a lot of companies make about their build environment. At these companies the build servers where Jenkins and/or the other build tools are installed are often prepared once in the far past by someone who probably doesn&rsquo;t work there anymore. It quickly becomes something that everyone is afraid to touch and just hope that it will work forever. As the projects improve there will be a lot of different tools with a lot of different versions on the build machine and soon it leads to a small chaos, where the maintenance will involve a lot of hard manual work. To get rid of these problems, we use <a href="http://www.ansible.com/">Ansible</a> to &ldquo;build the build infrastructure&rdquo;, and Docker to run the builds in separated self-sufficient containers.</p>

<!-- more -->


<h2>Ansible</h2>

<p>We have an Ansible script which starts an EC2 instance in the cloud and provisions everything on this server automatically. This script can be easily executed with a single command from a developer laptop:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ansible-playbook -i hosts ci.yml</span></code></pre></td></tr></table></div></figure>


<p>To run this command Ansible, python and some python modules must be installed on the local machine. To avoid having different version of these tools on the development machines we automated the installation of our development environment too &ndash; maybe the topic of another post in the future.
So let&rsquo;s see how the Ansible script works exactly.</p>

<h3>Creating an instance in the cloud</h3>

<p>First it needs to start an instance in the AWS cloud, so it invokes our <strong>ec2 role</strong> on localhost:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Request and init EC2 instance</span>
</span><span class='line'>  <span class="l-Scalar-Plain">hosts</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">localhost</span>
</span><span class='line'>  <span class="l-Scalar-Plain">roles</span><span class="p-Indicator">:</span>
</span><span class='line'>     <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">ec2</span>
</span></code></pre></td></tr></table></div></figure>


<p>The ec2 role requests an EC2 spot priced instance and associates it with an elastic IP. We can easily use a spot priced instance because if it gets shut down by AWS we can recreate it in a few minutes! Ansible has a few <a href="http://docs.ansible.com/list_of_cloud_modules.html">cloud modules</a> which makes it quite easy to manage EC2 instances. Requesting a spot priced instance looks like this (the placeholders come from Ansible group variables):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Create an EC2 spot priced instance</span>
</span><span class='line'>  <span class="l-Scalar-Plain">local_action</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">module</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">ec2</span>
</span><span class='line'>  <span class="l-Scalar-Plain">key_name</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.keypair</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">group</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.security_group</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">instance_type</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.instance_type</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">spot_price</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.spot_price</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">image</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.image</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">wait</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">yes</span>
</span><span class='line'>  <span class="l-Scalar-Plain">region</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.region</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">id</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.idempotent_id</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">register</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">ec2result</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Provisioning the build server</h3>

<p>After the EC2 instance is running and accepting SSH connections, the script can go on and start to install the tools needed. Because almost everything is running in separated Docker containers, we only need 3 things: Docker, Nginx and Jenkins. Installing Docker is pretty easy as the new Amazon Linux AMIs are <a href="http://aws.amazon.com/amazon-linux-ami/2014.03-release-notes/">prepared</a> to run Docker. We only need to install it from Amazon&rsquo;s provided Software Repository, and start the service. It looks like this in the Ansible script:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Install Docker on Amazon Linux AMI</span>
</span><span class='line'>  <span class="l-Scalar-Plain">when</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">ansible_os_family == &quot;RedHat&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">yum</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">name=docker state=present</span>
</span><span class='line'>
</span><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Start Docker service</span>
</span><span class='line'>  <span class="l-Scalar-Plain">service</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">name=docker state=started</span>
</span></code></pre></td></tr></table></div></figure>


<p>After Docker is installed we can start some containers that are used by some Jenkins builds later (e.g.: a SonarQube server and a MySQL database that holds the results &ndash; we&rsquo;ve created publicly available <a href="https://index.docker.io/u/sequenceiq/sonar-server/">containers</a> on our Github page.</p>

<p>To install and configure Nginx we use an <a href="https://galaxy.ansible.com/list#/roles/466">existing role</a> from Ansible Galaxy that is well prepared and easily configurable. We are configuring Nginx to forward requests from port 80 to either Jenkins or Sonar.</p>

<p>For example the configuration for Jenkins is the following in the Ansible group variables:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">nginx_sites</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">default</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">listen 80</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">server_name jenkins.sequenceiq.com</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">location / {</span>
</span><span class='line'>       <span class="l-Scalar-Plain">proxy_pass http://jenkins;</span>
</span><span class='line'>       <span class="l-Scalar-Plain">proxy_redirect off;</span>
</span><span class='line'>       <span class="l-Scalar-Plain">proxy_set_header Host $host;</span>
</span><span class='line'>       <span class="l-Scalar-Plain">proxy_set_header X-Forwarded-Host $server_name;</span>
</span><span class='line'>      <span class="l-Scalar-Plain">}</span>
</span><span class='line'><span class="l-Scalar-Plain">nginx_configs</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">proxy</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">proxy_set_header X-Real-IP $remote_addr</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for</span>
</span><span class='line'>  <span class="l-Scalar-Plain">upstream</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">upstream jenkins { server 127.0.0.1:8080 weight=10; }</span>
</span></code></pre></td></tr></table></div></figure>


<p>The most difficult thing to install is Jenkins: we want our configurations and jobs to be instantly available as well. Jenkins has a <a href="https://wiki.jenkins-ci.org/display/JENKINS/Jenkins+CLI">command-line interface</a> that allows access from a script. It has a lot of built-in commands to manage jobs and other configurations and it even has a command to execute a Groovy script on the server. We use these features extensively to prepare the whole Jenkins environment from sketch. Our Jenkins role (that will be available soon on Ansible Galaxy) is able to do the following:
&ndash; Install Jenkins and its dependencies, and get the Jenkins CLI jar from the specified URL.
&ndash; Configure the global Jenkins properties like the mail server, or the properties needed for the Github pull request builder plugin &ndash; it is simply achieved by copying a global config.xml to the Jenkins home directory using Ansible&rsquo;s copy module.
&ndash; Install and update plugins through the Jenkins CLI. Installing plugins looks like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Install plugins</span>
</span><span class='line'>  <span class="l-Scalar-Plain">sudo</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">yes</span>
</span><span class='line'>  <span class="l-Scalar-Plain">shell</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">java -jar {{ jenkins.cli_dest }} -s http://localhost:8080/ install-plugin {{item.item}}</span>
</span><span class='line'>  <span class="l-Scalar-Plain">when</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">item.stdout.find(&#39;false&#39;) != -1</span>
</span><span class='line'>  <span class="l-Scalar-Plain">with_items</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">check_plugins.results</span>
</span><span class='line'>  <span class="l-Scalar-Plain">notify</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="s">&#39;Restart</span><span class="nv"> </span><span class="s">Jenkins&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Configure security (we use Github OAuth). The Jenkins CLI doesn&rsquo;t have any dedicated commands for setting security, but it can be configured with a Groovy script that can be invoked from the CLI:</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='groovy'><span class='line'><span class="kt">def</span> <span class="n">githubSecurityRealm</span> <span class="o">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="na">jenkinsci</span><span class="o">.</span><span class="na">plugins</span><span class="o">.</span><span class="na">GithubSecurityRealm</span><span class="o">(</span><span class="s2">&quot;https://github.com&quot;</span><span class="o">,</span> <span class="s2">&quot;https://api.github.com&quot;</span><span class="o">,</span> <span class="n">clientId</span><span class="o">,</span> <span class="n">clientSecret</span><span class="o">)</span>
</span><span class='line'><span class="kt">def</span> <span class="n">authorizationStrategy</span> <span class="o">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="na">jenkinsci</span><span class="o">.</span><span class="na">plugins</span><span class="o">.</span><span class="na">GithubAuthorizationStrategy</span><span class="o">(</span><span class="s2">&quot;admin1,admin2&quot;</span><span class="o">,</span><span class="kc">true</span><span class="o">,</span><span class="s2">&quot;organization name&quot;</span><span class="o">,</span><span class="kc">true</span><span class="o">,</span><span class="kc">false</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>
</span><span class='line'><span class="n">jenkins</span><span class="o">.</span><span class="na">model</span><span class="o">.</span><span class="na">Jenkins</span><span class="o">.</span><span class="na">instance</span><span class="o">.</span><span class="na">setSecurityRealm</span><span class="o">(</span><span class="n">githubSecurityRealm</span><span class="o">)</span>
</span><span class='line'><span class="n">jenkins</span><span class="o">.</span><span class="na">model</span><span class="o">.</span><span class="na">Jenkins</span><span class="o">.</span><span class="na">instance</span><span class="o">.</span><span class="na">setAuthorizationStrategy</span><span class="o">(</span><span class="n">authorizationStrategy</span><span class="o">)</span>
</span><span class='line'><span class="n">jenkins</span><span class="o">.</span><span class="na">model</span><span class="o">.</span><span class="na">Jenkins</span><span class="o">.</span><span class="na">instance</span><span class="o">.</span><span class="na">save</span><span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li><p>Copy private keys for Github builds &ndash; it simply copies the predefined private SSH keys from a local directory to the <code>~/.ssh</code> directory of the Jenkins user. We use a dedicated Github user to communicate with Github from Jenkins.</p></li>
<li><p>Creating jobs from XML configuration. The Jenkins CLI supports the creation of Jenkins jobs through the create-job command that accepts an XML file as input that defines the Jenkins job. Currently our Jenkins role works by invoking this command for every job that is defined in the variables and has a corresponding XML file in a predefined directory.  We are planning to later modify this role to have a template that holds the structure of a Jenkins job XML so it won&rsquo;t be needed to create the whole XML file manually, only the required parameters among the Ansible variables.</p></li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Create jenkins jobs</span>
</span><span class='line'>  <span class="l-Scalar-Plain">shell</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">java -jar {{ jenkins.cli_dest }} -s http://localhost:8080/ create-job {{ item }} &lt; {{ jenkins.dest }}/{{item}}.xml</span>
</span><span class='line'>  <span class="l-Scalar-Plain">with_items</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">jenkins_jobs</span>
</span><span class='line'>  <span class="l-Scalar-Plain">when</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">existing_jobs.changed and existing_jobs.stdout.find(&#39;{{ item }}&#39;) == -1</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Docker</h2>

<p>The other tool besides Ansible that we use extensively in our build environment is Docker. Docker is a quickly expanding technology that enables the creation of lightweight application containers. If you don&rsquo;t know about Docker yet, check out the official <a href="https://www.docker.io/gettingstarted/">Getting Started guide</a> or our own <a href="http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction/">blog post</a> about it.
With the help of Docker we don&rsquo;t need to worry about the tools needed for the builds or its dependencies on the continuous integration server as they are packaged in separate containers. Every one of our builds on Jenkins are only a few lines that runs a container, maybe copies something out of it and removes the container after it finished. We provide a few environment variables, some shared directories or some links between containers where needed. One of our jobs in Jenkins that builds the master branch looks like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#!/bin/bash</span>
</span><span class='line'>docker run -i --name <span class="nv">$BUILD_TAG</span> <span class="se">\</span>
</span><span class='line'>-v <span class="s2">&quot;/var/lib/jenkins/.gradle-api:/root/.gradle:rw&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;SONAR_USERNAME=$SONAR_USERNAME&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;SONAR_PW=$SONAR_PW&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;BUILD_NUMBER=$BUILD_NUMBER&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;KEY=$(cat /var/lib/jenkins/.ssh/id_rsa| base64 -w 0)&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;REPO=$REPO_ADDRESS&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;BRANCH=master&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;BUILD_TASKS=clean build sonarRunner uploadArchives&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;BUILD_ENV=jenkins&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;GRADLE_OPTS=-XX:MaxPermSize=512m&quot;</span> <span class="se">\</span>
</span><span class='line'>--link sonar_server:sonar <span class="se">\</span>
</span><span class='line'>--link sonar_mysql:sonar_db <span class="se">\</span>
</span><span class='line'>sequenceiq/build /etc/build-project.sh
</span><span class='line'>sleep 5
</span><span class='line'>docker cp <span class="nv">$BUILD_TAG</span>:/tmp/prj/build/build.info <span class="nv">$WORKSPACE</span>
</span><span class='line'>docker rm <span class="nv">$BUILD_TAG</span>
</span></code></pre></td></tr></table></div></figure>


<p>And not only our builds run in Docker, some other tools we use on the build environment also run in containers. For instance our code quality management tool, SonarQube and the MySQL database it uses also runs in separate containers. This way we don&rsquo;t need to install them on the EC2 instance directly, we only need to link them where needed &ndash; see the example above!</p>

<p>In our next blog post about continuous integration we&rsquo;ll explain the process we use at SequenceIQ to continuously deliver the new features to production using the Github flow with Jenkins and Docker.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Job profiling with R]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/05/01/mapreduce-job-profiling-with-R/"/>
    <updated>2014-05-01T21:08:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/05/01/mapreduce-job-profiling-with-R</id>
    <content type="html"><![CDATA[<p>Management of a large Hadoop cluster is not an easy task &ndash; however thanks to projects like <a href="http://ambari.apache.org/">Apache Ambari</a> these tasks are getting easier. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its REST API to provision, manage and monitor a Hadoop cluster. While Ambari helps us a lot to monitor a cluster (leverages <a href="http://ganglia.sourceforge.net/">Ganglia</a> and <a href="http://www.nagios.org/">Nagios</a>), many times we have to profile our MapReduce jobs as well.</p>

<p>At SequenceIQ in order to profile MapReduce jobs, understand (job)internal statistics and create usefull graphs many times we rely on <a href="http://www.r-project.org/">R</a>. The metrics are collected from Ambari and the <a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html">YARN History Server</a>.</p>

<p>In this blog post we would like to explain and guide you through a simple process of collecting MapReduce job metrics, calculate different statistics and generate easy to understand charts.</p>

<p>The MapReduce application is the following:</p>

<ul>
<li>The input set of data is 12 pieces of 1 GB size files. Each file containes the same line of 16 bytes (012345678998765 plus the new line character)</li>
<li>The number of mappers running is 48, because the block size on HDFS is 256 MB and there are 12 files.</li>
<li>We use TextInputFormat (line num, line content) pairs. The output of the mapper function is the same as the input <code>IdentityMapper</code></li>
<li>The number of reducers is 20.</li>
<li>For simplicity we use <code>IdentityReducer</code> as the reducer function.</li>
<li>We use a special partitioner called <code>LinePartitoner</code>. The partitioning is based on line numbers (the key) and it makes sure that each reducer gets the same amount of data (line number <em>modulo</em> reducer number).</li>
</ul>


<h2>How to get the job results with R</h2>

<p>The job id that we are analysing with R is job_1395530889914_0005 (<em>replace this with your job is</em>)</p>

<p>First we load the R functions:</p>

<p><code>source("JobHistory.r")</code></p>

<p>Then we extract/read the job from the HistoryServer. It is actually using the Rest API of HistoryServer, parsing the JSON output.</p>

<p><code>job&lt;-getJob("job_1395530889914_0005","node02:19888")</code></p>

<p>The structure of the job follows the structure that is returned from the HistoryServer except that for example the parameters of all the tasks are converted into vectors so that can be easily handled in R.</p>

<!-- more -->


<p>A job is a list of <code>things</code>:</p>

<p><code>&gt; names(job)</code></p>

<p><code>[1] "job"      "counters" "tasks"    "attempts"</code></p>

<p>The job$job contains some basic data</p>

<p><code>&gt; names(job$job)</code></p>

<p><code>[1] "startTime"                "finishTime"               "id"                       "name"                     "queue"</code></p>

<p><code>[6] "user"                     "state"                    "mapsTotal"                "mapsCompleted"            "reducesTotal"</code></p>

<p><code>[11] "reducesCompleted"         "uberized"                 "diagnostics"              "avgMapTime"               "avgReduceTime"</code></p>

<p><code>[16] "avgShuffleTime"           "avgMergeTime"             "failedReduceAttempts"     "killedReduceAttempts"     "successfulReduceAttempts"</code></p>

<p><code>[21] "failedMapAttempts"        "killedMapAttempts"        "successfulMapAttempts"</code></p>

<p>The items below job$tasks are all vectors (if there are numeric) or non-named lists:</p>

<p><code>&gt; names(job$tasks)</code></p>

<p><code>[1] "startTime"         "finishTime"        "elapsedTime"       "progress"          "id"          "state"             "type"</code></p>

<p><code>[8] "successfulAttempt"</code></p>

<p>This way we can easily calculate the mean of the <code>running</code> times of all the tasks like this:</p>

<p><code>mean(job$tasks$finishTime-job$tasks$startTime)</code></p>

<p><code>[1] 147307</code></p>

<p>The <code>attempts</code> list also contains vectors or lists of parameters. Only the successful attempts are in the attempt list.</p>

<p><code>&gt; names(job$attempts)</code></p>

<p><code>[1] "startTime"           "finishTime"          "elapsedTime"         "progress"            "id"                  "rack"</code></p>

<p><code>[7] "state"               "nodeHttpAddress"     "diagnostics"         "type"                "assignedContainerId" "shuffleFinishTime"</code></p>

<p><code>[13] "mergeFinishTime"     "elapsedShuffleTime"  "elapsedMergeTime"    "elapsedReduceTime"</code></p>

<p>This way we can easily calculate the average <code>merge</code> times:</p>

<p><code>&gt; mean(job$attempts$mergeFinishTime-job$attempts$shuffleFinishTime)</code></p>

<p><code>[1] 4875.15</code></p>

<p>Which is the same as:</p>

<p><code>&gt; mean(job$attempts$elapsedMergeTime)</code></p>

<p><code>[1] 4875.15</code></p>

<h2>The R generated graphs</h2>

<p>The are two types of graphs for the beginning</p>

<p><code>plotTasksTimes(job)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_mr_task_times.png" alt="" /></p>

<p>This graph shows start and finish times for each tasks (mappers and reducers as well). The tasks are sorted by their start times, so the reducers are on the top. There are 48 mappers and 20 reducers. The times are relative to the startTime of the first mapper in milliseconds(could show absolute values as well).</p>

<p><code>plotActiveMRTasksNum(job)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_mr.png" alt="" /></p>

<p>The graph above contains the number of active tasks at each time. It shows the mappers with green and also show the reduce phases as well. The shuffle part is orange, the merge part is magenta and the reduce part (reducer function is running) is blue. The times are relative to the startTime of the first mapper in milliseconds (could show absolute values as well).</p>

<p><code>plotActiveReduceTasksNumDetailed(job, FALSE)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_reduce_phases.png" alt="" /></p>

<p>This graph shows only the reduce part with the three phases: shuffle, merge, reduce. The times are absolute times (could show absolute values as well).</p>

<p><code>plotTimeBoxes&lt;-function(data, nodeNum=21, slotsPerNode=4)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_mr_by_nodes.png" alt="" /></p>

<p>As you can see monitoring a MapReduce job through the HistoryServer it is extremely easy, and R is very usefull to apply different statistics and plot graphs. Also as you start playing with different setups the results can quickly be retrived, the graphs regenerated to analyze how different configuratins are affecting the execution time/behaviour of the jobs.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/96_mappers_20_reducers_mr_by_nodes.png" alt="" /></p>

<p>As always, the example project is available at our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-monitoring-R">GitHub</a> page. We are working on a <code>heuristic</code> queue scheduler for a better utilization of our cluster, and also to provide QoS on Hadoop &ndash; profiling and understanding the running MapReduce jobs and the job queues are essential for that. Also based on the charts broken down by nodes we can quickly identify servers with potential issues (slow I/O, memory, etc).</p>

<p>Follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a> to read about how we progress with the sceduler and get early access, or feel free to contribute to our YARN monitoring project.</p>
]]></content>
  </entry>
  
</feed>
