<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com//atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com//"/>
  <updated>2014-03-08T22:57:48+00:00</updated>
  <id>http://blog.sequenceiq.com//</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HDFS and java.nio.channels]]></title>
    <link href="http://blog.sequenceiq.com//blog/2014/03/07/read-from-hdfs/"/>
    <updated>2014-03-07T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com//blog/2014/03/07/read-from-hdfs</id>
    <content type="html"><![CDATA[<p>Many times there is a need to access files or interact with HDFS from Java applications or libraries. Hadoop has built in many tools in order to work or interact with HDFS &ndash; however in case you&rsquo;d like to read into a content of a file remotely (e.g. retrieve the headers of a CSV/TSV file) random exceptions can occurs. One of these remote exceptions coming from the HDFS NameNode is a <em>java.io.IOException: File /user/abc/xyz/ could only be replicated to 0 nodes, instead of 1.</em></p>

<p>Such an exception can be reproduced by the following code snippet:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>java BufferedInputStream bufferedInputStream
</span><span class='line'>
</span><span class='line'>/**
</span><span class='line'> * For the sake of readability, try/cacth/finally blocks are removed 
</span><span class='line'> * Don't Say We Didn't Warn You
</span><span class='line'> */
</span><span class='line'>
</span><span class='line'>FileSystem fs = FileSystem.get(configuration);
</span><span class='line'>          Path filePath = getFilePath(dataPath);
</span><span class='line'>
</span><span class='line'>BufferedInputStream bufferedInputStream = new BufferedInputStream(fs.open(filePath));
</span><span class='line'>  listReader = new CsvListReader(new BufferedReader(new InputStreamReader(bufferedInputStream)),
</span><span class='line'>                      CsvPreference.STANDARD_PREFERENCE);
</span><span class='line'>                     </span></code></pre></td></tr></table></div></figure>


<p>The exception looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ERROR SimpleFeatureSelector:67 - Exception {}
</span><span class='line'>java.lang.IllegalStateException: Must not use direct buffers with InputStream API
</span><span class='line'>  at com.google.common.base.Preconditions.checkState(Preconditions.java:176)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
</span><span class='line'>  at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:170)</span></code></pre></td></tr></table></div></figure>


<p><em>Note: actually all HDFS operations fail in case of the underlying input stream does not have a readable channel (check the java.nio.channels package. RemoteBlockReader2 needs channel based inputstreams to deal with direct buffers.</em></p>

<p>Digging into details and checking the Hadoop 2.2 source code we find the followings:</p>

<p>Through the<code>org.apache.hadoop.hdfs.BlockReaderFactory</code> you can get access to a BlockReader implementation like <code>org.apache.hadoop.hdfs.RemoteBlockReader2</code>, which it is responsible for reading a single block from a single datanode.</p>

<p>The blockreader is retrieved in the following way:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@SuppressWarnings</span><span class="o">(</span><span class="s">&quot;deprecation&quot;</span><span class="o">)</span>
</span><span class='line'><span class="kd">public</span> <span class="kd">static</span> <span class="n">BlockReader</span> <span class="nf">newBlockReader</span><span class="o">(</span>
</span><span class='line'>                                     <span class="n">Conf</span> <span class="n">conf</span><span class="o">,</span>
</span><span class='line'>                               <span class="n">Socket</span> <span class="n">sock</span><span class="o">,</span> <span class="n">String</span> <span class="n">file</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">ExtendedBlock</span> <span class="n">block</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">Token</span><span class="o">&lt;</span><span class="n">BlockTokenIdentifier</span><span class="o">&gt;</span> <span class="n">blockToken</span><span class="o">,</span>
</span><span class='line'>                                     <span class="kt">long</span> <span class="n">startOffset</span><span class="o">,</span> <span class="kt">long</span> <span class="n">len</span><span class="o">,</span>
</span><span class='line'>                                     <span class="kt">int</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="kt">boolean</span> <span class="n">verifyChecksum</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">String</span> <span class="n">clientName</span><span class="o">)</span>
</span><span class='line'>                                     <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">conf</span><span class="o">.</span><span class="na">useLegacyBlockReader</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">RemoteBlockReader</span><span class="o">.</span><span class="na">newBlockReader</span><span class="o">(</span>
</span><span class='line'>          <span class="n">sock</span><span class="o">,</span> <span class="n">file</span><span class="o">,</span> <span class="n">block</span><span class="o">,</span> <span class="n">blockToken</span><span class="o">,</span> <span class="n">startOffset</span><span class="o">,</span> <span class="n">len</span><span class="o">,</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="n">verifyChecksum</span><span class="o">,</span> <span class="n">clientName</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">RemoteBlockReader2</span><span class="o">.</span><span class="na">newBlockReader</span><span class="o">(</span>
</span><span class='line'>          <span class="n">sock</span><span class="o">,</span> <span class="n">file</span><span class="o">,</span> <span class="n">block</span><span class="o">,</span> <span class="n">blockToken</span><span class="o">,</span> <span class="n">startOffset</span><span class="o">,</span> <span class="n">len</span><span class="o">,</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="n">verifyChecksum</span><span class="o">,</span> <span class="n">clientName</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>In order to avoid the exception and use the right version of the block reader, the followin property <code>conf.useLegacyBlockReader</code> should be TRUE.</p>

<p>Long story short, the configuration set of a job should be set to: <code>configuration.set("dfs.client.use.legacy.blockreader", "true")</code>.</p>

<p>Unluckily in all cases when interacting with HDFS, and the underlying input stream does not have a readable channel, you can&rsquo;t use the <em>RemoteBlockReader2</em> implementation, but you have to fall back to the old legacy <em>RemoteBlockReader</em>.</p>

<p>Hope this helps,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Accessing HDP2 sandbox from the host]]></title>
    <link href="http://blog.sequenceiq.com//blog/2014/03/05/access-hdp2-sandbox/"/>
    <updated>2014-03-05T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com//blog/2014/03/05/access-hdp2-sandbox</id>
    <content type="html"><![CDATA[<p>During development of a Hadoop project people have many options of where and how to run Hadoop. We at SequenceIQ use different environments as well (cloud based, VM or host) &ndash; and different versions/vendor distributions. A very popular distribution among developers is the Hortonworks Sandbox &ndash; which contains the latest releases across Hadoop (2.2.0) and the key related projects into a single integrated and tested platform.
While using the sandbox gets you going running a single node Hadoop (pseudo distributed) in less than 5 minutes, many developers find inconvenient to &lsquo;live&rsquo; and work inside the VM when deploying, debugging or submitting jobs into a Hadoop cluster.</p>

<p>There is a well documented VM host file configuration on the <a href="http://docs.hortonworks.com/">Hortonworks site</a> describing how to start interacting with the VM sandbox from outside (e.g host machine), but quite soon this will turn into a port-forwarding saga (those who know how many ports does Hadoop and the ecosystem use will know what we mean). An easier and more elegant way is to use a SOCKS5 proxy (which comes with SSL by default).
Check this short goal/problem/resolution and code example snippet if you&rsquo;d like to interact with the Hortonworks Sandbox from your host (outside the VM).</p>

<h2>Goal</h2>

<ul>
<li>accessing the pseudo distributed hadoop cluster from the  host</li>
<li>reading / writing to the  HDFS</li>
<li>submitting  M/R jobs to the RM</li>
</ul>


<h2>Problem(s)</h2>

<ul>
<li>it&rsquo;s hard to reach resources inside the sandbox (e.g. interact with HDFS, or the DataNode)</li>
<li>lots of ports need to be portforwarded</li>
<li>entries to be added to the hosts file of the  host machine</li>
<li>circumstantial configuration of clients  accessing the sandbox</li>
</ul>


<h2>Resolution</h2>

<ul>
<li>use an SSL socks proxy</li>
</ul>


<h2>Example</h2>

<ul>
<li>check the following sample from our <em><a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/hdp-sandbox-access">GitHub page</a></em></li>
</ul>


<p>Start the SOCKS proxy</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>ssh root@127.0.0.1 -p 2222 -D 1099
</span></code></pre></td></tr></table></div></figure>


<p>Once the proxy is up and running, edit the core-site.xml</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>      <span class="nt">&lt;name&gt;</span>hadoop.socks.server<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>      <span class="nt">&lt;value&gt;</span>localhost:1099<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>      <span class="nt">&lt;name&gt;</span>hadoop.rpc.socket.factory.class.default<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>      <span class="nt">&lt;value&gt;</span>org.apache.hadoop.net.SocksSocketFactory<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now you can run the test client</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>     
</span><span class='line'><span class="c"># You can use Maven</span>
</span><span class='line'>mvn <span class="nb">exec</span>:java -Dexec.mainClass<span class="o">=</span><span class="s2">&quot;com.sequenceiq.samples.SandboxTester&quot;</span> -Dexec.args<span class="o">=</span><span class="s2">&quot;hdfs sandbox 8020&quot;</span> -Dhadoop.home.dir<span class="o">=</span>/tmp
</span><span class='line'>      
</span><span class='line'><span class="c"># or run from the JAR file</span>
</span><span class='line'>      
</span><span class='line'>java -jar sandbox-playground-1.0.jar hdfs sandbox 8020
</span></code></pre></td></tr></table></div></figure>


<p>As you see it&rsquo;s pretty easy and convenient to use the Hortonworks sandbox as a pre-configured development environment.</p>

<p>In case you&rsquo;d like to use (as we do most of the time) a Hadoop cluster in the cloud (Amazon EC2), check our previous blog post <a href="http://blog.sequenceiq.com/blog/2014/02/07/hdp2-on-amazon/">HDP2 on Amazon</a>.
In case you ever wondered whether it&rsquo;s possible to use Hadoop with Docker please follow this blog.</p>

<p>Hope this helps,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ETL - producing better quality data]]></title>
    <link href="http://blog.sequenceiq.com//blog/2014/02/28/etl-and-data-quality/"/>
    <updated>2014-02-28T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com//blog/2014/02/28/etl-and-data-quality</id>
    <content type="html"><![CDATA[<p>On my way to work this morning I read an interesting article about the quality of data being produced by different systems and applications. While the article was emphasizing that the quality of the data should not be an IT problem (but management), our believe is that at the high volume, velocity and variety (the &ldquo;3Vs&rdquo; of big data) the data is produced today, the process of producing data is a shared responsibility between management and the IT department.</p>

<p>Since the emerging of Hadoop, the TCO of storing large amounts of data in HDFS is lower than ever before &ndash; and now it makes sense to store all the data an enterprise produces in order to find patterns, correlations and break the data silos &ndash; something which was very specific for different departments within an organization. Storing such an amount of data (structured, unstructured, logs, clickstream, etc) inevitable produces a &lsquo;bad&rsquo; data quality &ndash; but this depends on your point of view. For us data is just data &ndash; we don&rsquo;t want to qualify it &ndash; and has it&rsquo;s own intrinsic value, but the quality of it depends on the ETL process. When someone engages with our API and the xTract Spacetime platform, among the first step is the configuration of data sources, and the attached ETL processes. We offer an extremely sophisticated ETL process and the ability to &lsquo;clean&rsquo; the data (batch or streaming) while arrives into xTract Spacetime, but we always suggest our customers to keep the raw data as well.</p>

<p>During the architecture of the xTract Spacetime platform we have tried and PoCd different ETL frameworks and implementations &ndash; and we choose <a href="https://github.com/kite-sdk/kite/tree/master/kite-morphlines">Kite Morphlines</a> being at the core of our ETL process. Morphlines is an open source framework that reduces the time and skills necessary to build and change Hadoop ETL stream processing applications that extract, transform and load data into Apache Solr, HBase, HDFS, Enterprise Data Warehouses, or Analytic Online Dashboards.</p>

<!-- more -->


<p>Since runs on Hadoop, scalability is not a problem &ndash; we have seen enterprises producing 50 terabytes data per day and missing the 24 hour ETL window, by not being able to scale horizontally their ETL processes. Morphlines is built on top of the Kite framework (a great framework for making easier to build systems on top of the Hadoop), and it&rsquo;s extremely easy to extend. We would like to show and give you examples about how to use and create a Morphlines Command to implement your custom transformation (if the many existing ones does not fit your requirement).</p>

<p>The implementation of a Command starts with implementing a CommandBuilder</p>

<p>Actually a new morphline command implementation is not that hard. You have to implement a builder class (see below), define the name of the command and  extend the AbstractCommand base class. That simple.</p>

<figure class='code'><figcaption><span>ToLowerCaseBuilder implements CommandBuilder</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'>  <span class="kd">public</span> <span class="n">Collection</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="nf">getNames</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">Collections</span><span class="o">.</span><span class="na">singletonList</span><span class="o">(</span><span class="s">&quot;toLowerCase&quot;</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span>toLowerCase Morphlines command</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'><span class="kd">protected</span> <span class="kt">boolean</span> <span class="nf">doProcess</span><span class="o">(</span><span class="n">Record</span> <span class="n">record</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">ListIterator</span> <span class="n">iter</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">fieldName</span><span class="o">).</span><span class="na">listIterator</span><span class="o">();</span>
</span><span class='line'>  <span class="k">while</span> <span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="na">hasNext</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">iter</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">transformFieldValue</span><span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="na">next</span><span class="o">()));</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>    <span class="k">return</span> <span class="kd">super</span><span class="o">.</span><span class="na">doProcess</span><span class="o">(</span><span class="n">record</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="kd">private</span> <span class="n">Object</span> <span class="nf">transformFieldValue</span><span class="o">(</span><span class="n">Object</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="na">toString</span><span class="o">().</span><span class="na">toLowerCase</span><span class="o">(</span><span class="n">locale</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To configure your new morhline command</p>

<figure class='code'><figcaption><span>toLowerCase config</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">morphlines</span> <span class="o">:</span> <span class="o">[</span>
</span><span class='line'>  <span class="o">{</span>
</span><span class='line'>   <span class="n">id</span> <span class="o">:</span> <span class="n">morphline1</span>
</span><span class='line'>       <span class="n">importCommands</span> <span class="o">:</span> <span class="o">[</span><span class="s">&quot;com.sequenceiq.samples.**&quot;</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>       <span class="n">commands</span> <span class="o">:</span> <span class="o">[</span>
</span><span class='line'>         <span class="o">{</span>
</span><span class='line'>           <span class="n">toLowerCase</span> <span class="o">{</span>
</span><span class='line'>             <span class="n">field</span> <span class="o">:</span> <span class="n">Name</span>
</span><span class='line'>             <span class="n">locale</span> <span class="o">:</span> <span class="n">en_us</span>
</span><span class='line'>           <span class="o">}</span>
</span><span class='line'>         <span class="o">}</span>
</span><span class='line'>       <span class="o">]</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>There is a custom tester tool which waiting a file as an input, a file as a config and an expected output. Among our plans is to build a UI on top of Kite Morphlines as well &ndash; part of our product stack. While we consider the Morhplines configuration, and defining the transformations simple and easy to use, many of our users might prefer a custom UI whee you can define your own ETL process visually.</p>

<p>That’s it. You can find the samples at our <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub page</a>.
Hope you enjoy it, let us know if you need help or have any questions.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vote for us - 2014 Hadoop Summit San Jose]]></title>
    <link href="http://blog.sequenceiq.com//blog/2014/02/26/vote-for-us/"/>
    <updated>2014-02-26T10:17:04+00:00</updated>
    <id>http://blog.sequenceiq.com//blog/2014/02/26/vote-for-us</id>
    <content type="html"><![CDATA[<p>While we are extremely proud that our abstract came 2nd (out of 107) in the 2014 Hadoop Summit in Amsterdam (see you all there in April 2-3), we will not stop there and our plan is to continue the hard work and we&rsquo;re looking forward to meet you at 2014 Hadoop Summit in San Jose.
We would like to ask for your support by submitting your vote for our session in the largest Hadoop conference in the world.</p>

<p>Please use the following link to vote, or read our abstract below.</p>

<p><a href="http://hadoopsummit.uservoice.com/forums/242807-hadoop-deployment-operations-track/suggestions/5568417-moving-to-hadoop-2-0-yarn-at-sequenceiq">Vote for us</a></p>

<p>Should you have any questions regarding our abstract, the technical solution or implementation detailsm feel free to contact us or
check our <a href="https://github.com/sequenceiq">GitHub</a> page.</p>

<p><strong>Moving to Hadoop 2.0/YARN at SequenceIQ</strong></p>

<p>A showcase of our efforts to bring all our Hadoop based applications under one common cluster management framework &ndash; YARN.
Our deployment consists of MR2, HBase, Mahout and Hive-all running within one single auto-scaling cluster. We have faced many challenges such as load imbalances, SLA misses, cluster scheduling and VM container deployments &ndash; and would like to share our struggle and solution with the community.
As a startup, cost savings is important for us &ndash; switching to Hadoop 2.0 helped us save significant costs through better utilization of our hardware and cloud VMs. Our decision and investment of moving to YARN has paid off &ndash; and opened up new business and technical opportunities.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Custom Apache Flume source]]></title>
    <link href="http://blog.sequenceiq.com//blog/2014/02/22/custom-flume-source/"/>
    <updated>2014-02-22T14:45:48+00:00</updated>
    <id>http://blog.sequenceiq.com//blog/2014/02/22/custom-flume-source</id>
    <content type="html"><![CDATA[<p>The process of data analytics starts with collecting the data into a common system, in our case a Hadoop cluster. Flume is an Apache project aiming to help us solve this problem in a very efficient and elegant way.</p>

<p>In Flume terminology a source is responsible to listen and consume events coming from many distributed clients and forwards them to one or more channels. Events can have any arbitrary format, it all depends on what source do we use. Flume provides many sources, but only a few of them is capable to collect data through network.</p>

<p>In this article I will discuss how you can implement your own that meets your demands through creating a websocket source.
There are two types of sources: event driven and pollable. In case of a pollable source, Flume will start a thread to periodically call the following method to check whether there is new data available or not:</p>

<figure class='code'><figcaption><span>PollableSource interface</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="n">Status</span> <span class="nf">process</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">EventDeliveryException</span><span class="o">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>With event driven source you will have to take care yourself of receiving the data from the clients. For our websocket example we will use embedded Jetty 9.1. Extend the AbstractEventDrivenSource class and override the mandatory methods to bootstrap the source. In the doConfigure method you can ask the properties you need from the context. These properties are coming from your agent’s configuration file. More on this later..</p>

<figure class='code'><figcaption><span>protected void doConfigure(Context context)</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">host</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="n">HOST_KEY</span><span class="o">);</span>
</span><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">port</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getInteger</span><span class="o">(</span><span class="n">PORT_KEY</span><span class="o">);</span>
</span><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">path</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="n">PATH_KEY</span><span class="o">);</span>
</span><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">enableSsl</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getBoolean</span><span class="o">(</span><span class="n">SSL_KEY</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Eventually the doStart will kick off the embedded Jetty as shown:</p>

<figure class='code'><figcaption><span>protected void doStart()</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="k">try</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">JettyWebSocketServer</span> <span class="n">server</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JettyWebSocketServer</span><span class="o">(</span><span class="n">host</span><span class="o">,</span> <span class="n">port</span><span class="o">,</span> <span class="n">path</span><span class="o">,</span> <span class="n">getChannelProcessor</span><span class="o">());</span>
</span><span class='line'>    <span class="n">server</span><span class="o">.</span><span class="na">start</span><span class="o">();</span>
</span><span class='line'><span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">LOGGER</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="s">&quot;Error starting jetty server&quot;</span><span class="o">,</span> <span class="n">e</span><span class="o">);</span>
</span><span class='line'>    <span class="k">throw</span> <span class="k">new</span> <span class="nf">FlumeException</span><span class="o">(</span><span class="n">e</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>Channel processor plays an important role here. Its purpose to forward the incoming events to the configured channels.</p>

<p>Creating an embedded Jetty server is pretty easy and straightforward even with SSL support. I am not going into details you can find the source code here <a href="https://github.com/sequenceiq/sequenceiq-samples">https://github.com/sequenceiq/sequenceiq-samples</a> You will have to create a Servlet which will create a new listener for every session or you can just simply ignore some requests based on different headers. On new message all you have to do is create a flume event out of it and pass is to the channelprocessor.</p>

<figure class='code'><figcaption><span>public void onWebSocketText(String s) </span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">SimpleEvent</span> <span class="n">event</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SimpleEvent</span><span class="o">();</span>
</span><span class='line'><span class="n">event</span><span class="o">.</span><span class="na">setBody</span><span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="na">getBytes</span><span class="o">());</span>
</span><span class='line'><span class="n">channelProcessor</span><span class="o">.</span><span class="na">processEvent</span><span class="o">(</span><span class="n">event</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>From this point the data will travel through the configured channels and sinks to end up on its final destination. It is committed in one transaction so if any component fails the whole process fails.</p>

<p>To deploy your custom source put the packaged jar to Flume’s classpath.</p>

<blockquote><p>Flume now supports a special directory called plugins.d which automatically picks up plugins that are packaged in a specific format.</p></blockquote>


<p>e.g plugins.d/websocket/lib/yoursource.jar</p>

<p>From now on you can use it:<br/>
agent.sources = websocket<br/>
agent.sources.websocket.type = com.sequenceiq.samples.flume.source.JettyWebSocketSource<br/>
agent.sources.websocket.host = localhost<br/>
agent.sources.websocket.port = 60000<br/>
agent.sources.websocket.path = /flume</p>

<p>Test it directly from your browser:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">var</span> <span class="nx">ws</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">WebSocket</span><span class="p">(</span><span class="s2">&quot;ws://127.0.0.1:60000/flume&quot;</span><span class="p">)</span>
</span><span class='line'><span class="nx">ws</span><span class="p">.</span><span class="nx">send</span><span class="p">(</span><span class="s2">&quot;Some message&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>That&rsquo;s it. Hope you enjoyed. We will be back soon with some ETL processing examples.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Set up HDP2 on Amazon EC2]]></title>
    <link href="http://blog.sequenceiq.com//blog/2014/02/07/hdp2-on-amazon/"/>
    <updated>2014-02-07T16:17:04+00:00</updated>
    <id>http://blog.sequenceiq.com//blog/2014/02/07/hdp2-on-amazon</id>
    <content type="html"><![CDATA[<p>During the last years we have seen many blog entries and articles about how to set up Hadoop on Amazon EC2. All these tutorials and articles had one thing in common &ndash; you had to go through a large number of manual (and painful) steps, read screenshots and redo the whole thing all over again, in case you needed a new cluster.</p>

<p>Since we use Amazon EC2 quite a lot, and Hadoop as well (Hortonworks distribution) we have gone through these steps many times &ndash; and have scripted the whole process from the first steps up to launching an N node Hadoop/HDP2 cluster in less then five minutes.</p>

<p>Moreover, the cluster is a &lsquo;production ready&rsquo; setup from infrastructural point of view &ndash; it is provisioned in a logically isolated section of the cloud (Virtual Private Cloud), with his own IP address range, creation of subnets, and configuration of route tables and network gateways.</p>

<p>Once the instances are provisoned, the HDP2 setup is done by Apache Ambari &ndash; for more advanced users we will provide the setup thorugh Ambari&rsquo;s RESTful API &ndash; watch this space or our GitHub page.</p>

<p>All the EC2 instances are tagged with the user name &ndash; thus you can create different clusters for different employees, all under the same AWS account (with IAM).</p>

<p>We believe that this is the right way to provision Hadoop in the cloud &ndash; during development and testing we had to provision Hadoop clusters of different sizes, and going through these steps manually would take lots of time.
This way we are able to provision clusters in the cloud in the matter of minutes &ndash; independently of the size.</p>

<p>The script is available at: <a href="https://github.com/sequenceiq/hadoop-cloud-scripts">https://github.com/sequenceiq/hadoop-cloud-scripts</a></p>

<p>Enjoy,
SequenceIQ</p>
]]></content>
  </entry>
  
</feed>
