<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-06-23T21:22:04+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Pearson correlation with Scalding]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/23/scalding-correlation-example/"/>
    <updated>2014-06-23T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/23/scalding-correlation-example</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>At SequenceIQ we are processing data in batch and streaming &ndash; for both we use Scala as our prefered language; for batch processing in particular we use Scalding to build our job and data pipelines. Actually there is <code>Babylon</code> at SequenceIQ as we use Java, Scala, Go, R, Groovy, Ansible, shell, JavaScript and what not &ndash; follow up with us for a post talking about the language heterogeneity.</p>

<p>Scalding is a powerful tool and great choice to simplify the writing and abstracting MapReduce jobs &ndash; an open source project originally developed by Twitter and recently the community.
In the following detailed example we&rsquo;d like show you an example of how to write and test Scalding jobs, running on Hadoop.</p>

<h2>Writing a Pearson correlation job</h2>

<p>In this example, we&rsquo;d like to calculate a Pearson&rsquo;s product-moment coefficient on 2 columns of a given <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/scalding-correlation/data">input</a>.
This is a simple computation and the easiest way to find any dependency between two datasets.
First of all we need all the parameters for the given <a href="http://www.statisticshowto.com/what-is-the-correlation-coefficient-formula/">formula</a>.
In Scala the code would look like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">trait</span> <span class="nc">CorrelationOp</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">calculateCorrelation</span><span class="o">(</span><span class="n">size</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">su1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">su2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq1</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">sq2</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">dotProd</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">dividend</span> <span class="k">=</span> <span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">dotProd</span><span class="o">)</span> <span class="o">-</span> <span class="o">(</span><span class="n">su1</span> <span class="o">*</span> <span class="n">su2</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">divisor</span> <span class="k">=</span> <span class="n">scala</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">sq1</span> <span class="o">-</span> <span class="n">su1</span> <span class="o">*</span> <span class="n">su1</span><span class="o">)</span> <span class="o">*</span> <span class="n">scala</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">sq2</span> <span class="o">-</span> <span class="n">su2</span> <span class="o">*</span> <span class="n">su2</span><span class="o">)</span>
</span><span class='line'>    <span class="n">dividend</span> <span class="o">/</span> <span class="n">divisor</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>In this example we compute all the required parameters for the correlation formula using the <a href="https://github.com/twitter/scalding/wiki/Fields-based-API-Reference">Field API</a> of Scala.
First we obtain the input/output and the two comparable column arguments which comes from command line parameters (usage : &mdash;key value) and provide the schema for the CSV input.
After the input is read we map the two selected fields (product and squares); with the underlined informations, we are able to produce the required parameters (grouping part).
At the end we just need to use the formula on the given fields (second map) and write the results into a TSV file.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="k">val</span> <span class="n">comparableColumn1</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="s">&quot;column1&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">comparableColumn2</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="s">&quot;column2&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">samplePercent</span> <span class="k">=</span> <span class="n">args</span><span class="o">.</span><span class="n">getOrElse</span><span class="o">(</span><span class="s">&quot;samplePercent&quot;</span><span class="o">,</span><span class="s">&quot;1.00&quot;</span><span class="o">).</span><span class="n">toDouble</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">scheme</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Fields</span><span class="o">(</span><span class="s">&quot;id&quot;</span><span class="o">,</span> <span class="s">&quot;num1&quot;</span><span class="o">,</span> <span class="s">&quot;num2&quot;</span><span class="o">,</span> <span class="s">&quot;num3&quot;</span><span class="o">,</span> <span class="s">&quot;num4&quot;</span><span class="o">,</span> <span class="s">&quot;num5&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="nc">Csv</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="s">&quot;input&quot;</span><span class="o">),</span> <span class="n">fields</span> <span class="k">=</span> <span class="n">scheme</span><span class="o">,</span> <span class="n">skipHeader</span> <span class="k">=</span> <span class="kc">true</span><span class="o">).</span><span class="n">read</span>
</span><span class='line'>  <span class="o">.</span><span class="n">sample</span><span class="o">(</span><span class="n">samplePercent</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="n">comparableColumn1</span><span class="o">,</span><span class="n">comparableColumn2</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="-Symbol">&#39;prod</span><span class="o">,</span> <span class="-Symbol">&#39;compSq1</span><span class="o">,</span> <span class="-Symbol">&#39;compSq2</span><span class="o">)){</span>
</span><span class='line'>    <span class="n">values</span> <span class="k">:</span> <span class="o">(</span><span class="kt">Double</span><span class="o">,</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="o">(</span><span class="n">values</span><span class="o">.</span><span class="n">_1</span> <span class="o">*</span> <span class="n">values</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="n">values</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="n">values</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">groupAll</span><span class="o">{</span>
</span><span class='line'>    <span class="k">_</span><span class="o">.</span><span class="n">size</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sum</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">comparableColumn1</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;compSum1</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sum</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">comparableColumn2</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;compSum2</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sum</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="-Symbol">&#39;compSq1</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;normSq1</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sum</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="-Symbol">&#39;compSq2</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;normSq2</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sum</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="-Symbol">&#39;prod</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;dotProduct</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">limit</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">project</span><span class="o">(</span><span class="-Symbol">&#39;size</span><span class="o">,</span><span class="-Symbol">&#39;compSum1</span><span class="o">,</span> <span class="-Symbol">&#39;compSum2</span><span class="o">,</span> <span class="-Symbol">&#39;normSq1</span><span class="o">,</span> <span class="-Symbol">&#39;normSq2</span><span class="o">,</span> <span class="-Symbol">&#39;dotProduct</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="-Symbol">&#39;size</span><span class="o">,</span> <span class="-Symbol">&#39;compSum1</span><span class="o">,</span> <span class="-Symbol">&#39;compSum2</span><span class="o">,</span><span class="-Symbol">&#39;normSq1</span><span class="o">,</span> <span class="-Symbol">&#39;normSq2</span><span class="o">,</span> <span class="-Symbol">&#39;dotProduct</span><span class="o">)</span>
</span><span class='line'>    <span class="o">-&gt;</span> <span class="o">(</span><span class="-Symbol">&#39;key</span><span class="o">,</span> <span class="-Symbol">&#39;correlation</span><span class="o">)){</span>
</span><span class='line'>    <span class="n">fields</span> <span class="k">:</span> <span class="o">(</span><span class="kt">Long</span><span class="o">,</span> <span class="kt">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="k">val</span> <span class="o">(</span><span class="n">size</span><span class="o">,</span> <span class="n">sum1</span><span class="o">,</span> <span class="n">sum2</span><span class="o">,</span> <span class="n">normSq1</span><span class="o">,</span> <span class="n">normSq2</span><span class="o">,</span> <span class="n">dotProduct</span><span class="o">)</span> <span class="k">=</span> <span class="n">fields</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">corr</span> <span class="k">=</span> <span class="n">calculateCorrelation</span><span class="o">(</span><span class="n">size</span><span class="o">,</span> <span class="n">sum1</span><span class="o">,</span> <span class="n">sum2</span><span class="o">,</span> <span class="n">normSq1</span><span class="o">,</span> <span class="n">normSq2</span><span class="o">,</span> <span class="n">dotProduct</span><span class="o">)</span>
</span><span class='line'>      <span class="o">(</span><span class="n">comparableColumn1</span> <span class="o">+</span> <span class="s">&quot;-&quot;</span> <span class="o">+</span> <span class="n">comparableColumn2</span><span class="o">,</span> <span class="n">corr</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="o">.</span><span class="n">project</span><span class="o">(</span><span class="-Symbol">&#39;key</span><span class="o">,</span> <span class="-Symbol">&#39;correlation</span><span class="o">)</span>
</span><span class='line'>  <span class="o">.</span><span class="n">write</span><span class="o">(</span><span class="nc">Tsv</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="s">&quot;output&quot;</span><span class="o">)))</span>
</span></code></pre></td></tr></table></div></figure>


<p>For running the example you will have to run the following command: (<em>you can use &mdash;hdfs instead of &mdash;local</em>)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>yarn jar scalding-correalation-1.0.jar com.sequenceiq.scalding.correlation.SimpleCorrelationJob --local --input data/data.csv --output data/corr-out.tsv --column1 num1 --column2 num2 --samplePercent 0.1
</span></code></pre></td></tr></table></div></figure>


<h2>Testing Scalding jobs</h2>

<p>In order to test that your data transformations are correct, you can use the
<a href="http://twitter.github.io/scalding/com/twitter/scalding/JobTest.html">JobTest</a> class for unit testing.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="nd">@RunWith</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">JUnitRunner</span><span class="o">])</span>
</span><span class='line'><span class="k">class</span> <span class="nc">SimpleCorrelationJobTest</span>  <span class="k">extends</span> <span class="nc">Specification</span> <span class="o">{</span>
</span><span class='line'>  <span class="s">&quot;A SimpleCorrelation Job&quot;</span> <span class="n">should</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">input</span> <span class="k">=</span> <span class="nc">List</span><span class="o">((</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">),(</span><span class="mi">2</span><span class="o">,</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">),(</span><span class="mi">3</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">))</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">correctOutputLimit</span> <span class="k">=</span> <span class="mf">0.8</span>
</span><span class='line'>
</span><span class='line'>    <span class="nc">JobTest</span><span class="o">(</span><span class="s">&quot;com.sequenceiq.scalding.correlation.SimpleCorrelationJob&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">arg</span><span class="o">(</span><span class="s">&quot;input&quot;</span><span class="o">,</span> <span class="s">&quot;fakeInput&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">arg</span><span class="o">(</span><span class="s">&quot;output&quot;</span><span class="o">,</span> <span class="s">&quot;fakeOutput&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">arg</span><span class="o">(</span><span class="s">&quot;column1&quot;</span><span class="o">,</span> <span class="s">&quot;num1&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">arg</span><span class="o">(</span><span class="s">&quot;column2&quot;</span><span class="o">,</span> <span class="s">&quot;num2&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">arg</span><span class="o">(</span><span class="s">&quot;correlationThreshold&quot;</span><span class="o">,</span> <span class="s">&quot;0.8&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">source</span><span class="o">(</span><span class="nc">Csv</span><span class="o">(</span><span class="s">&quot;fakeInput&quot;</span><span class="o">,</span> <span class="s">&quot;,&quot;</span><span class="o">,</span> <span class="k">new</span> <span class="nc">Fields</span><span class="o">(</span><span class="s">&quot;id&quot;</span><span class="o">,</span><span class="s">&quot;num1&quot;</span><span class="o">,</span><span class="s">&quot;num2&quot;</span><span class="o">,</span><span class="s">&quot;num3&quot;</span><span class="o">,</span><span class="s">&quot;num4&quot;</span><span class="o">,</span><span class="s">&quot;num5&quot;</span><span class="o">),</span><span class="n">skipHeader</span> <span class="k">=</span> <span class="kc">true</span><span class="o">),</span> <span class="n">input</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">sink</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Double</span><span class="o">)](</span><span class="nc">Tsv</span><span class="o">(</span><span class="s">&quot;fakeOutput&quot;</span><span class="o">,</span> <span class="n">fields</span> <span class="k">=</span> <span class="nc">Fields</span><span class="o">.</span><span class="nc">ALL</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">outputBuf</span> <span class="k">=&gt;</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">actualOutput</span> <span class="k">=</span> <span class="n">outputBuf</span><span class="o">.</span><span class="n">toList</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">_2</span>
</span><span class='line'>        <span class="s">&quot;return greater correlation result than 0.8&quot;</span> <span class="n">in</span> <span class="o">{</span>
</span><span class='line'>          <span class="n">correctOutputLimit</span> <span class="n">must</span> <span class="n">be_&lt;</span> <span class="o">(</span><span class="n">actualOutput</span><span class="o">)</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>      <span class="o">.</span><span class="n">run</span>
</span><span class='line'>      <span class="o">.</span><span class="n">finish</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Writing results to HBase</h2>

<p>In case we&rsquo;d like to store our data in a database (at SequenceIQ we use HBase) we can use a special Cascading Tap for it.
In this example we used <a href="https://github.com/ParallelAI/SpyGlass">Spyglass</a> to store the correlation results in HBase.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="k">val</span> <span class="n">tableName</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="s">&quot;tableName&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">quorum_name</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="s">&quot;quorum&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">quorum_port</span> <span class="k">=</span> <span class="n">args</span><span class="o">(</span><span class="s">&quot;quorumPort&quot;</span><span class="o">).</span><span class="n">toInt</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">scheme</span> <span class="k">=</span> <span class="nc">List</span><span class="o">(</span><span class="-Symbol">&#39;key</span><span class="o">,</span> <span class="-Symbol">&#39;correlation</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">familyNames</span> <span class="k">=</span> <span class="nc">List</span><span class="o">(</span><span class="s">&quot;corrCf&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="nc">Tsv</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="s">&quot;input&quot;</span><span class="o">)).</span><span class="n">read</span>
</span><span class='line'>    <span class="o">.</span><span class="n">toBytesWritable</span><span class="o">(</span><span class="n">scheme</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="n">write</span><span class="o">(</span>
</span><span class='line'>      <span class="k">new</span> <span class="nc">HBaseSource</span><span class="o">(</span>
</span><span class='line'>        <span class="n">tableName</span><span class="o">,</span>
</span><span class='line'>        <span class="n">quorum_name</span> <span class="o">+</span> <span class="s">&quot;:&quot;</span> <span class="o">+</span> <span class="n">quorum_port</span><span class="o">,</span>
</span><span class='line'>        <span class="n">scheme</span><span class="o">.</span><span class="n">head</span><span class="o">,</span>
</span><span class='line'>        <span class="n">familyNames</span><span class="o">,</span>
</span><span class='line'>        <span class="n">scheme</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="n">x</span><span class="k">:</span> <span class="kt">Symbol</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="k">new</span> <span class="nc">Fields</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="o">)).</span><span class="n">toList</span><span class="o">,</span>
</span><span class='line'>        <span class="n">timestamp</span> <span class="k">=</span> <span class="nc">Platform</span><span class="o">.</span><span class="n">currentTime</span>
</span><span class='line'>      <span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Build the application</h2>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./gradlew clean jar
</span></code></pre></td></tr></table></div></figure>


<p>or</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">export </span><span class="nv">GRADLE_OPTS</span><span class="o">=</span><span class="s2">&quot;-XX:MaxPermSize=2048m&quot;</span> <span class="c"># for tests</span>
</span><span class='line'>./gradlew clean build
</span></code></pre></td></tr></table></div></figure>


<h2>Running the example and persisting to HBase</h2>

<p>In order to run the example you&rsquo;ll have to run the following command: (you can use &mdash;hdfs instead of &mdash;local)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>yarn jar scalding-correalation-1.0.jar com.sequenceiq.scalding.hbase.HBaseWriterJob --local --input data/corr-out.tsv --tableName corrTable --quorum localhost --quorumPort 2181
</span></code></pre></td></tr></table></div></figure>


<p>Hope this correlation example and introduction into Scalding was useful &ndash; you can get the example project from our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/scalding-correlation">GitHub</a> repository.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Multi-node Hadoop cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/"/>
    <updated>2014-06-19T20:29:10+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker</id>
    <content type="html"><![CDATA[<p>In the <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/">previous post</a>
you saw how easy is to create a single-node Hadoop <em>cluster</em> on your devbox.</p>

<p>Now lets raise the bar and create a multinode Hadoop cluster on Docker. Before we
start, make sure you have the latest Ambari image:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull sequenceiq/ambari:latest</span></code></pre></td></tr></table></div></figure>


<h2>One-liner</h2>

<p>Once you have the latest image, you can start running Docker containers.
But instead of typing long commands like <code>docker run [options] image [command]</code>,
we have created a couple of <a href="https://github.com/sequenceiq/docker-ambari/blob/master/ambari-functions">shell functions</a> to help you with Docker commands.</p>

<p>Using these functions the impatient can provision a 3 node Hadoop cluster with this one-liner:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Lo .amb j.mp/docker-ambari && . .amb && amb-deploy-cluster</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>Note that you can always alter the default parameters as the blueprint, cluster size, etc &hellip; check the shell <code>j.mp/docker-ambari</code> function&rsquo;s head for the parameters list.</p>

<p>It does the following steps:</p>

<ul>
<li>runs <code>ambari-server start</code> in a daemon Docker (background) container (and also an <code>ambari-agent start</code>)</li>
<li>runs <code>n-1</code> daemon containers with <code>ambari-agent start</code> connecting to the server</li>
<li>runs AmbariShell with attached terminal (to see provision progress)

<ul>
<li>AmbariShell will post the built-in multi-node blueprint to <code>/api/v1/blueprints</code> REST API</li>
<li>AmbariShell auto-assign hosts to host_groups defined in the blueprint</li>
<li>creates a cluster, by posting to the <code>/api/v1/clusters</code> REST API</li>
</ul>
</li>
</ul>


<h2>Custom blueprint</h2>

<p>If you have your own blueprint, put it into a <a href="https://gist.github.com/">gist</a>
and you can use it from AmbariShell. First start AmbariShell:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>amb-start-cluster 2
</span><span class='line'>amb-shell</span></code></pre></td></tr></table></div></figure>


<p>AmbariShell will wait for:</p>

<ul>
<li>Ambari REST API
Below you will see a happy path to create a multi node Hadoop cluster using the AmbariShell.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>host list
</span><span class='line'>blueprint add --url https://gist.githubusercontent.com/lalyos/xxx/raw/custum-blueprint.json
</span><span class='line'>cluster build --blueprint custom-blueprint
</span><span class='line'>cluster assign --hostGroup host_group_1 --host amb0.mycorp.kom
</span><span class='line'>cluster assign --hostGroup host_group_2 --host amb1.mycorp.kom
</span><span class='line'>cluster assign --hostGroup host_group_2 --host amb1.mycorp.kom
</span><span class='line'>cluster create</span></code></pre></td></tr></table></div></figure>


<p>In AmbariShell the <code>hint</code> command will always guide you on the happy path,
and remember that devops are lazy, so instead of typing press <code>&lt;TAB&gt;</code> for autocomplete or suggestions.</p>

<p>Autocomplete will help you to:</p>

<ul>
<li>complete the command in the given context (e.g. without any blueprint, cluster commands are not available)</li>
<li>add required parameters</li>
<li>add optional parameters: press tab after double dash <code>--&lt;TAB&gt;</code></li>
<li>complete parameter arguments, such as blueprint names, hostnames &hellip;</li>
</ul>


<h2>Summary</h2>

<p>Ever since we started to use Docker we are always developing against a multi-node
Hadoop cluster &ndash; as running a 3-4 node cluster in a laptop actually has less overhead
than working on a Sandbox VM.</p>

<p>We are <em>Dockerizing</em> the Hadoop ecosystem and simplifying the provisioning
process &ndash; watch this space or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>
for the latest news about <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> &ndash; the
open source cloud agnostic <em>Hadoop as a Service</em> API built on Docker.</p>

<p>Hope this helps and simplifies your development process &ndash; let us know how it goes
for you or if you need any help with Hadoop on Docker.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ambari provisioned Hadoop cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/"/>
    <updated>2014-06-17T08:51:14+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker</id>
    <content type="html"><![CDATA[<p>We are getting close to release and open source our <strong>Docker-based Hadoop Provisioning</strong> project.
The <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">slides</a>
were presented recently at the <a href="http://hadoopsummit.org/san-jose/">Hadoop Summit</a>, and
there is an interest from the community to learn the technical details.</p>

<p>The project &ndash; called <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> &ndash; will provide a REST API to provision a Hadoop cluster &ndash; anywhere. The cluster can be hosted
on AWS EC2, Azure, physical servers or even your laptop &ndash; we are adding more providers &ndash; but always based on the same concept:
<a href="http://ambari.apache.org/">Apache Ambari</a> managed <a href="http://www.docker.com/">Docker</a>
containers.</p>

<p>This blog entry is the first in a series, where we describe the Docker layer step-by-step:</p>

<ul>
<li>Single-node Docker based Hadoop &ldquo;cluster&rdquo; locally</li>
<li>Multi-node Docker based Hadoop cluster</li>
<li>Multi-node Docker based Hadoop cluster on EC2</li>
<li>Cloudbreak</li>
</ul>


<h2>Get Docker</h2>

<p>The only required software is Docker, so if you don&rsquo;t have it yet, jump to the
installation section of the <a href="https://docs.docker.com/installation/">official documentation</a>.</p>

<p>The very basic you need to work with Docker containers, is described in the
<a href="https://docs.docker.com/userguide/dockerizing/">users guide</a>.</p>

<h2>Single-node Cluster</h2>

<p>All setup is based on <a href="https://hub.docker.com/u/sequenceiq/">Docker images</a> only
the glue-code is different. Let&rsquo;s start with the most simple setup:</p>

<ul>
<li>start the first Docker container in the background that runs <strong>ambari-server</strong> and <strong>ambari-agent</strong>.</li>
<li>start the second Docker container which:

<ul>
<li>waits for the agent connecting to the server</li>
<li>starts an <a href="https://github.com/sequenceiq/ambari-shell">ambari-shell</a>, which will instruct ambari-server on its REST API:

<ul>
<li>define an <strong><a href="https://cwiki.apache.org/confluence/display/AMBARI/Blueprints">Ambari Blueprint</a></strong> by posting a JSON to <code>&lt;AMBARI_URL&gt;/api/v1/blueprints</code></li>
<li>create a Hadoop cluster by posting a JSON to <code>&lt;AMBARI_URL&gt;/api/v1/clusters</code> using the blueprint created in the previous step</li>
</ul>
</li>
</ul>
</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d -p 8080 -h amb0.mycorp.kom --name ambari-singlenode sequenceiq/ambari --tag ambari-server=true
</span><span class='line'>docker run -e BLUEPRINT=single-node-hdfs-yarn --link ambari-singlenode:ambariserver -t --rm --entrypoint /bin/sh sequenceiq/ambari -c /tmp/install-cluster.sh</span></code></pre></td></tr></table></div></figure>


<p>or if you want a <strong>twitter-sized</strong> one-liner to start with Hadoop in less than a minute:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -LOs j.mp/ambari-singlenode && . ambari-singlenode</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>When you pull the <code>sequenceiq/ambari</code> image first it will take a couple of minutes (for me it was 4 minutes).
Meanwhile you have started and running the download let&rsquo;s explain all those parameters.</p>

<h2>First container: ambari-server and ambari-agent</h2>

<p>Let&rsquo;s break down the parameters of the first container:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d -p 8080 -h amb0.mycorp.kom --name ambari-singlenode sequenceiq/ambari --tag ambari-server=true</span></code></pre></td></tr></table></div></figure>


<ul>
<li><strong>-d</strong> : Detached mode, container runs in the background</li>
<li><strong>-p 8080</strong> : Publish ambari web and REST API port</li>
<li><strong>-h amb0.mycorp.kom</strong> : hostname</li>
<li><strong>&mdash;name ambari-singlenode</strong> : assign a name to the container</li>
<li><strong>sequenceiq/ambari</strong> : the name of the image</li>
<li><strong>&mdash;tag ambari-server=true</strong> : the <em>command</em> but please note that this is appended to the <em>entrypoint</em>.</li>
</ul>


<p>The default <em>entrypoint</em> of the image is <code>start-serf-agent.sh</code>
<a href="https://github.com/sequenceiq/docker-ambari/blob/master/ambari-server/Dockerfile#L24">see the Dockerfile</a>
so the <code>--tag ambari-server=true</code> command is actually an argument of the <a href="http://www.serfdom.io/">serf agent</a>.</p>

<h3>Serf</h3>

<p>What is <a href="http://www.serfdom.io/">Serf</a>? The definition goes like:</p>

<blockquote><p>Serf is a decentralized solution for cluster membership, failure detection, and orchestration. Lightweight and highly available.</p></blockquote>

<p>Right now it doesn&rsquo;t seem to make any sense to talk about membership and cluster, but remember we want to
have the exact same process/tools for dev env and production.</p>

<p>The only Serf feature we use at this point is that you can define shell scripts based <strong>event-handlers</strong> for
each membership events:</p>

<ul>
<li>member-join</li>
<li>member-failed</li>
<li>member-leave</li>
<li>member-xxx</li>
</ul>


<p>The <strong>member-join</strong> event-handler script will check the Serf tags, defined by <code>--tag name=value</code>
and will start:
 &ndash; ambari-server java process: if the <strong>ambari-server</strong> tag is <strong>true</strong>
 &ndash; ambari-agent python process: if the <strong>ambari-agent</strong> tag is <strong>true</strong></p>

<p>You might noted that only the <strong>ambari-server</strong> tag is defined. The reason is that <strong>ambari-agent</strong> is defined as <strong>true</strong> by default.</p>

<h2>Second container: ambari-shell</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -e BLUEPRINT=single-node-hdfs-yarn --link ambari-singlenode:ambariserver -t --rm --entrypoint /bin/sh sequenceiq/ambari -c /tmp/install-cluster.sh</span></code></pre></td></tr></table></div></figure>


<ul>
<li><strong>-e BLUEPRINT=single-node-hdfs-yarn</strong> : the template to use for the cluster (single-node-hdfs-yarn/multi-node-hdfs-yarn/lambda-architecture) <a href="https://github.com/sequenceiq/ambari-rest-client/tree/master/src/main/resources/blueprints">see the blueprint JSON on GitHub</a></li>
<li><strong>&mdash;link ambari-singlenode:ambariserver </strong> :  it will make all exposed ports and the private IP of <code>ambari-singlenode</code> available as <code>AMBARISERVER_xxx</code> env variables</li>
<li><strong>-t</strong> : pseudo terminal, to see the progress</li>
<li><strong>&mdash;rm</strong> : remove the container once it&rsquo;s finished</li>
<li><strong>&mdash;entrypoint /bin/sh</strong> : the default entrypoint runs the shell in interactive mode, we want to overwrite it with a script specified as <code>/tmp/install-cluster.sh</code></li>
</ul>


<h1>Install completed</h1>

<p>Once Ambari-shell completed with the installation, you are ready to use it.
To find out the IP of the Ambari server run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker inspect -f "" ambari-singlenode</span></code></pre></td></tr></table></div></figure>


<p>To start with you can browse ambari web ui on <code>port 8080</code>. The default username/password is admin/admin.</p>

<p>or if you can&rsquo;t reach directly the private IP of the container (windows users), use the port exposed to the host:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker port ambari-singlenode 8080</span></code></pre></td></tr></table></div></figure>


<h1>Next steps</h1>

<p>In the upcomming blog posts we will do a multinode Hadoop cluster with the same toolset, so stay tuned &hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Summit 2014 - SequenceIQ slides]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/06/hadoop-summit-slides/"/>
    <updated>2014-06-06T13:42:11+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/06/hadoop-summit-slides</id>
    <content type="html"><![CDATA[<p>These are the slides of our presentation from the Hadoop Summit 2014, San Jose. We would like to thank all who have joined the session and the positive feedbacks we have received. This gives us a great confidence and validates our efforts that there is a great need to an easy and seamless Hadoop provisionig &ndash; let it be bare metal, cloud or other virtualizations.</p>

<p>Watch this space as <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> will be open sourced in the coming weeks.</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/35573123" width="640" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning" title="Docker based Hadoop provisioning - Hadoop Summit 2014 " target="_blank">Docker based Hadoop provisioning &ndash; Hadoop Summit 2014 </a> </strong> from <strong><a href="http://www.slideshare.net/JanosMatyas" target="_blank">Janos Matyas</a></strong> </div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Ambari + Spring Shell = Ambari Shell]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/"/>
    <updated>2014-05-26T13:42:11+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p><a href="http://ambari.apache.org/">Apache Ambari&rsquo;s</a> goal is to make Hadoop cluster management
as simple as possible. It provides an intuitive easy-to-use web UI backed by its RESTful API.
With only a few clicks you&rsquo;re able to install Hadoop services across any number of hosts
and Ambari will take care of the configurations as well. After the installation is complete
you can monitor your cluster taking leverage of <a href="http://ganglia.sourceforge.net/">Ganglia</a>
and <a href="http://www.nagios.org/">Nagios</a>.</p>

<p>At SequenceIQ we prefer to use command line tools whenever it&rsquo;s possible,
because it&rsquo;s much faster than interacting with a web UI and it&rsquo;s a better candidate for automation.
Here comes <a href="https://github.com/spring-projects/spring-shell#readme">Spring Shell</a> to our rescue.</p>

<p>It&rsquo;s an interactive shell that can be easily extended using a Spring based programming model and battle
tested in various projects like <a href="http://projects.spring.io/spring-roo/">Spring Roo</a>,
<a href="http://docs.spring.io/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/">Spring XD</a>, and
<a href="https://github.com/spring-projects/rest-shell">Spring REST Shell</a> Combine these two projects
and a really powerful tool will come to light.</p>

<h2>Ambari Shell</h2>

<p>The goal is to provide an interactive command line tool which supports:</p>

<ul>
<li>all functionality available through the REST API or Ambari web UI</li>
<li>makes possible complete automation of management task via <strong>scripts</strong></li>
<li>context aware command availability</li>
<li>tab completion</li>
<li>required/optional parameter support</li>
<li><strong>hint</strong> command to guide you on the usual path</li>
</ul>


<p>Since we&rsquo;re open sourcing the project, it should be available and part of the official Ambari repository
<a href="https://issues.apache.org/jira/browse/AMBARI-5482">soon</a>.</p>

<h2>Install Ambari Shell</h2>

<p>For now you have 3 options to give it a try:</p>

<ul>
<li>use our prepared <a href="https://index.docker.io/u/sequenceiq/ambari-shell/">docker image</a></li>
<li>download the latest self-containing executable jar form our maven repo</li>
<li>build it from source</li>
</ul>


<h3>Run via docker</h3>

<p>As we love to dockerize everything, we prepared a <a href="https://index.docker.io/u/sequenceiq/ambari-shell/">docker image</a>
with jdk1.7 on latest ubuntu, ambari-shell preinstalled. Detailed description is available on <a href="https://github.com/sequenceiq/ambari-shell-docker">github</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -it --rm sequenceiq/ambari-shell --ambari.host=&lt;HOSTNAME&gt; --ambari.port=&lt;PORT&gt;</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h3>Run latest snapshot</h3>

<p>You need only jdk 1.7. The script below will download the latest ambari-shell.jar into a
temporary folder, and give you instruction on how to use.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -Ls https://raw.githubusercontent.com/sequenceiq/ambari-shell/master/latest-snap.sh | bash</span></code></pre></td></tr></table></div></figure>


<h3>Build from source</h3>

<p>If want to check the code, or extend it with new commands, Follow the steps below. You will need:
&ndash; jdk 1.7
&ndash; maven 3.x.x</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/sequenceiq/ambari-shell.git
</span><span class='line'>cd ambari-shell
</span><span class='line'>mvn package</span></code></pre></td></tr></table></div></figure>


<p>The shell is built as a single executable jar with the help of <a href="http://projects.spring.io/spring-boot/">Spring Boot</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>java -jar target/ambari-shell-1.0-SNAPSHOT.jar --ambari.host=&lt;HOSTNAME&gt; --ambari.port=&lt;PORT&gt;</span></code></pre></td></tr></table></div></figure>


<h2>Start a &ldquo;sandbox&rdquo; Ambari Server</h2>

<p>The image is available at the Docker repository, which means you only need to run the following to get a running Ambari server:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d -P -h server.ambari.com --name ambari-singlenode sequenceiq/ambari</span></code></pre></td></tr></table></div></figure>


<h2>Connect Ambari Shell to the server</h2>

<p>Once the server is up and running (10-20 sec) you can connect to it with the shell:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage:
</span><span class='line'>  java -jar ambari-shell.jar                  : Starts Ambari Shell in interactive mode.
</span><span class='line'>  java -jar ambari-shell.jar --cmdfile=&lt;FILE&gt; : Ambari Shell executes commands read from the file.
</span><span class='line'>
</span><span class='line'>Options:
</span><span class='line'>  --ambari.host=&lt;HOSTNAME&gt;       Hostname of the Ambari Server [default: localhost].
</span><span class='line'>  --ambari.port=&lt;PORT&gt;           Port of the Ambari Server [default: 8080].
</span><span class='line'>  --ambari.user=&lt;USER&gt;           Username of the Ambari admin [default: admin].
</span><span class='line'>  --ambari.password=&lt;PASSWORD&gt;   Password of the Ambari admin [default: admin].
</span><span class='line'>
</span><span class='line'>Note:
</span><span class='line'>  At least one option is mandatory.</span></code></pre></td></tr></table></div></figure>


<h2>Create a cluster</h2>

<p>All commands are context aware and are available only when it makes sense. For example the <code>cluster create</code> command is not available
until a blueprint hasn&rsquo;t been added or selected. A good approach is to use the <code>hint</code> command &ndash; as the Ambari UI, this will give
you hints about the available commands and the flow of creating or configuring a cluster. You can always use TAB for completion
or available parameters.</p>

<p>Initially there are no blueprints available &ndash; you can add blueprints from file or URL. For your convenience we&rsquo;ve added two
blueprints as defaults. You can get these blueprints by using the <code>blueprint defaults</code> command. The result is the following:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ambari-shell&gt; blueprint defaults
</span><span class='line'>ambari-shell&gt; blueprint list</span></code></pre></td></tr></table></div></figure>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  BLUEPRINT              STACK
</span><span class='line'>  ---------------------  -------
</span><span class='line'>  multi-node-hdfs-yarn   HDP:2.0
</span><span class='line'>  single-node-hdfs-yarn  HDP:2.0</span></code></pre></td></tr></table></div></figure>


<p>Once the blueprints are added you can use them to create a cluster by typing <code>cluster build --blueprint single-node-hdfs-yarn</code>.
Now that the blueprint is selected you have to assign the hosts to the available host groups. Use</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ambari-shell&gt; cluster build --blueprint single-node-hdfs-yarn
</span><span class='line'>CLUSTER_BUILD:single-node-hdfs-yarn&gt; cluster assign --hostGroup host_group_1 --host server.ambari.com
</span><span class='line'>
</span><span class='line'>  HOSTGROUP     HOST
</span><span class='line'>  ------------  -----------------
</span><span class='line'>  host_group_1  server.ambari.com</span></code></pre></td></tr></table></div></figure>


<p>Once you are happy with the host &ndash; host group associations you can choose <code>cluster create</code> to start building the cluster.
Progress can be checked either at the Amabri UI or using the <code>tasks</code> command.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>CLUSTER_BUILD:single-node-hdfs-yarn&gt; cluster create
</span><span class='line'>Successfully created the cluster
</span><span class='line'>CLUSTER:single-node-hdfs-yarn&gt; tasks
</span><span class='line'>
</span><span class='line'>  TASK                        STATUS
</span><span class='line'>  --------------------------  -------
</span><span class='line'>  HISTORYSERVER INSTALL       QUEUED
</span><span class='line'>  ZOOKEEPER_SERVER START      PENDING
</span><span class='line'>  ZOOKEEPER_CLIENT INSTALL    PENDING
</span><span class='line'>  HDFS_CLIENT INSTALL         PENDING
</span><span class='line'>  HISTORYSERVER START         PENDING
</span><span class='line'>  NODEMANAGER INSTALL         QUEUED
</span><span class='line'>  NODEMANAGER START           PENDING
</span><span class='line'>  ZOOKEEPER_SERVER INSTALL    QUEUED
</span><span class='line'>  YARN_CLIENT INSTALL         PENDING
</span><span class='line'>  NAMENODE INSTALL            QUEUED
</span><span class='line'>  RESOURCEMANAGER INSTALL     QUEUED
</span><span class='line'>  NAMENODE START              PENDING
</span><span class='line'>  RESOURCEMANAGER START       PENDING
</span><span class='line'>  DATANODE START              PENDING
</span><span class='line'>  SECONDARY_NAMENODE START    PENDING
</span><span class='line'>  DATANODE INSTALL            QUEUED
</span><span class='line'>  MAPREDUCE2_CLIENT INSTALL   PENDING
</span><span class='line'>  SECONDARY_NAMENODE INSTALL  QUEUED</span></code></pre></td></tr></table></div></figure>


<p>Each time you start the shell the executed commands are logged in a file line by line and later either with the <code>script</code> command
or specifying an <code>--cmdfile</code> option the same commands can be executed.</p>

<h2>Commands</h2>

<p>The currently supported commands are:</p>

<ul>
<li><code>blueprint add</code> &ndash; Add a new blueprint with either &mdash;url or &mdash;file</li>
<li><code>blueprint defaults</code> &ndash; Adds the default blueprints to Ambari</li>
<li><code>blueprint list</code> &ndash; Lists all known blueprints</li>
<li><code>blueprint show</code> &ndash; Shows the blueprint by its id</li>
<li><code>cluster assign</code> &ndash; Assign host to host group</li>
<li><code>cluster build</code> &ndash; Starts to build a cluster</li>
<li><code>cluster create</code> &ndash; Create a cluster based on current blueprint and assigned hosts</li>
<li><code>cluster delete</code> &ndash; Delete the cluster</li>
<li><code>cluster preview</code> &ndash; Shows the currently assigned hosts</li>
<li><code>cluster reset</code> &ndash; Clears the host &ndash; host group assignments</li>
<li><code>debug off</code> &ndash; Stops showing the URL of the API calls</li>
<li><code>debug on</code> &ndash; Shows the URL of the API calls</li>
<li><code>exit</code> &ndash; Exits the shell</li>
<li><code>hello</code> &ndash; Prints a simple elephant to the console</li>
<li><code>help</code> &ndash; List all commands usage</li>
<li><code>hint</code> &ndash; Shows some hints</li>
<li><code>host components</code> &ndash; Lists the components assigned to the selected host</li>
<li><code>host focus</code> &ndash; Sets the useHost to the specified host</li>
<li><code>host list</code> &ndash; Lists the available hosts</li>
<li><code>quit</code> &ndash; Exits the shell</li>
<li><code>script</code> &ndash; Parses the specified resource file and executes its commands</li>
<li><code>service components</code> &ndash; Lists all services with their components</li>
<li><code>service list</code> &ndash; Lists the available services</li>
<li><code>tasks</code> &ndash; Lists the Ambari tasks</li>
<li><code>version</code> &ndash; Displays shell version</li>
</ul>


<h2>Summary</h2>

<p>To sum it up in less than two minutes watch this video:</p>

<script type="text/javascript" src="https://asciinema.org/a/9783.js" id="asciicast-9783" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building the build environment with Ansible and Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker/"/>
    <updated>2014-05-09T11:51:57+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker</id>
    <content type="html"><![CDATA[<p>At SequenceIQ we put a strong emphasis on automating everything we can and this automation starts with our continuous integration &amp; delivery process.</p>

<h2>Introduction</h2>

<p>Lately there is a lot of buzz around continuous integration, development and deployment. More and more companies are moving away from long release cycles towards the &ldquo;release early, release often&rdquo; approach. The advantages of this approach are well known: lower overhead, earlier bug discovery and bug fixing, fewer context switches for the developers to name just a few. There are very good resources to learn about these concepts &ndash; blog posts by different companies (e.g.: by <a href="http://techblog.netflix.com/2013/08/deploying-netflix-api.html">Netflix</a>) and of course the <a href="http://www.amazon.com/dp/0321601912">book</a> &lsquo;Continuous Delivery&rsquo; by Jez Humble and David Farley &ndash; we&rsquo;ll now try to add our own experiences as well.</p>

<p>We&rsquo;ll share two blog posts about our continuous delivery at SequenceIQ: the first one being an introductory post about some tools we use to make the whole process easier and more robust, the second one explains the <a href="http://scottchacon.com/2011/08/31/github-flow.html">flow</a> we use from committing changes to being the changes available in our different environments.</p>

<h2>Tools</h2>

<p>Our CI and CD process at SequenceIQ is based on Ansible, Jenkins and of course Docker.
When we started to build our own process, we decided that we don&rsquo;t want to commit the same mistake that a lot of companies make about their build environment. At these companies the build servers where Jenkins and/or the other build tools are installed are often prepared once in the far past by someone who probably doesn&rsquo;t work there anymore. It quickly becomes something that everyone is afraid to touch and just hope that it will work forever. As the projects improve there will be a lot of different tools with a lot of different versions on the build machine and soon it leads to a small chaos, where the maintenance will involve a lot of hard manual work. To get rid of these problems, we use <a href="http://www.ansible.com/">Ansible</a> to &ldquo;build the build infrastructure&rdquo;, and Docker to run the builds in separated self-sufficient containers.</p>

<!-- more -->


<h2>Ansible</h2>

<p>We have an Ansible script which starts an EC2 instance in the cloud and provisions everything on this server automatically. This script can be easily executed with a single command from a developer laptop:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ansible-playbook -i hosts ci.yml</span></code></pre></td></tr></table></div></figure>


<p>To run this command Ansible, python and some python modules must be installed on the local machine. To avoid having different version of these tools on the development machines we automated the installation of our development environment too &ndash; maybe the topic of another post in the future.
So let&rsquo;s see how the Ansible script works exactly.</p>

<h3>Creating an instance in the cloud</h3>

<p>First it needs to start an instance in the AWS cloud, so it invokes our <strong>ec2 role</strong> on localhost:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Request and init EC2 instance</span>
</span><span class='line'>  <span class="l-Scalar-Plain">hosts</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">localhost</span>
</span><span class='line'>  <span class="l-Scalar-Plain">roles</span><span class="p-Indicator">:</span>
</span><span class='line'>     <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">ec2</span>
</span></code></pre></td></tr></table></div></figure>


<p>The ec2 role requests an EC2 spot priced instance and associates it with an elastic IP. We can easily use a spot priced instance because if it gets shut down by AWS we can recreate it in a few minutes! Ansible has a few <a href="http://docs.ansible.com/list_of_cloud_modules.html">cloud modules</a> which makes it quite easy to manage EC2 instances. Requesting a spot priced instance looks like this (the placeholders come from Ansible group variables):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Create an EC2 spot priced instance</span>
</span><span class='line'>  <span class="l-Scalar-Plain">local_action</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">module</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">ec2</span>
</span><span class='line'>  <span class="l-Scalar-Plain">key_name</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.keypair</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">group</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.security_group</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">instance_type</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.instance_type</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">spot_price</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.spot_price</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">image</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.image</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">wait</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">yes</span>
</span><span class='line'>  <span class="l-Scalar-Plain">region</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.region</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">id</span><span class="p-Indicator">:</span> <span class="s">&quot;{{</span><span class="nv"> </span><span class="s">ec2.idempotent_id</span><span class="nv"> </span><span class="s">}}&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">register</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">ec2result</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Provisioning the build server</h3>

<p>After the EC2 instance is running and accepting SSH connections, the script can go on and start to install the tools needed. Because almost everything is running in separated Docker containers, we only need 3 things: Docker, Nginx and Jenkins. Installing Docker is pretty easy as the new Amazon Linux AMIs are <a href="http://aws.amazon.com/amazon-linux-ami/2014.03-release-notes/">prepared</a> to run Docker. We only need to install it from Amazon&rsquo;s provided Software Repository, and start the service. It looks like this in the Ansible script:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Install Docker on Amazon Linux AMI</span>
</span><span class='line'>  <span class="l-Scalar-Plain">when</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">ansible_os_family == &quot;RedHat&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">yum</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">name=docker state=present</span>
</span><span class='line'>
</span><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Start Docker service</span>
</span><span class='line'>  <span class="l-Scalar-Plain">service</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">name=docker state=started</span>
</span></code></pre></td></tr></table></div></figure>


<p>After Docker is installed we can start some containers that are used by some Jenkins builds later (e.g.: a SonarQube server and a MySQL database that holds the results &ndash; we&rsquo;ve created publicly available <a href="https://index.docker.io/u/sequenceiq/sonar-server/">containers</a> on our Github page.</p>

<p>To install and configure Nginx we use an <a href="https://galaxy.ansible.com/list#/roles/466">existing role</a> from Ansible Galaxy that is well prepared and easily configurable. We are configuring Nginx to forward requests from port 80 to either Jenkins or Sonar.</p>

<p>For example the configuration for Jenkins is the following in the Ansible group variables:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">nginx_sites</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">default</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">listen 80</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">server_name jenkins.sequenceiq.com</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">location / {</span>
</span><span class='line'>       <span class="l-Scalar-Plain">proxy_pass http://jenkins;</span>
</span><span class='line'>       <span class="l-Scalar-Plain">proxy_redirect off;</span>
</span><span class='line'>       <span class="l-Scalar-Plain">proxy_set_header Host $host;</span>
</span><span class='line'>       <span class="l-Scalar-Plain">proxy_set_header X-Forwarded-Host $server_name;</span>
</span><span class='line'>      <span class="l-Scalar-Plain">}</span>
</span><span class='line'><span class="l-Scalar-Plain">nginx_configs</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">proxy</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">proxy_set_header X-Real-IP $remote_addr</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for</span>
</span><span class='line'>  <span class="l-Scalar-Plain">upstream</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">upstream jenkins { server 127.0.0.1:8080 weight=10; }</span>
</span></code></pre></td></tr></table></div></figure>


<p>The most difficult thing to install is Jenkins: we want our configurations and jobs to be instantly available as well. Jenkins has a <a href="https://wiki.jenkins-ci.org/display/JENKINS/Jenkins+CLI">command-line interface</a> that allows access from a script. It has a lot of built-in commands to manage jobs and other configurations and it even has a command to execute a Groovy script on the server. We use these features extensively to prepare the whole Jenkins environment from sketch. Our Jenkins role (that will be available soon on Ansible Galaxy) is able to do the following:
&ndash; Install Jenkins and its dependencies, and get the Jenkins CLI jar from the specified URL.
&ndash; Configure the global Jenkins properties like the mail server, or the properties needed for the Github pull request builder plugin &ndash; it is simply achieved by copying a global config.xml to the Jenkins home directory using Ansible&rsquo;s copy module.
&ndash; Install and update plugins through the Jenkins CLI. Installing plugins looks like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Install plugins</span>
</span><span class='line'>  <span class="l-Scalar-Plain">sudo</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">yes</span>
</span><span class='line'>  <span class="l-Scalar-Plain">shell</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">java -jar {{ jenkins.cli_dest }} -s http://localhost:8080/ install-plugin {{item.item}}</span>
</span><span class='line'>  <span class="l-Scalar-Plain">when</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">item.stdout.find(&#39;false&#39;) != -1</span>
</span><span class='line'>  <span class="l-Scalar-Plain">with_items</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">check_plugins.results</span>
</span><span class='line'>  <span class="l-Scalar-Plain">notify</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="s">&#39;Restart</span><span class="nv"> </span><span class="s">Jenkins&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Configure security (we use Github OAuth). The Jenkins CLI doesn&rsquo;t have any dedicated commands for setting security, but it can be configured with a Groovy script that can be invoked from the CLI:</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='groovy'><span class='line'><span class="kt">def</span> <span class="n">githubSecurityRealm</span> <span class="o">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="na">jenkinsci</span><span class="o">.</span><span class="na">plugins</span><span class="o">.</span><span class="na">GithubSecurityRealm</span><span class="o">(</span><span class="s2">&quot;https://github.com&quot;</span><span class="o">,</span> <span class="s2">&quot;https://api.github.com&quot;</span><span class="o">,</span> <span class="n">clientId</span><span class="o">,</span> <span class="n">clientSecret</span><span class="o">)</span>
</span><span class='line'><span class="kt">def</span> <span class="n">authorizationStrategy</span> <span class="o">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="na">jenkinsci</span><span class="o">.</span><span class="na">plugins</span><span class="o">.</span><span class="na">GithubAuthorizationStrategy</span><span class="o">(</span><span class="s2">&quot;admin1,admin2&quot;</span><span class="o">,</span><span class="kc">true</span><span class="o">,</span><span class="s2">&quot;organization name&quot;</span><span class="o">,</span><span class="kc">true</span><span class="o">,</span><span class="kc">false</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>
</span><span class='line'><span class="n">jenkins</span><span class="o">.</span><span class="na">model</span><span class="o">.</span><span class="na">Jenkins</span><span class="o">.</span><span class="na">instance</span><span class="o">.</span><span class="na">setSecurityRealm</span><span class="o">(</span><span class="n">githubSecurityRealm</span><span class="o">)</span>
</span><span class='line'><span class="n">jenkins</span><span class="o">.</span><span class="na">model</span><span class="o">.</span><span class="na">Jenkins</span><span class="o">.</span><span class="na">instance</span><span class="o">.</span><span class="na">setAuthorizationStrategy</span><span class="o">(</span><span class="n">authorizationStrategy</span><span class="o">)</span>
</span><span class='line'><span class="n">jenkins</span><span class="o">.</span><span class="na">model</span><span class="o">.</span><span class="na">Jenkins</span><span class="o">.</span><span class="na">instance</span><span class="o">.</span><span class="na">save</span><span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li><p>Copy private keys for Github builds &ndash; it simply copies the predefined private SSH keys from a local directory to the <code>~/.ssh</code> directory of the Jenkins user. We use a dedicated Github user to communicate with Github from Jenkins.</p></li>
<li><p>Creating jobs from XML configuration. The Jenkins CLI supports the creation of Jenkins jobs through the create-job command that accepts an XML file as input that defines the Jenkins job. Currently our Jenkins role works by invoking this command for every job that is defined in the variables and has a corresponding XML file in a predefined directory.  We are planning to later modify this role to have a template that holds the structure of a Jenkins job XML so it won&rsquo;t be needed to create the whole XML file manually, only the required parameters among the Ansible variables.</p></li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Create jenkins jobs</span>
</span><span class='line'>  <span class="l-Scalar-Plain">shell</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">java -jar {{ jenkins.cli_dest }} -s http://localhost:8080/ create-job {{ item }} &lt; {{ jenkins.dest }}/{{item}}.xml</span>
</span><span class='line'>  <span class="l-Scalar-Plain">with_items</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">jenkins_jobs</span>
</span><span class='line'>  <span class="l-Scalar-Plain">when</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">existing_jobs.changed and existing_jobs.stdout.find(&#39;{{ item }}&#39;) == -1</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Docker</h2>

<p>The other tool besides Ansible that we use extensively in our build environment is Docker. Docker is a quickly expanding technology that enables the creation of lightweight application containers. If you don&rsquo;t know about Docker yet, check out the official <a href="https://www.docker.io/gettingstarted/">Getting Started guide</a> or our own <a href="http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction/">blog post</a> about it.
With the help of Docker we don&rsquo;t need to worry about the tools needed for the builds or its dependencies on the continuous integration server as they are packaged in separate containers. Every one of our builds on Jenkins are only a few lines that runs a container, maybe copies something out of it and removes the container after it finished. We provide a few environment variables, some shared directories or some links between containers where needed. One of our jobs in Jenkins that builds the master branch looks like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#!/bin/bash</span>
</span><span class='line'>docker run -i --name <span class="nv">$BUILD_TAG</span> <span class="se">\</span>
</span><span class='line'>-v <span class="s2">&quot;/var/lib/jenkins/.gradle-api:/root/.gradle:rw&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;SONAR_USERNAME=$SONAR_USERNAME&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;SONAR_PW=$SONAR_PW&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;BUILD_NUMBER=$BUILD_NUMBER&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;KEY=$(cat /var/lib/jenkins/.ssh/id_rsa| base64 -w 0)&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;REPO=$REPO_ADDRESS&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;BRANCH=master&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;BUILD_TASKS=clean build sonarRunner uploadArchives&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;BUILD_ENV=jenkins&quot;</span> <span class="se">\</span>
</span><span class='line'>-e <span class="s2">&quot;GRADLE_OPTS=-XX:MaxPermSize=512m&quot;</span> <span class="se">\</span>
</span><span class='line'>--link sonar_server:sonar <span class="se">\</span>
</span><span class='line'>--link sonar_mysql:sonar_db <span class="se">\</span>
</span><span class='line'>sequenceiq/build /etc/build-project.sh
</span><span class='line'>sleep 5
</span><span class='line'>docker cp <span class="nv">$BUILD_TAG</span>:/tmp/prj/build/build.info <span class="nv">$WORKSPACE</span>
</span><span class='line'>docker rm <span class="nv">$BUILD_TAG</span>
</span></code></pre></td></tr></table></div></figure>


<p>And not only our builds run in Docker, some other tools we use on the build environment also run in containers. For instance our code quality management tool, SonarQube and the MySQL database it uses also runs in separate containers. This way we don&rsquo;t need to install them on the EC2 instance directly, we only need to link them where needed &ndash; see the example above!</p>

<p>In our next blog post about continuous integration we&rsquo;ll explain the process we use at SequenceIQ to continuously deliver the new features to production using the Github flow with Jenkins and Docker.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Job profiling with R]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/05/01/mapreduce-job-profiling-with-R/"/>
    <updated>2014-05-01T21:08:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/05/01/mapreduce-job-profiling-with-R</id>
    <content type="html"><![CDATA[<p>Management of a large Hadoop cluster is not an easy task &ndash; however thanks to projects like <a href="http://ambari.apache.org/">Apache Ambari</a> these tasks are getting easier. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its REST API to provision, manage and monitor a Hadoop cluster. While Ambari helps us a lot to monitor a cluster (leverages <a href="http://ganglia.sourceforge.net/">Ganglia</a> and <a href="http://www.nagios.org/">Nagios</a>), many times we have to profile our MapReduce jobs as well.</p>

<p>At SequenceIQ in order to profile MapReduce jobs, understand (job)internal statistics and create usefull graphs many times we rely on <a href="http://www.r-project.org/">R</a>. The metrics are collected from Ambari and the <a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html">YARN History Server</a>.</p>

<p>In this blog post we would like to explain and guide you through a simple process of collecting MapReduce job metrics, calculate different statistics and generate easy to understand charts.</p>

<p>The MapReduce application is the following:</p>

<ul>
<li>The input set of data is 12 pieces of 1 GB size files. Each file containes the same line of 16 bytes (012345678998765 plus the new line character)</li>
<li>The number of mappers running is 48, because the block size on HDFS is 256 MB and there are 12 files.</li>
<li>We use TextInputFormat (line num, line content) pairs. The output of the mapper function is the same as the input <code>IdentityMapper</code></li>
<li>The number of reducers is 20.</li>
<li>For simplicity we use <code>IdentityReducer</code> as the reducer function.</li>
<li>We use a special partitioner called <code>LinePartitoner</code>. The partitioning is based on line numbers (the key) and it makes sure that each reducer gets the same amount of data (line number <em>modulo</em> reducer number).</li>
</ul>


<h2>How to get the job results with R</h2>

<p>The job id that we are analysing with R is job_1395530889914_0005 (<em>replace this with your job is</em>)</p>

<p>First we load the R functions:</p>

<p><code>source("JobHistory.r")</code></p>

<p>Then we extract/read the job from the HistoryServer. It is actually using the Rest API of HistoryServer, parsing the JSON output.</p>

<p><code>job&lt;-getJob("job_1395530889914_0005","node02:19888")</code></p>

<p>The structure of the job follows the structure that is returned from the HistoryServer except that for example the parameters of all the tasks are converted into vectors so that can be easily handled in R.</p>

<!-- more -->


<p>A job is a list of <code>things</code>:</p>

<p><code>&gt; names(job)</code></p>

<p><code>[1] "job"      "counters" "tasks"    "attempts"</code></p>

<p>The job$job contains some basic data</p>

<p><code>&gt; names(job$job)</code></p>

<p><code>[1] "startTime"                "finishTime"               "id"                       "name"                     "queue"</code></p>

<p><code>[6] "user"                     "state"                    "mapsTotal"                "mapsCompleted"            "reducesTotal"</code></p>

<p><code>[11] "reducesCompleted"         "uberized"                 "diagnostics"              "avgMapTime"               "avgReduceTime"</code></p>

<p><code>[16] "avgShuffleTime"           "avgMergeTime"             "failedReduceAttempts"     "killedReduceAttempts"     "successfulReduceAttempts"</code></p>

<p><code>[21] "failedMapAttempts"        "killedMapAttempts"        "successfulMapAttempts"</code></p>

<p>The items below job$tasks are all vectors (if there are numeric) or non-named lists:</p>

<p><code>&gt; names(job$tasks)</code></p>

<p><code>[1] "startTime"         "finishTime"        "elapsedTime"       "progress"          "id"          "state"             "type"</code></p>

<p><code>[8] "successfulAttempt"</code></p>

<p>This way we can easily calculate the mean of the <code>running</code> times of all the tasks like this:</p>

<p><code>mean(job$tasks$finishTime-job$tasks$startTime)</code></p>

<p><code>[1] 147307</code></p>

<p>The <code>attempts</code> list also contains vectors or lists of parameters. Only the successful attempts are in the attempt list.</p>

<p><code>&gt; names(job$attempts)</code></p>

<p><code>[1] "startTime"           "finishTime"          "elapsedTime"         "progress"            "id"                  "rack"</code></p>

<p><code>[7] "state"               "nodeHttpAddress"     "diagnostics"         "type"                "assignedContainerId" "shuffleFinishTime"</code></p>

<p><code>[13] "mergeFinishTime"     "elapsedShuffleTime"  "elapsedMergeTime"    "elapsedReduceTime"</code></p>

<p>This way we can easily calculate the average <code>merge</code> times:</p>

<p><code>&gt; mean(job$attempts$mergeFinishTime-job$attempts$shuffleFinishTime)</code></p>

<p><code>[1] 4875.15</code></p>

<p>Which is the same as:</p>

<p><code>&gt; mean(job$attempts$elapsedMergeTime)</code></p>

<p><code>[1] 4875.15</code></p>

<h2>The R generated graphs</h2>

<p>The are two types of graphs for the beginning</p>

<p><code>plotTasksTimes(job)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_mr_task_times.png" alt="" /></p>

<p>This graph shows start and finish times for each tasks (mappers and reducers as well). The tasks are sorted by their start times, so the reducers are on the top. There are 48 mappers and 20 reducers. The times are relative to the startTime of the first mapper in milliseconds(could show absolute values as well).</p>

<p><code>plotActiveMRTasksNum(job)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_mr.png" alt="" /></p>

<p>The graph above contains the number of active tasks at each time. It shows the mappers with green and also show the reduce phases as well. The shuffle part is orange, the merge part is magenta and the reduce part (reducer function is running) is blue. The times are relative to the startTime of the first mapper in milliseconds (could show absolute values as well).</p>

<p><code>plotActiveReduceTasksNumDetailed(job, FALSE)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_reduce_phases.png" alt="" /></p>

<p>This graph shows only the reduce part with the three phases: shuffle, merge, reduce. The times are absolute times (could show absolute values as well).</p>

<p><code>plotTimeBoxes&lt;-function(data, nodeNum=21, slotsPerNode=4)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_mr_by_nodes.png" alt="" /></p>

<p>As you can see monitoring a MapReduce job through the HistoryServer it is extremely easy, and R is very usefull to apply different statistics and plot graphs. Also as you start playing with different setups the results can quickly be retrived, the graphs regenerated to analyze how different configuratins are affecting the execution time/behaviour of the jobs.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/96_mappers_20_reducers_mr_by_nodes.png" alt="" /></p>

<p>As always, the example project is available at our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-monitoring-R">GitHub</a> page. We are working on a <code>heuristic</code> queue scheduler for a better utilization of our cluster, and also to provide QoS on Hadoop &ndash; profiling and understanding the running MapReduce jobs and the job queues are essential for that. Also based on the charts broken down by nodes we can quickly identify servers with potential issues (slow I/O, memory, etc).</p>

<p>Follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a> to read about how we progress with the sceduler and get early access, or feel free to contribute to our YARN monitoring project.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL on HBase with Apache Phoenix]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/04/22/sql-on-hbase-with-apache-phoenix/"/>
    <updated>2014-04-22T12:24:11+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/04/22/sql-on-hbase-with-apache-phoenix</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we use HBase to store large amounts of high velocity data and interact with them &ndash; many times we use native HBase interfaces but recently there was a need (internal and external) to access the data through an SQL interface.</p>

<h2>Introduction</h2>

<p>HBase is an open-source, distributed, versioned, non-relational database modeled after Google&rsquo;s Bigtable. It&rsquo;s designed to handle
billions of rows and millions of columns. However, using it as a relational database where you would store your data normalized,
split into multiple tables is not easy and most likely you will struggle with it as you would do in any other non-relational database.
Here comes <a href="http://phoenix.incubator.apache.org/">Apache Phoenix</a> in the picture. It&rsquo;s an SQL skin over HBase delivered as a
client-embedded JDBC driver targeting low latency queries. The project is in incubating state and under heavy development, but you
can already start embracing it.</p>

<h2>Installation</h2>

<p>Download the appropriate distribution from <a href="http://xenia.sote.hu/ftp/mirrors/www.apache.org/incubator/phoenix/">here</a>:</p>

<ul>
<li>Phoenix 2.x &ndash; HBase 0.94.x</li>
<li>Phoenix 3.x &ndash; HBase 0.94.x</li>
<li>Phoenix 4.x &ndash; HBase 0.98.1+</li>
</ul>


<p><em>Note the compatibilities between the HBase and Phoenix versions</em></p>

<p>Alternatively you can clone the <a href="https://github.com/apache/incubator-phoenix/tree/4.0">repository</a> and build it yourself (mvn clean install -DskipTests).
It should produce a jar file like this: phoenix-<code>version</code>-incubating-client.jar. Copy it to HBase&rsquo;s classpath (easiest way is to copy into
HBASE_HOME/lib). If you have multiple nodes it has to be there on every node. Restart the RegionServers and you are good to go. That&rsquo;s it?
Yes!</p>

<h2>Sample</h2>

<p>We&rsquo;ve pre-cooked a <a href="https://github.com/sequenceiq/phoenix-docker">Docker</a> image for you so you can follow this sample and play with it:
(the image is based on Hadoop 2.4, HBase 0.98.1, Phoenix 4.1.0-SNAPSHOT) <code>docker run -i -t sequenceiq/phoenix</code>.</p>

<!-- more -->


<h3>Create tables</h3>

<p>The downloaded or built distribution&rsquo;s bin directory contains a pure-Java console based utility called sqlline.py. You can use this
to connect to HBase via the Phoenix JDBC driver. You need to specify the Zookeeper&rsquo;s QuorumPeer&rsquo;s address. If the default (2181) port is
used then type <em>sqlline.py localhost</em> (to quit type: !quit). Let&rsquo;s create two different tables:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='mysql'><span class='line'><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="nf">CUSTOMERS</span> <span class="p">(</span><span class="n">ID</span> <span class="kt">INTEGER</span> <span class="k">NOT</span> <span class="no">NULL</span> <span class="k">PRIMARY</span> <span class="k">KEY</span><span class="p">,</span> <span class="n">NAME</span> <span class="kt">VARCHAR</span><span class="p">(</span><span class="mi">40</span><span class="p">)</span> <span class="k">NOT</span> <span class="no">NULL</span><span class="p">,</span> <span class="n">AGE</span> <span class="kt">INTEGER</span> <span class="k">NOT</span> <span class="no">NULL</span><span class="p">,</span> <span class="n">CITY</span> <span class="kt">CHAR</span><span class="p">(</span><span class="mi">25</span><span class="p">));</span>
</span><span class='line'><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="nf">ORDERS</span> <span class="p">(</span><span class="n">ID</span> <span class="kt">INTEGER</span> <span class="k">NOT</span> <span class="no">NULL</span> <span class="k">PRIMARY</span> <span class="k">KEY</span><span class="p">,</span> <span class="kt">DATE</span> <span class="kt">DATE</span><span class="p">,</span> <span class="n">CUSTOMER_ID</span> <span class="kt">INTEGER</span><span class="p">,</span> <span class="n">AMOUNT</span> <span class="kt">DOUBLE</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>It&rsquo;s worth checking which <a href="http://phoenix.incubator.apache.org/language/datatypes.html">datatypes</a> and
<a href="http://phoenix.incubator.apache.org/language/index.html">functions</a> are currently supported. These tables will be translated into
HBase tables and the metadata is stored along with it and versioned, such that snapshot queries over prior versions will automatically
use the correct schema. You can check with HBase shell as <code>describe 'CUSTOMERS'</code></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='mysql'><span class='line'><span class="n">DESCRIPTION</span>                                                                                                                         <span class="n">ENABLED</span>
</span><span class='line'> <span class="s1">&#39;CUSTOMERS&#39;</span><span class="p">,</span> <span class="err">{</span><span class="n">TABLE_ATTRIBUTES</span> <span class="o">=&gt;</span> <span class="err">{</span><span class="n">coprocessor</span><span class="err">$</span><span class="mi">1</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apache.phoenix.coprocessor.ScanRegionObserver|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">2</span> <span class="o">=&gt;</span> <span class="s1">&#39;|or true</span>
</span><span class='line'><span class="s1"> g.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">3</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apache.phoenix.coprocessor.GroupedAggreg</span>
</span><span class='line'><span class="s1"> ateRegionObserver|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">4</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apache.phoenix.coprocessor.ServerCachingEndpointImpl|1|&#39;</span><span class="p">,</span> <span class="n">coprocessor</span><span class="err">$</span><span class="mi">5</span> <span class="o">=&gt;</span> <span class="s1">&#39;|org.apa</span>
</span><span class='line'><span class="s1"> che.phoenix.hbase.index.Indexer|1073741823|index.builder=org.apache.phoenix.index.PhoenixIndexBuilder,org.apache.hadoop.hbase.inde</span>
</span><span class='line'><span class="s1"> x.codec.class=org.apache.phoenix.index.PhoenixIndexCodec&#39;</span><span class="err">}</span><span class="p">,</span> <span class="err">{</span><span class="n">NAME</span> <span class="o">=&gt;</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">DATA_BLOCK_ENCODING</span> <span class="o">=&gt;</span> <span class="s1">&#39;FAST_DIFF&#39;</span><span class="p">,</span> <span class="n">BLOOMFILTER</span> <span class="o">=&gt;</span> <span class="s1">&#39;ROW&#39;</span>
</span><span class='line'> <span class="p">,</span> <span class="n">REPLICATION_SCOPE</span> <span class="o">=&gt;</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">VERSIONS</span> <span class="o">=&gt;</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="n">COMPRESSION</span> <span class="o">=&gt;</span> <span class="s1">&#39;NONE&#39;</span><span class="p">,</span> <span class="n">MIN_VERSIONS</span> <span class="o">=&gt;</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">TTL</span> <span class="o">=&gt;</span> <span class="s1">&#39;2147483647&#39;</span><span class="p">,</span> <span class="n">KEEP_DELETED_CELLS</span> <span class="o">=</span>
</span><span class='line'> <span class="o">&gt;</span> <span class="s1">&#39;true&#39;</span><span class="p">,</span> <span class="n">BLOCKSIZE</span> <span class="o">=&gt;</span> <span class="s1">&#39;65536&#39;</span><span class="p">,</span> <span class="n">IN_MEMORY</span> <span class="o">=&gt;</span> <span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="n">BLOCKCACHE</span> <span class="o">=&gt;</span> <span class="s1">&#39;true&#39;</span><span class="err">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see there are bunch of co-processors. Co-processors were introduced in version 0.92.0 to push arbitrary computation out
to the HBase nodes and run in parallel across all the RegionServers. There are two types of them: <code>observers</code> and <code>endpoints</code>.
Observers allow the cluster to behave differently during normal client operations. Endpoints allow you to extend the cluster’s
capabilities, exposing new operations to client applications. Phoenix uses them to translate the SQL queries to scans and that&rsquo;s
why it can operate so quickly. It is also possible to map an existing HBase table to a Phoenix table. In this case the binary
representation of the row key and key values must match one of the Phoenix data types.</p>

<h3>Load data</h3>

<p>After the tables are created fill them with data. For this purpose we&rsquo;ll use the <a href="http://www.jooq.org/">Jooq</a> library&rsquo;s fluent API.
The related sample project (Spring based) can be found in our
<a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/phoenix-jooq">GitHub</a> repository. To connect you&rsquo;ll need Phoenix&rsquo;s
JDBC driver on your classpath (org.apache.phoenix.jdbc.PhoenixDriver). At the moment it is not available anywhere, but temporary we
deployed into our <a href="https://github.com/sequenceiq/sequenceiq-maven-repo">maven</a> repository, so use it if you&rsquo;d like, but don&rsquo;t rely on that
it will be always there in this form. The url to connect to should be familiar as it uses the same Zookeeper QuorumPeer&rsquo;s address:
<code>jdbc:phoenix:localhost:2181</code>. Unfortunately Jooq&rsquo;s insert statement is not suitable for us since the JDBC driver only supports the
upsert statement so we cannot make use of the fluent API here.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">String</span> <span class="n">userSql</span> <span class="o">=</span> <span class="n">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;upsert into customers values (%d, &#39;%s&#39;, %d, &#39;%s&#39;)&quot;</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">i</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">escapeSql</span><span class="o">(</span><span class="n">names</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="n">names</span><span class="o">.</span><span class="na">size</span><span class="o">()</span> <span class="o">-</span> <span class="mi">1</span><span class="o">))),</span>
</span><span class='line'>                    <span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="mi">40</span><span class="o">)</span> <span class="o">+</span> <span class="mi">18</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">escapeSql</span><span class="o">(</span><span class="n">locales</span><span class="o">[</span><span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="n">locales</span><span class="o">.</span><span class="na">length</span> <span class="o">-</span> <span class="mi">1</span><span class="o">)].</span><span class="na">getDisplayCountry</span><span class="o">()));</span>
</span><span class='line'><span class="n">String</span> <span class="n">orderSql</span> <span class="o">=</span> <span class="n">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;upsert into orders values (%d, CURRENT_DATE(), %d, %d)&quot;</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">i</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">i</span><span class="o">,</span>
</span><span class='line'>                    <span class="n">random</span><span class="o">.</span><span class="na">nextInt</span><span class="o">(</span><span class="mi">1</span><span class="n">_000_000</span><span class="o">));</span>
</span><span class='line'><span class="n">dslContext</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="n">userSql</span><span class="o">);</span>
</span><span class='line'><span class="n">dslContext</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="n">orderSql</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Query</h3>

<p>On the generated data let&rsquo;s create queries:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">dslContext</span>
</span><span class='line'>          <span class="o">.</span><span class="na">select</span><span class="o">()</span>
</span><span class='line'>          <span class="o">.</span><span class="na">from</span><span class="o">(</span><span class="n">tableByName</span><span class="o">(</span><span class="s">&quot;customers&quot;</span><span class="o">).</span><span class="na">as</span><span class="o">(</span><span class="s">&quot;c&quot;</span><span class="o">))</span>
</span><span class='line'>          <span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">tableByName</span><span class="o">(</span><span class="s">&quot;orders&quot;</span><span class="o">).</span><span class="na">as</span><span class="o">(</span><span class="s">&quot;o&quot;</span><span class="o">)).</span><span class="na">on</span><span class="o">(</span><span class="s">&quot;o.customer_id = c.id&quot;</span><span class="o">)</span>
</span><span class='line'>          <span class="o">.</span><span class="na">where</span><span class="o">(</span><span class="n">fieldByName</span><span class="o">(</span><span class="s">&quot;o.amount&quot;</span><span class="o">).</span><span class="na">lessThan</span><span class="o">(</span><span class="n">amount</span><span class="o">))</span>
</span><span class='line'>          <span class="o">.</span><span class="na">orderBy</span><span class="o">(</span><span class="n">fieldByName</span><span class="o">(</span><span class="s">&quot;c.name&quot;</span><span class="o">).</span><span class="na">asc</span><span class="o">())</span>
</span><span class='line'>          <span class="o">.</span><span class="na">fetch</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>This query resulted the following:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">+----+------------+-----+-----------+----+----------+-------------+--------+</span>
</span><span class='line'><span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">ID</span><span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">NAME</span>      <span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">AGE</span><span class="o">|</span><span class="n">C</span><span class="o">.</span><span class="na">CITY</span>     <span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">ID</span><span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">DATE</span>    <span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">CUSTOMER_ID</span><span class="o">|</span><span class="n">O</span><span class="o">.</span><span class="na">AMOUNT</span><span class="o">|</span>
</span><span class='line'><span class="o">+----+------------+-----+-----------+----+----------+-------------+--------+</span>
</span><span class='line'><span class="o">|</span> <span class="mi">976</span><span class="o">|</span><span class="n">Bogan</span><span class="o">,</span> <span class="n">Elias</span><span class="o">|</span>   <span class="mi">26</span><span class="o">|</span><span class="n">Japan</span>      <span class="o">|</span> <span class="mi">976</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">976</span><span class="o">|</span>  <span class="mf">8664.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">827</span><span class="o">|</span><span class="n">Constrictor</span> <span class="o">|</span>   <span class="mi">29</span><span class="o">|{</span><span class="kc">null</span><span class="o">}</span>     <span class="o">|</span> <span class="mi">827</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">827</span><span class="o">|</span>  <span class="mf">7856.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">672</span><span class="o">|</span><span class="n">Hardwire</span>    <span class="o">|</span>   <span class="mi">31</span><span class="o">|</span><span class="n">Tunisia</span>    <span class="o">|</span> <span class="mi">672</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">672</span><span class="o">|</span>  <span class="mf">9292.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">746</span><span class="o">|</span><span class="n">Lady</span> <span class="n">Killer</span> <span class="o">|</span>   <span class="mi">37</span><span class="o">|</span><span class="n">Cyprus</span>     <span class="o">|</span> <span class="mi">746</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">746</span><span class="o">|</span>  <span class="mf">1784.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">242</span><span class="o">|</span><span class="n">Lifeforce</span>   <span class="o">|</span>   <span class="mi">35</span><span class="o">|</span><span class="n">Switzerland</span><span class="o">|</span> <span class="mi">242</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">242</span><span class="o">|</span>  <span class="mf">5406.0</span><span class="o">|</span>
</span><span class='line'><span class="o">|</span> <span class="mi">487</span><span class="o">|</span><span class="n">Topspin</span>     <span class="o">|</span>   <span class="mi">48</span><span class="o">|{</span><span class="kc">null</span><span class="o">}</span>     <span class="o">|</span> <span class="mi">487</span><span class="o">|</span><span class="mi">2014</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">20</span><span class="o">|</span>          <span class="mi">487</span><span class="o">|</span>  <span class="mf">6512.0</span><span class="o">|</span>
</span><span class='line'><span class="o">+----+------------+-----+-----------+----+----------+-------------+--------+</span>
</span></code></pre></td></tr></table></div></figure>


<p>The same thing could&rsquo;ve been achieved with sqlline also.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='mysql'><span class='line'><span class="k">select</span> <span class="n">c</span><span class="p">.</span><span class="n">name</span> <span class="k">as</span> <span class="n">name</span><span class="p">,</span> <span class="n">o</span><span class="p">.</span><span class="n">amount</span> <span class="k">as</span> <span class="n">amount</span><span class="p">,</span> <span class="n">o</span><span class="p">.</span><span class="kt">date</span> <span class="k">as</span> <span class="kt">date</span> <span class="k">from</span> <span class="n">customers</span> <span class="k">as</span> <span class="n">c</span> <span class="k">inner</span> <span class="k">join</span> <span class="n">orders</span> <span class="k">as</span> <span class="n">o</span> <span class="k">on</span> <span class="n">o</span><span class="p">.</span><span class="n">id</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="n">id</span> <span class="k">where</span> <span class="n">o</span><span class="p">.</span><span class="n">amount</span> <span class="o">&lt;</span> <span class="mi">10000</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Nested queries are not supported yet, but it will come soon.</p>

<h2>Summary</h2>

<p>As you saw it is pretty easy to get started with Phoenix both command line and programmatically. There are lots of lacking features, but
the contributors are dedicated and working hard to make this project moving forward. Next step? ORM for HBase? It is also ongoing.. :)</p>

<p>Follow up with <a href="https://www.linkedin.com/company/sequenceiq/">us</a> if you are interested in HBase and building an SQL interface on top.
Don&rsquo;t hesitate to contact us should you have any questions.</p>

<p><a href="http://sequenceiq.com/">SequenceIQ</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Phoenix (sneak peak)]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/04/17/apache-phoenix-sneak-peak/"/>
    <updated>2014-04-17T13:51:08+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/04/17/apache-phoenix-sneak-peak</id>
    <content type="html"><![CDATA[<script type="text/javascript" src="https://asciinema.org/a/8982.js" id="asciicast-8982" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Writing MapReduce jobs in Scala]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/04/14/mapreduce-with-scalding/"/>
    <updated>2014-04-14T11:55:38+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/04/14/mapreduce-with-scalding</id>
    <content type="html"><![CDATA[<p>At SequenceIQ we have many pre-built and configurable MapReduce jobs (complex math algorithms, filtering, sorting and correlation patterns, samplings, top-n, joins, partitioning, etc) &ndash; as building blocks of our job worklow. We needed to find a quick way to build and test these jobs during developement on &lsquo;local&rsquo; mode and be able to push the same jobs to a large test cluster without any modifications.
Though in general we use Java, we always strive for efficiency when we need to solve a problem and we use different  languages (not just JVM based) in our stack (e.g. Groovy, Go and R) &ndash; to write MapReduce jobs we have choosen Scala and the Scalding library. Scalding is a Scala library developed by Twitter that abstracts and makes easy to write Hadoop MapReduce jobs. In many ways Scalding is similar to Pig, but it was writen in Scala, bringing the advantages of Scala to your MapReduce jobs (e.g. type safety &ndash; how many times you have submitted a job to a cluster only to learn 5 hours later that you can&rsquo;t convert a String to Double).</p>

<p>This example will show you how you can use Scalding with Hadoop 2.3 and how easy is to write a MapReduce job with few lines of Scala code.</p>

<h2>Build the project</h2>

<p>In our example we will transform a csv file to an other one with a filter step.
To build the project use:</p>

<p><code>./gradlew clean build</code> in the project library.</p>

<h2>Run the sample</h2>

<p>To run the sample with these parameters in local mode:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>yarn jar scalding-sample-0.1.jar com.sequenceiq.samples.scalding.CsvToCsvFilterJob --local --schema <span class="o">{</span>YOUR_SCHEME<span class="o">}</span> --input <span class="o">{</span>INPUT<span class="o">}</span> --type <span class="o">{</span>TYPE<span class="o">}</span> --operator <span class="o">{</span>OPERATOR<span class="o">}</span> --field <span class="o">{</span>FILTER_FIELD<span class="o">}</span> --operand <span class="o">{</span>OPERAND<span class="o">}</span> --output <span class="o">{</span>OUTPUT_PATH<span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>or if you want to run the exampke using HDFS then use:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>yarn jar scalding-sample-0.1.jar com.sequenceiq.samples.scalding.CsvToCsvFilterJob --hdfs --schema <span class="o">{</span>YOUR_SCHEME<span class="o">}</span> --input <span class="o">{</span>INPUT<span class="o">}</span> --type <span class="o">{</span>TYPE<span class="o">}</span> --operator <span class="o">{</span>OPERATOR<span class="o">}</span> --field <span class="o">{</span>FILTER_FIELD<span class="o">}</span> --operand <span class="o">{</span>OPERAND<span class="o">}</span> --output <span class="o">{</span>OUTPUT_PATH<span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To run the filtering example the parameters are like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>yarn jar scalding-sample-0.1.jar com.sequenceiq.samples.scalding.CsvToCsvFilterJob --hdfs --schema id,name --input /input.csv --type int --operator eq --field id --operand 1 --output /output.csv
</span></code></pre></td></tr></table></div></figure>


<p>The code looks extremely simple:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">validation</span><span class="o">()</span>
</span><span class='line'>  <span class="n">input</span><span class="o">(</span><span class="n">args</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="na">filter</span><span class="o">(</span><span class="n">filterableField</span><span class="o">)</span> <span class="o">{</span><span class="nl">field:</span> <span class="n">String</span> <span class="o">=&gt;</span> <span class="n">createFilterCriterion</span><span class="o">(</span><span class="n">field</span><span class="o">)}</span>
</span><span class='line'>    <span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">output</span><span class="o">(</span><span class="n">args</span><span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>First there is a validation and in case of the input data is OK then we are doing a filtering with the specified criterias.
In this example (as in all our other examples) we are using Hadoop 2 &ndash; with the ability to submit Scalding jobs into a remote Hadoop 2 cluster. Note that Scalding depends on the Cascading library which does not support Hadoop 2 and there is no ability to submit jobs to a remote cluster &ndash; our example has removed the Hadoop 1 dependencies and lets you to submit jobs to any remote Hadoop 2 cluster.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>     <span class="n">JobRunner</span><span class="o">.</span><span class="na">runJob</span><span class="o">(</span>
</span><span class='line'>          <span class="n">configurationService</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">(),</span>
</span><span class='line'>        <span class="k">new</span> <span class="n">String</span><span class="o">[]{</span>
</span><span class='line'>            <span class="n">parameters</span><span class="o">..</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can get the example project from our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/scalding-sample">GitHub</a> repository.</p>

<p>Should you have any Scalding or Scala questions or observations let us know.
Enjoy,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop on Docker introduction]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction/"/>
    <updated>2014-04-04T18:24:17+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction</id>
    <content type="html"><![CDATA[<p>In the last few weeks we&rsquo;ve created and published several Docker images (<a href="https://github.com/sequenceiq/hadoop-docker">Hadoop</a>, <a href="https://github.com/sequenceiq/hoya-docker">Hoya</a>, <a href="https://github.com/sequenceiq/tez-docker">Tez</a>) to help you to quick-start with Hadoop and the latest innovations using YARN.
While many people have downloaded and started to use these preconfigured images we&rsquo;ve been asked to give a short introduction of what Docker is, and how one can build Docker images. Also during the Hadoop Summit in Amsterdem we have been inquired in particular about running Hadoop on Docker, so this post is our answer for all the requests we received.</p>

<p>Docker is an open-source engine that automates the deployment of any application as a lightweight, portable, self-sufficient container that will run virtually anywhere.</p>

<h2>Installation</h2>

<p>First install Docker with a package manager. On Ubuntu there is an easy way to start with by running a simple curl script which will do it for you:
<code>curl -s https://get.docker.io/ubuntu/ | sudo sh</code>.
Unfortunately Mac, Windows and some Linux distributions cannot natively run Docker (yet). At <a href="http://sequenceiq.com/">SequenceIQ</a> we develop on OSX and run a 3-6 node Hadoop mini cluster on our laptops. To overcome the limitation of running Docker natively
you will have to install <code>boot2docker</code>. It is a Tiny Core Linux made specifically to run Docker containers and weights less than 24MB memory.
Initialize <em>(boot2docker init)</em> and start <em>(boot2docker up)</em> and you can SSH into the VM <em>(boot2docker ssh, pass: tcuser)</em>.</p>

<p>To verify the installation let&rsquo;s test it: <code>docker run ubuntu /bin/echo hello docker</code>. Docker did a bunch of things within seconds:</p>

<ul>
<li>Downloaded the base image from the docker.io index</li>
<li>Created a new LXC container</li>
<li>Allocated a filesystem for it</li>
<li>Mounted a read-write layer</li>
<li>Allocated a network interface</li>
<li>Setup an IP for it, with network address translation</li>
<li>Executed a process inside the container</li>
<li>Captured the output and printed it</li>
</ul>


<p>You can run an interactive shell as well <code>docker run -i -t ubuntu /bin/bash</code> and use this shell as you would use any other shell.</p>

<p>While there are lots of different Docker images available we would like to share how to create your own images.</p>

<!-- more -->


<h2>Dockerfile</h2>

<p>The <code>Dockerfile</code> describes the build steps and it can be viewed as an image representation. They provide a simple syntax for building images and
they are a great way to automate and script the images creation. Dockerfile instructions look like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>INSTRUCTION arguments</span></code></pre></td></tr></table></div></figure>


<h3>FROM</h3>

<p>Every Dockerfile has to start with the <code>FROM image</code> instruction which sets the base image for subsequent instructions (e.g. in our <a href="https://github.com/sequenceiq/hoya-docker">Hoya</a> and <a href="https://github.com/sequenceiq/tez-docker">Tez</a> images we used our <a href="https://github.com/sequenceiq/hadoop-docker">Hadoop</a> image as a base, while the Hadoop image was built on top of the <code>tianon/centos</code> base image).
A base image is built from a trusted build (more on this later) and in case of Hoya and Tez the base image was: <code>sequenceiq/hadoop-docker</code>. You can browse the available containers in the
<a href="https://index.docker.io/">Docker index</a>.</p>

<h3>RUN</h3>

<p>The next instruction is usually the <code>RUN command</code>. This will execute any commands on the current image and commit the results. The resulting committed image
will be used for the next step in the Dockerfile. Example: RUN yum install -y openssh-server. One important thing to keep in mind is that the
following set of instructions will not act as we would like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>RUN cd /usr/local  
</span><span class='line'>RUN mkdir apple  </span></code></pre></td></tr></table></div></figure>


<p>This will create an apple folder in the root directory. Surprised, huh? The reason of this that the RUN command is equivalent to the docker commands:
docker run image command + docker commit container_id, where the image would be replaced automatically with the current image,
and container_id would be the result of the previous RUN instruction. But it doesn&rsquo;t mean it can&rsquo;t be done:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>RUN cd /usr/local && mkdir apple</span></code></pre></td></tr></table></div></figure>


<h3>ADD</h3>

<p>The <code>ADD from to</code> command will copy the specified file into the container. Example:
ADD data.xml /usr/local/data.xml. In this case the data.xml is in the same directory as the Dockerfile. After this command you can rely on that this file
is present in the container and you can use it as well: RUN rm /usr/local/data.xml.</p>

<h3>EXPOSE</h3>

<p>The <code>EXPOSE port</code> instruction sets ports to be exposed to the host when running the image. Example: EXPOSE 8080 80 22 50070</p>

<h3>ENV</h3>

<p>Setting an environment variable by running a RUN export KEY=value won&rsquo;t work in dockerland. Instead you can use the <code>ENV key value</code> instruction.
Example: ENV JAVA_HOME /usr/java/default</p>

<h3>ENTRYPOINT</h3>

<p>The <code>ENTRYPOINT [command]</code> instruction permits you to trigger a command as soon as the container starts. Example: ENTRYPOINT [&ldquo;echo&rdquo;, &ldquo;Whale you be my container&rdquo;]</p>

<p>There are more instructions, but these are enough to start with and build your own images.</p>

<h2>Build &amp; Trusted build</h2>

<p>Once the Dockerfile is ready you can build it. If the file is in the current directory build it with <code>docker build .</code> (-t name to TAG the image). It&rsquo;s possible
to create trusted builds. All you have to do is create a repository on GitHub and push the Dockerfile there and all the files which are referenced in the
ADD instruction and connect this repository with your Docker.io account. Docker.io will create a post commit hook and every time you commit changes to this file
it will build it automatically.</p>

<h2>Usage</h2>

<p>Use this environment variable to make things easier: export DOCKER_HOST=tcp://localhost:4243. Few frequently used commands:</p>

<ul>
<li>List of your local images: docker images</li>
<li>List of running containers: docker ps</li>
<li>List of all containers: docker ps -a</li>
</ul>


<p>After you built your image it should show in the image list, and ready to use. Run it with <code>docker run -i -t -P image_name /bin/bash</code>. The -P variable will
publish all exposed ports to the host interfaces.</p>

<h2>Complete example</h2>

<p>As a reference check out our Hadoop 2.3 based <a href="https://github.com/sequenceiq/hadoop-docker">Dockerfile</a>.</p>

<h2>OSX Tweaks</h2>

<h3>Passwordless ssh</h3>

<p>On OSX it&rsquo;s quite tedious to always type tcuser password when you ssh into boot2docker. You can install your public key with a oneliner. You have to set the
KEYCHAIN variable to your <a href="http://keychain.io">Keychain.io</a> registered email.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>(export KEYCHAIN=&lt;email&gt;; curl -L j.mp/chain2docker|bash)</span></code></pre></td></tr></table></div></figure>


<p>If you restart boot2docker, you have to run this command again, for a passwordless ssh. To install your public ssh key into keychain is as simple as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -s ssh.keychain.io/&lt;email&gt;/upload | bash</span></code></pre></td></tr></table></div></figure>


<p>than you will receive a confirmation email, that&rsquo;s all.</p>

<h3>Expose ports from boot2docker to host</h3>

<p>Let&rsquo;s say you have a docker image starting Hadoop Name Node on port 50070. When you start 3 images you will get something like this:</p>

<ul>
<li>instance1: 50070 &ndash;> 49153</li>
<li>instance1: 50070 &ndash;> 49154</li>
<li>instance1: 50070 &ndash;> 49155</li>
</ul>


<p>But all those 4915X ports are only available when you are inside of boot2docker. Now if you forward all 49XXX ports straight to to your host,
you can reach the namenodes in your browser running on your mac as: <a href="http://localhost:4915X">http://localhost:4915X</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>boot2docker stop
</span><span class='line'>for i in {49000..49900}; do
</span><span class='line'> echo -n .
</span><span class='line'> VBoxManage modifyvm "boot2docker-vm" --natpf1 "tcp-port$i,tcp,,$i,,$i";
</span><span class='line'> VBoxManage modifyvm "boot2docker-vm" --natpf1 "udp-port$i,udp,,$i,,$i";
</span><span class='line'>done
</span><span class='line'>boot2docker up</span></code></pre></td></tr></table></div></figure>


<p>That&rsquo;s it. Hope this helps you to start with building your own Docker images. Let us know how it goes, we are happy to help you quick start Hadoop on Docker.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Mahout with Tez]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/31/mahout-on-tez/"/>
    <updated>2014-03-31T10:22:09+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/31/mahout-on-tez</id>
    <content type="html"><![CDATA[<p>At SequenceIQ we are always open to the latest innovations in Hadoop, and trying to find a way to offer a better performance and cluster utilization to our customers. We came in close touch with the <a href="http://hortonworks.com/labs/stinger/">Stinger initiative</a> last year at the Hadoop Summit in Amsterdam &ndash; and ever since we have followed up with the project progress (latest <a href="http://hortonworks.com/blog/apache-tez-0-3-released/">release</a> is 0.3). The project was initiated by Hortonworks with the goal of a 100x performance improvement of Hive.
Although Hive is not part of our product stack (we use other ways for SQL on Hadoop), there is one particular key component of the Stinger initiative which was very interesting to us: <a href="https://github.com/apache/incubator-tez">Apache Tez</a>.</p>

<p><a href="http://incubator.apache.org/projects/tez.html">Apache Tez</a> is a new application framework built on top of Hadoop Yarn that can execute complex directed acyclic graphs (DAGs) of general data processing tasks. In many ways it can be thought of as a more flexible and powerful successor of the map-reduce framework. This was exactly what draw our attention and made us start thinking about using Tez as our runtime for map-reduce jobs.</p>

<h2>Tez and MapReduce</h2>

<p>At SequenceIQ we have chains of map-reduce jobs which are scheduled individually and read the output of previous jobs from HBase or HDFS. Many times our map-reduce job flow can be represented as a map-reduce-reduce pattern, however building complex job chains with the current map-reduce framework is not that easy (nor saves on performance) &ndash; we combined the ChainMapper/ChainReducer and IdentityMapper trying to build MRR like DAG job flows.</p>

<p>In Tez data coming from reducers&#8217; output can be pipelined together and eliminates IO/sync barriers, as no temporary HDFS write is required. Jobs can also be chained and represented as MRR steps with no restriction.
In MapReduce disregarding the data size, the shuffle (internal step between the map and reducer) phase writes the sorted partitions to disk, merge-sorts them and feed into the reducers. All these steps are done <em>in memory</em> with Tez and saves on this I/O heavy step, avoiding unnecessary temporary writes and reads.</p>

<h2>Tez and Mahout</h2>

<p>Part of our system is running machine learning algorithms in batch, using Mahout (we do ML on streaming data using Scala, MLlib and Apache Spark as well). To improve the runtime performance of these Mahout algorithms, and decrease the cluster execution time we started to experiment with combining Tez and Mahout, and rewrite a few Mahout drivers in order to build DAGs of MR jobs (MRR in particular where applicable) and submit the jobs in a Tez runtime on a YARN cluster.</p>

<!--more-->


<p>In this blog post we would like to introduce you to Tez &ndash; for your convenience we have put together a Hadoop 2.3/YARN/Tez  <a href="https://github.com/sequenceiq/tez-docker">Tez-Docker</a> image &ndash; where the Tez runtime is already pre-configured. We have submitted a Mahout classification job into a YARN cluster as a regular MR job and then resubmitted the same job into Tez on a YARN cluster. Finally we made some metrics to highlight the differences: both in elapsed time and resource utilization.</p>

<p>If you don&rsquo;t want to use this docker image, you should configure Tez on your Hadoop cluster first.</p>

<h3>Building Tez</h3>

<p>Get the Tez code from <a href="https://github.com/apache/incubator-tez">GitHub</a>, and run <code>mvn clean install -DskipTests=true -Dmaven.javadoc.skip=true</code>. Alternatively you can get the jars from <a href="https://s3-eu-west-1.amazonaws.com/seq-tez/tez-0.3.0-incubating.tar.gz">SequenceIQ S3</a> and copy into HDFS under the &lsquo;/usr/lib/tez&rsquo; folder.</p>

<h3>Add *-site.xml</h3>

<p>Add <a href="https://raw.githubusercontent.com/sequenceiq/tez-docker/master/tez-site.xml">tez-site.xml</a> and <a href="https://github.com/sequenceiq/tez-docker/blob/master/mapred-site.xml">mapred-site.xml</a> to Hadoop (in case of the docker image it&rsquo;s $HADOOP_PREFIX/etc/hadoop/).</p>

<h3>Add Tez jars and config to HADOOP_CLASSPATH</h3>

<p>Edit your hadoop-env.sh file by executing this script:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">echo</span> <span class="s1">&#39;TEZ_JARS=/usr/local/tez/*&#39;</span> &gt;&gt; <span class="nv">$HADOOP_PREFIX</span>/etc/hadoop/hadoop-env.sh
</span><span class='line'><span class="nb">echo</span> <span class="s1">&#39;TEZ_LIB=/usr/local/tez/lib/*&#39;</span> &gt;&gt; <span class="nv">$HADOOP_PREFIX</span>/etc/hadoop/hadoop-env.sh
</span><span class='line'><span class="nb">echo</span> <span class="s1">&#39;TEZ_CONF=/usr/local/hadoop/etc/hadoop&#39;</span> &gt;&gt; <span class="nv">$HADOOP_PREFIX</span>/etc/hadoop/hadoop-env.sh
</span><span class='line'><span class="nb">echo</span> <span class="s1">&#39;export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$TEZ_CONF:$TEZ_JARS:$TEZ_LIB&#39;</span> &gt;&gt; <span class="nv">$HADOOP_PREFIX</span>/etc/hadoop/hadoop-env.sh
</span></code></pre></td></tr></table></div></figure>


<p>Make sure you set your HADOOP_PREFIX env variable, or use <a href="http://ambari.apache.org/">Apache Ambari</a> to configure Tez (change the mapreduce.framework.name property to yarn-tez).</p>

<h3>Submit a classification job &ndash; get the code and instructions from the SequenceIQ samples <em><a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/tez-dag-jobs">GitHub</a></em> page.</h3>

<p>After running the job and collecting the metrics we will see that the differences between using MapReduce and Tez are quite significant (~10x faster with Tez).</p>

<p>Below you can see the sample Mahout classification job submitted in YARN using MapReduce.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/tez-dag-jobs/resources/Classification_Mahout_MR.png" alt="" /></p>

<p>Below you can see the sample Mahout classification job submitted in YARN using Tez.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/tez-dag-jobs/resources/Classification_Mahout_TEZ.png" alt="" /></p>

<p>If we dig into deeper metrics we can see the huge differences between the file operations and HDFS I/O. The Tez framework does way less file operations as the MapReduce one.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/tez-dag-jobs/resources/fileops_tez_vs_mr.png" alt="" /></p>

<p>Also if we check the HDFS I/O operations we see the same results &ndash; less and more efficient HDFS operations in case of Tez.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/tez-dag-jobs/resources/hdfsio_tez_vs_mr.png" alt="" /></p>

<p>All these are because the Tez runtime is using in-memory operations whenever is possible instead of temporarily persisting the sorted partitions to HDFS.
Tez and <a href="http://hortonworks.com/labs/stinger/">Hortonworks&#8217; Stinger initiative</a> is opening up new possibilities to write faster and more performant Hadoop jobs, and closes the gap between stream and batch processing.</p>

<p>We are in the middle of rewriting &ndash; and sharing with the Hadoop community all the Mahout drivers we use &ndash; to Apache Tez. Also we are in the middle of proof-of-concepting our Scala/Scalding based map-reduce jobs to use Tez as a runtime.</p>

<p>Follow up with this <a href="http://blog.sequenceiq.com/">blog</a> and visit our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/tez-dag-jobs">GitHub</a> page for further details.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Hortonworks Hoya at SequenceIQ]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/24/hoya-at-sequenceiq/"/>
    <updated>2014-03-24T03:54:09+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/24/hoya-at-sequenceiq</id>
    <content type="html"><![CDATA[<p>With this blog post we are starting a series of articles where we&rsquo;d like to describe how we use YARN, why it is central to our product stack and why we believe that Hortonworks Hoya will be a determining building block in the Hadoop ecosystem.</p>

<p>While we don&rsquo;t want to get in details about <a href="http://hortonworks.com/hadoop/yarn/">YARN</a>, we&rsquo;d like to briefly explain the advantages of running an application on YARN, and introduce you to <a href="https://github.com/hortonworks/hoya">Hortonworks Hoya</a>.</p>

<p>At SequenceIQ we are building a multi-tenant, scale on demand data platform, with unpredictable batch and streaming workloads.
Before YARN we have tried different cluster management frameworks with Hadoop and managed to have pretty good results with <a href="http://aws.amazon.com/autoscaling/">Amazon EC2 Autoscaling groups</a>. Being true believers in open source and the need to diversify of provisioning Hadoop on different environments we needed to find an open source and &lsquo;standardised&rsquo; solution &ndash; welcome YARN.
(for example we provision Hadoop on <a href="http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker/">Docker</a>).</p>

<p>YARN separates the processing engine from the resource management &ndash; and acting effectively as an OS for Hadoop.
With YARN, you can now run multiple applications in Hadoop, all sharing a common resource management and improving cluster utilisation (we will release some metrics soon).
YARN also provides the following features out of the box:</p>

<ul>
<li>Multi-tenancy</li>
<li>Management and monitoring</li>
<li>High availability</li>
<li>Security</li>
<li>Failover and recovery</li>
</ul>


<p>All of the above and the effort of the Hadoop community and the wide adoption convinced us to start implementing our platform to run on top of YARN.
During our proof of concepts we went as far as starting all our non Hadoop (and not YARN compatible) applications on YARN &ndash; using <a href="https://github.com/hortonworks/hoya">Hoya</a>.</p>

<p>Hoya was introduced by Hortonworks mid last year &ndash; with the purpose to create Apache HBase clusters on YARN (since than it supports Apache Accumulo as well).
The code evolved pretty fast and now Hoya is a framework/application which allows you to deploy existing distributed applications on YARN &ndash; and benefit all the nice features of YARN.</p>

<p>In order to support different applications Hoya has a plugin provider architecture (supported plugins are in the <em>org.apache.hoya.providers</em> package).
Once a plugin is implemented (pretty straightforward, took us a few days only to understand and build a Flume and Tomcat plugin), the application is started in a YARN container and is monitored and controlled by YARN/Hoya.
The clusters can be started, stopped, frozen and re-sized dynamically &ndash; and in case of container failures Hoya deploys a replacement.</p>

<p>For a better architectural understanding of Hoya please read the following blog <a href="http://hortonworks.com/blog/hoya-hbase-on-yarn-application-architecture/">post</a>.</p>

<p>In this first post we would like to help you to get familiar with the benefits offered by Hoya and start an HBase cluster, re-scale it dynamically, freeze and stop.
First and foremost you will need an installation of Hadoop (2.3), the latest Hoya release (0.13.1) and HBase (0.98).
For your convenience we have put together an automated install script which lets you start with Hoya in a few minutes.</p>

<p>The script is available from our <a href="https://github.com/sequenceiq/hoya-docker/blob/master/hoya-centos-install.sh">GitHub page</a>. Also an official docker.io image is available at <a href="https://index.docker.io/u/sequenceiq/hoya-docker/">hoya-docker</a>, and the Dockerfile can be downloaded from our <a href="https://github.com/sequenceiq/hoya-docker">GitHub</a> page.</p>

<p>Once Hadoop, HBase and Hoya are installed you can create an HBase cluster.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>create-hoya-cluster<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  hoya create hbase --role master 1 --role worker 1
</span><span class='line'>    --manager localhost:8032
</span><span class='line'>    --filesystem hdfs://localhost:9000 --image hdfs://localhost:9000/hbase.tar.gz
</span><span class='line'>    --appconf file:///tmp/hoya-master/hoya-core/src/main/resources/org/apache/hoya/providers/hbase/conf
</span><span class='line'>    --zkhosts localhost
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This will launch a 2 node HBase cluster (1 Master and 1 RegionServer). Now lets increase the number of RegionServers.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>flex-hoya-cluster<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  <span class="nv">num_of_workers</span><span class="o">=</span><span class="nv">$1</span>
</span><span class='line'>  hoya flex hbase --role worker <span class="nv">$num_of_workers</span> --manager localhost:8032 --filesystem hdfs://localhost:9000
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>This will start as many RegionServers as specified &ndash; in new YARN containers. Also the size of the cluster can be decreased if the load on the system does not demand for a larger number of RegionServers. The cluster can also be freezed (Hoya takes care about persisting the state).</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>freeze-hoya-cluster<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  hoya freeze hbase --manager localhost:8032 --filesystem hdfs://localhost:9000
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Finally when you&rsquo;d like to destroy the cluster and the state associated with the application you can use:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>destroy-hoya-cluster<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  hoya destroy hbase --manager localhost:8032 --filesystem hdfs://localhost:9000
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you see installing Hoya and starting different applications (HBase in this case) is very simple &ndash; and all the nice features of YARN are instantly available for any clustered applications.
In our next post we will drive you through the steps of creating your own Hoya provider, deploy it and run on a YARN cluster.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop 2.3 with docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker/"/>
    <updated>2014-03-19T03:54:09+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker</id>
    <content type="html"><![CDATA[<p>You want to try out hadoop 2.3? Go to the zoo and <a href="http://sethgodin.typepad.com/seths_blog/2005/03/dont_shave_that.html">shave a yak</a>.
Or simply just use <a href="https://www.docker.io/">docker</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -i -t sequenceiq/hadoop-docker /etc/bootstrap.sh -bash</span></code></pre></td></tr></table></div></figure>


<h2>Testing</h2>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># start ssh and hdfs</span>
</span><span class='line'><span class="nb">cd</span> <span class="nv">$HADOOP_PREFIX</span>
</span><span class='line'>
</span><span class='line'><span class="c"># run the mapreduce</span>
</span><span class='line'>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.3.0.jar grep input output <span class="s1">&#39;dfs[a-z.]+&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="c"># check the output</span>
</span><span class='line'>bin/hdfs dfs -cat output/*
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>Yak shaving an elefant</h2>

<p>I had problems installing hadoop 2.3 and by googling i stumbled upon this <a href="http://mail-archives.apache.org/mod_mbox/hadoop-mapreduce-user/201403.mbox/%3C53192FD4.2040003@oss.nttdata.co.jp%3E">email thread</a>,
which references an <a href="http://aajisaka.github.io/hadoop-project/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation">alternative hadoop docs</a> deployed on github.</p>

<p>By following that description i run into an other issue:
hadoop is delivered with 32 bit native libraries. No big deal &hellip;</p>

<h2>Hadoop native libraries</h2>

<p>Of course there is an official <a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries Guide</a> it instructs you
to simple download the sources and <strong>mvn package</strong>. But than you face a new issue: missing <em>protobuf</em>. Eeeasy &hellip;</p>

<h2>Protobuf 2.5</h2>

<p>Unfortunately <strong>yum install protobuf</strong> installs an older 2.3 version, which is close but no cigar.
 So you download protobuf source, and <strong>./configure &amp;&amp; make &amp;&amp; make install</strong></p>

<p>To succeed on that one you have to install a couple of development packages, and there you go.</p>

<h2>Bintray</h2>

<p>I wanted to save you those steps so created a binary distro of the native libs
compiled with 64 bit CentOS. So I created <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.3.0/view/files">Bintray r̨epo</a>. Enjoy</p>

<h2>Automate everything</h2>

<p>As I&rsquo;m an automation fetishist, a Docker file was created, and released in the official <a href="https://index.docker.io/u/sequenceiq/hadoop-docker/">docker repo</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YARN Capacity Scheduler]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/14/yarn-capacity-scheduler/"/>
    <updated>2014-03-14T15:17:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/14/yarn-capacity-scheduler</id>
    <content type="html"><![CDATA[<p>Since the emergence of Hadoop 2 and the YARN based architecture we have a platform where we can run multiple applications (of different types) not constrained only to MapReduce. Different applications or different MapReduce job profiles have different resource needs, however since Hadoop 2.0 is a multi tenant platform the different users could have different access patterns or need for cluster capacity. In Hadoop 2.0 this is achieved through YARN schedulers — to allocate resources to various applications subject to constraints of capacities and queues (for more information on YARN follow this <a href="http://hortonworks.com/hadoop/yarn/">link</a> or feel free to ask us should you have any questions).</p>

<p>In Hadoop 2.0, the scheduler is a pluggable piece of code that lives inside the <em>ResourceManager</em> (the JobTracker in MR1) &ndash; the ultimate authority that arbitrates resources among all the applications in the system. The scheduler in YARN does not perform monitoring or status tracking and offers no guarantees to restart failed tasks — check our sample <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-queue-tests">GitHub</a> project to check how monitoring or progress can be tracked.</p>

<p>The Capacity Scheduler was designed to allow significantly higher cluster utilization while still providing predictability for Hadoop workloads, while sharing resources in a predictable and simple manner, using the common notion of <em>job queues</em>.</p>

<p>In our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-queue-tests">example</a> we show you how to use the Capacity Scheduler, configure queues with different priorities, submit MapReduce jobs into these queues, monitor and track the progress of the jobs &ndash; and ultimately see the differences between execution times and throughput of different queue setups.</p>

<p>First, let’s config the Capacity Scheduler (you can use xml, <a href="http://ambari.apache.org/">Apache Ambari</a> or you can configure queues programmatically). In this example we use a simple xml configuration.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.queues<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>default,highPriority,lowPriority<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.highPriority.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>70<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.lowPriority.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>20<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>We have 3 queues, with different queue setups/priorities. Each queue is given a <em>minimum</em> guaranteed percentage of total cluster capacity available &ndash; the total guaranteed capacity must equal 100%. In our example the <em>highPriority</em> queue has 70% of the resources, the <em>lowPriority</em> 20%, and the default queue has the remaining 10%. While it is not highlight in the example above, the Capacity Scheduler provides elastic resource scheduling, which means that if there are idle resources in the cluster, then one queue can take up more of the cluster capacity than was minimally allocated . In our case we could allocate a <em>maximum</em> capacity to the <em>lowPriority</em> queue:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root. lowPriority.maximum-capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;value&gt;</span>50<span class="nt">&lt;/value&gt;</span>
</span><span class='line'><span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>Now lets submit some jobs into these queues. We will use the QuasiMonteCarlo.java example (coming with Hadoop) &ndash; a MapReduce job that estimates the value of Pi, and submit the same MapReduce jobs into the low and high priority queues.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="c1">//get a configuration</span>
</span><span class='line'><span class="n">Configuration</span> <span class="n">priorityConf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="o">();</span>
</span><span class='line'><span class="n">priorityConf</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="s">&quot;mapreduce.job.queuename&quot;</span><span class="o">,</span> <span class="n">queueName</span><span class="o">);</span>
</span><span class='line'><span class="err">………………………</span>      
</span><span class='line'><span class="c1">//submit the job</span>
</span><span class='line'><span class="n">JobID</span> <span class="n">jobID</span> <span class="o">=</span> <span class="n">QuasiMonteCarlo</span><span class="o">.</span><span class="na">submitPiEstimationMRApp</span><span class="o">(</span><span class="err">“</span><span class="n">Pi</span> <span class="n">estimation</span> <span class="nl">into:</span> <span class="err">&quot;</span><span class="o">+</span> <span class="n">queueName</span><span class="o">,</span> <span class="mi">10</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="n">tempDir</span><span class="o">,</span> <span class="n">priorityConf</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Once the jobs are submitted in the different queues, you can track the MapReduce job progress and monitor the queues through YARN. Using YARNRunner you can get ahold of a job status, and retrieve different informations:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="c1">//print overall job M/R progresses</span>
</span><span class='line'><span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;\nJob &quot;</span> <span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getJobName</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;in queue (&quot;</span> <span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getQueue</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;)&quot;</span> <span class="o">+</span> <span class="s">&quot; progress M/R: &quot;</span> <span class="o">+</span>              <span class="n">jobStatus</span><span class="o">.</span><span class="na">getMapProgress</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;/&quot;</span> <span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getReduceProgress</span><span class="o">());</span>
</span><span class='line'><span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;Tracking URL : &quot;</span> <span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getTrackingUrl</span><span class="o">());</span>
</span><span class='line'><span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;Reserved memory : &quot;</span> <span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getReservedMem</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;, used memory : &quot;</span><span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getUsedMem</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot; and          used slots : &quot;</span><span class="o">+</span> <span class="n">jobStatus</span><span class="o">.</span><span class="na">getNumUsedSlots</span><span class="o">());</span>
</span><span class='line'>  
</span><span class='line'><span class="c1">// list map &amp; reduce tasks statuses and progress     </span>
</span><span class='line'><span class="n">TaskReport</span><span class="o">[]</span> <span class="n">reports</span> <span class="o">=</span> <span class="n">yarnRunner</span><span class="o">.</span><span class="na">getTaskReports</span><span class="o">(</span><span class="n">jobID</span><span class="o">,</span> <span class="n">TaskType</span><span class="o">.</span><span class="na">MAP</span><span class="o">);</span>
</span><span class='line'><span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">reports</span><span class="o">.</span><span class="na">length</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
</span><span class='line'><span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;MAP: Status &quot;</span> <span class="o">+</span> <span class="n">reports</span><span class="o">[</span><span class="n">i</span><span class="o">].</span><span class="na">getCurrentStatus</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot; with task ID &quot;</span> <span class="o">+</span> <span class="n">reports</span><span class="o">[</span><span class="n">i</span><span class="o">].</span><span class="na">getTaskID</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot;, and                 progress &quot;</span> <span class="o">+</span> <span class="n">reports</span><span class="o">[</span><span class="n">i</span><span class="o">].</span><span class="na">getProgress</span><span class="o">());</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Same way the queue capacity can be tracked as well:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">ArrayNode</span> <span class="n">queues</span> <span class="o">=</span> <span class="o">(</span><span class="n">ArrayNode</span><span class="o">)</span> <span class="n">jsonNode</span><span class="o">.</span><span class="na">path</span><span class="o">(</span><span class="s">&quot;scheduler&quot;</span><span class="o">).</span><span class="na">path</span><span class="o">(</span><span class="s">&quot;schedulerInfo&quot;</span><span class="o">).</span><span class="na">path</span><span class="o">(</span><span class="s">&quot;queues&quot;</span><span class="o">).</span><span class="na">get</span><span class="o">(</span><span class="s">&quot;queue&quot;</span><span class="o">);</span>
</span><span class='line'><span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">queues</span><span class="o">.</span><span class="na">size</span><span class="o">();</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
</span><span class='line'><span class="n">JsonNode</span> <span class="n">queueNode</span> <span class="o">=</span> <span class="n">queues</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">i</span><span class="o">);</span>                       
</span><span class='line'><span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;queueName / usedCapacity / absoluteUsedCap / absoluteCapacity / absMaxCapacity: &quot;</span> <span class="o">+</span>
</span><span class='line'>                  <span class="n">queueNode</span><span class="o">.</span><span class="na">findValue</span><span class="o">(</span><span class="s">&quot;queueName&quot;</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; / &quot;</span> <span class="o">+</span>
</span><span class='line'>                  <span class="n">queueNode</span><span class="o">.</span><span class="na">findValue</span><span class="o">(</span><span class="s">&quot;usedCapacity&quot;</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; / &quot;</span> <span class="o">+</span>
</span><span class='line'>                  <span class="n">queueNode</span><span class="o">.</span><span class="na">findValue</span><span class="o">(</span><span class="s">&quot;absoluteUsedCapacity&quot;</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; / &quot;</span> <span class="o">+</span>
</span><span class='line'>                  <span class="n">queueNode</span><span class="o">.</span><span class="na">findValue</span><span class="o">(</span><span class="s">&quot;absoluteCapacity&quot;</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; / &quot;</span> <span class="o">+</span>
</span><span class='line'>                  <span class="n">queueNode</span><span class="o">.</span><span class="na">findValue</span><span class="o">(</span><span class="s">&quot;absoluteMaxCapacity&quot;</span><span class="o">));</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can run the example with <code>hadoop jar yarn-queue-tests-1.0-SNAPSHOT.jar com.sequenceiq.yarntest.client.JobClient</code>, and play with different queue setups. Once you have changed the queue setup you can refresh the settings with <code>yarn rmadmin -refreshQueues</code>.</p>

<p>You can check the progress through the <a href="https://gist.github.com/matyix/9528220">logs</a>.The <a href="http://sandbox.hortonworks.com:8088/cluster/scheduler">cluster statistics</a> and <a href="http://sandbox.hortonworks.com:8088/cluster/apps">application statistics</a> are available as well (we run this example on Hortonworks HDP2 sandbox, but any other Hadoop 2 distribution works &ndash; you can set your own cluster on Amazon EC2 using SequenceIQ&rsquo;s setup scripts from our <a href="https://github.com/sequenceiq/hadoop-cloud-scripts">GitHub</a> page).</p>

<p>As you expect, the jobs submitted into the  <code>highPriority</code> queue are finished earlier than those submitted into the <code>lowPriority</code> one &ndash; though (in case of submitting into the same queue) the MapReduce jobs should take the same time (as they are the same MapReduce job, have the same job profile).</p>

<p>This is a good way to start experimenting multi-tenancy and parallel jobs submission into a shared cluster (beyond the Fair Scheduler). At <a href="http://sequenceiq.com">SequenceIQ</a> we are working on a heuristic YARN scheduler &ndash; where we can adapt to increased work loads, submit jobs into queues based on different customer QoS profiles, and increase or downsize our cloud based cluster based on load and capacity.</p>

<p>For more details and updates please follow us through our <a href="http://www.linkedin.com/company/sequenceiq/">LinkedIn</a> page.
You can access the code from our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-queue-tests">GitHub</a> repository.</p>

<p>Thanks,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data cleaning with MapReduce and Morphlines]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines/"/>
    <updated>2014-03-11T09:21:07+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines</id>
    <content type="html"><![CDATA[<p>In one of our <a href="http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality/">previous</a> posts we showed how easy is to extend the Kite Morphlines framework with your custom commands. In this post we are going to use it to remove columns from a dataset to demonstrate how it can be used and embeded in MapReduce jobs.
Download the MovieLens + IMDb/Rotten Tomatoes dataset from <a href="http://grouplens.org/datasets/hetrec-2011/">Grouplens</a>, extract it, and it should contain a file called user_ratedmovies.dat.
It is a simple tsv file &ndash; we are going to use the same column names as it shows in the first line (header)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>userID   movieID rating  date_day  date_month  date_year date_hour  date_minute  date_second
</span><span class='line'>75        3       1       29       10           2006      23          17          16
</span><span class='line'>75        32      4.5     29       10           2006      23          23          44
</span><span class='line'>75        110     4       29       10           2006      23          30          8
</span><span class='line'>75        160     2       29       10           2006      23          16          52
</span><span class='line'>75        163     4       29       10           2006      23          29          30
</span><span class='line'>75        165     4.5     29       10           2006      23          25          15
</span><span class='line'>75        173     3.5     29       10           2006      23          17          37</span></code></pre></td></tr></table></div></figure>


<p>Let’s just pretend that we don’t need all the data from the file and remove the last 3 columns (date_hour, date_minute, date_second). We can achieve this task with the following 2 commands:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  readCSV {
</span><span class='line'>          separator : "\t"
</span><span class='line'>          columns : [userID,movieID,rating,date_day,date_month,date_year,date_hour,date_minute,date_second]
</span><span class='line'>          ignoreFirstLine : false
</span><span class='line'>          trim : true
</span><span class='line'>          charset : UTF-8
</span><span class='line'>  }
</span><span class='line'>}  
</span><span class='line'>
</span><span class='line'>{
</span><span class='line'>  java {
</span><span class='line'>        imports : "import java.util.*;"
</span><span class='line'>        code: """
</span><span class='line'>          record.removeAll("date_hour");
</span><span class='line'>          record.removeAll("date_minute");
</span><span class='line'>          record.removeAll("date_second");
</span><span class='line'>        return child.process(record);
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<!-- more -->


<p>Now lets create our mapper only job to process the data. What we need to do is build the Morphlines command chain by parsing the configuration file as shown</p>

<figure class='code'><figcaption><span>protected void setup(Context context)</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">File</span> <span class="n">morphLineFile</span> <span class="o">=</span> <span class="k">new</span> <span class="n">File</span><span class="o">(</span><span class="n">context</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">MORPHLINE_FILE</span><span class="o">));</span>
</span><span class='line'><span class="n">String</span> <span class="n">morphLineId</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getConfiguration</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">MORPHLINE_ID</span><span class="o">);</span>
</span><span class='line'><span class="n">RecordEmitter</span> <span class="n">recordEmitter</span> <span class="o">=</span> <span class="k">new</span> <span class="n">RecordEmitter</span><span class="o">(</span><span class="n">context</span><span class="o">);</span>
</span><span class='line'><span class="n">MorphlineContext</span> <span class="n">morphlineContext</span> <span class="o">=</span> <span class="k">new</span> <span class="n">MorphlineContext</span><span class="o">.</span><span class="na">Builder</span><span class="o">().</span><span class="na">build</span><span class="o">();</span>
</span><span class='line'><span class="n">Command</span> <span class="n">morphline</span> <span class="o">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="na">kitesdk</span><span class="o">.</span><span class="na">morphline</span><span class="o">.</span><span class="na">base</span><span class="o">.</span><span class="na">Compiler</span><span class="o">().</span><span class="na">compile</span><span class="o">(</span><span class="n">morphLineFile</span><span class="o">,</span> <span class="n">morphLineId</span><span class="o">,</span> <span class="n">morphlineContext</span><span class="o">,</span> <span class="n">recordEmitter</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>and pass the lines through it.</p>

<figure class='code'><figcaption><span>protected void map(Object key, Text value, Context context)</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">Record</span> <span class="n">record</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Record</span><span class="o">()</span>
</span><span class='line'><span class="n">record</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="n">Fields</span><span class="o">.</span><span class="na">ATTACHMENT_BODY</span><span class="o">,</span> <span class="k">new</span> <span class="n">ByteArrayInputStream</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="na">toString</span><span class="o">().</span><span class="na">getBytes</span><span class="o">()));</span>
</span><span class='line'><span class="k">if</span> <span class="o">(!</span><span class="n">morphline</span><span class="o">.</span><span class="na">process</span><span class="o">(</span><span class="n">record</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">LOGGER</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;Morphline failed to process record: {}&quot;</span><span class="o">,</span> <span class="n">record</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="n">record</span><span class="o">.</span><span class="na">removeAll</span><span class="o">(</span><span class="n">Fields</span><span class="o">.</span><span class="na">ATTACHMENT_BODY</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Notice that the compile method takes an important parameter called finalChild which is in our example the <code>RecordEmitter</code>.
The returned command will feed records into finalChild which means if this parameter is not provided a DropRecord command will
be assigned automatically. In Apache Flume there is a Collector command to avoid loosing any transformed record.
The only thing left is to outbox the processed record and write the results to HDFS. The RecordEmitter will serve this purpose:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'><span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">process</span><span class="o">(</span><span class="n">Record</span> <span class="n">record</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">line</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">record</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>
</span><span class='line'>  <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">line</span><span class="o">,</span> <span class="kc">null</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">LOGGER</span><span class="o">.</span><span class="na">warn</span><span class="o">(</span><span class="s">&quot;Cannot write record to context&quot;</span><span class="o">,</span> <span class="n">e</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="k">return</span> <span class="kc">true</span><span class="o">;</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>By default the readCSV command extracts the ATTACHMENT_BODY into headers with id provided in the columns field so the
transformed data will look like this (3 columns were dropped):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">{</span><span class="n">date_day</span><span class="o">=[</span><span class="mi">10</span><span class="o">],</span> <span class="n">date_month</span><span class="o">=[</span><span class="mi">10</span><span class="o">],</span> <span class="n">date_year</span><span class="o">=[</span><span class="mi">2008</span><span class="o">],</span> <span class="n">movieID</span><span class="o">=[</span><span class="mi">62049</span><span class="o">],</span> <span class="n">rating</span><span class="o">=[</span><span class="mf">4.5</span><span class="o">],</span> <span class="n">userID</span><span class="o">=[</span><span class="mi">71534</span><span class="o">]}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The source code is available in our samples repository on <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub</a>.
It is just a simple example but you can go further and download a much bigger dataset with 10 millions of lines and process it with multiple nodes to see how it scales.</p>

<p>Enjoy,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS and java.nio.channels]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/07/read-from-hdfs/"/>
    <updated>2014-03-07T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/07/read-from-hdfs</id>
    <content type="html"><![CDATA[<p>Many times there is a need to access files or interact with HDFS from Java applications or libraries. Hadoop has built in many tools in order to work or interact with HDFS &ndash; however in case you&rsquo;d like to read into a content of a file remotely (e.g. retrieve the headers of a CSV/TSV file) random exceptions can occurs. One of these remote exceptions coming from the HDFS NameNode is a <em>java.io.IOException: File /user/abc/xyz/ could only be replicated to 0 nodes, instead of 1.</em></p>

<p>Such an exception can be reproduced by the following code snippet:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>java BufferedInputStream bufferedInputStream
</span><span class='line'>
</span><span class='line'>/**
</span><span class='line'> * For the sake of readability, try/cacth/finally blocks are removed 
</span><span class='line'> * Don't Say We Didn't Warn You
</span><span class='line'> */
</span><span class='line'>
</span><span class='line'>FileSystem fs = FileSystem.get(configuration);
</span><span class='line'>          Path filePath = getFilePath(dataPath);
</span><span class='line'>
</span><span class='line'>BufferedInputStream bufferedInputStream = new BufferedInputStream(fs.open(filePath));
</span><span class='line'>  listReader = new CsvListReader(new BufferedReader(new InputStreamReader(bufferedInputStream)),
</span><span class='line'>                      CsvPreference.STANDARD_PREFERENCE);
</span><span class='line'>                     </span></code></pre></td></tr></table></div></figure>


<p>The exception looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ERROR SimpleFeatureSelector:67 - Exception {}
</span><span class='line'>java.lang.IllegalStateException: Must not use direct buffers with InputStream API
</span><span class='line'>  at com.google.common.base.Preconditions.checkState(Preconditions.java:176)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
</span><span class='line'>  at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
</span><span class='line'>  at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:170)</span></code></pre></td></tr></table></div></figure>


<p><em>Note: actually all HDFS operations fail in case of the underlying input stream does not have a readable channel (check the java.nio.channels package. RemoteBlockReader2 needs channel based inputstreams to deal with direct buffers.</em></p>

<!-- more -->


<p>Digging into details and checking the Hadoop 2.2 source code we find the followings:</p>

<p>Through the<code>org.apache.hadoop.hdfs.BlockReaderFactory</code> you can get access to a BlockReader implementation like <code>org.apache.hadoop.hdfs.RemoteBlockReader2</code>, which it is responsible for reading a single block from a single datanode.</p>

<p>The blockreader is retrieved in the following way:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@SuppressWarnings</span><span class="o">(</span><span class="s">&quot;deprecation&quot;</span><span class="o">)</span>
</span><span class='line'><span class="kd">public</span> <span class="kd">static</span> <span class="n">BlockReader</span> <span class="nf">newBlockReader</span><span class="o">(</span>
</span><span class='line'>                                     <span class="n">Conf</span> <span class="n">conf</span><span class="o">,</span>
</span><span class='line'>                               <span class="n">Socket</span> <span class="n">sock</span><span class="o">,</span> <span class="n">String</span> <span class="n">file</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">ExtendedBlock</span> <span class="n">block</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">Token</span><span class="o">&lt;</span><span class="n">BlockTokenIdentifier</span><span class="o">&gt;</span> <span class="n">blockToken</span><span class="o">,</span>
</span><span class='line'>                                     <span class="kt">long</span> <span class="n">startOffset</span><span class="o">,</span> <span class="kt">long</span> <span class="n">len</span><span class="o">,</span>
</span><span class='line'>                                     <span class="kt">int</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="kt">boolean</span> <span class="n">verifyChecksum</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">String</span> <span class="n">clientName</span><span class="o">)</span>
</span><span class='line'>                                     <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">conf</span><span class="o">.</span><span class="na">useLegacyBlockReader</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">RemoteBlockReader</span><span class="o">.</span><span class="na">newBlockReader</span><span class="o">(</span>
</span><span class='line'>          <span class="n">sock</span><span class="o">,</span> <span class="n">file</span><span class="o">,</span> <span class="n">block</span><span class="o">,</span> <span class="n">blockToken</span><span class="o">,</span> <span class="n">startOffset</span><span class="o">,</span> <span class="n">len</span><span class="o">,</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="n">verifyChecksum</span><span class="o">,</span> <span class="n">clientName</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">RemoteBlockReader2</span><span class="o">.</span><span class="na">newBlockReader</span><span class="o">(</span>
</span><span class='line'>          <span class="n">sock</span><span class="o">,</span> <span class="n">file</span><span class="o">,</span> <span class="n">block</span><span class="o">,</span> <span class="n">blockToken</span><span class="o">,</span> <span class="n">startOffset</span><span class="o">,</span> <span class="n">len</span><span class="o">,</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="n">verifyChecksum</span><span class="o">,</span> <span class="n">clientName</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>In order to avoid the exception and use the right version of the block reader, the followin property <code>conf.useLegacyBlockReader</code> should be TRUE.</p>

<p>Long story short, the configuration set of a job should be set to: <code>configuration.set("dfs.client.use.legacy.blockreader", "true")</code>.</p>

<p>Unluckily in all cases when interacting with HDFS, and the underlying input stream does not have a readable channel, you can&rsquo;t use the <em>RemoteBlockReader2</em> implementation, but you have to fall back to the old legacy <em>RemoteBlockReader</em>.</p>

<p>Hope this helps,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Accessing HDP2 sandbox from the host]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/05/access-hdp2-sandbox/"/>
    <updated>2014-03-05T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/05/access-hdp2-sandbox</id>
    <content type="html"><![CDATA[<p>During development of a Hadoop project people have many options of where and how to run Hadoop. We at SequenceIQ use different environments as well (cloud based, VM or host) &ndash; and different versions/vendor distributions. A very popular distribution among developers is the Hortonworks Sandbox &ndash; which contains the latest releases across Hadoop (2.2.0) and the key related projects into a single integrated and tested platform.
While using the sandbox gets you going running a single node Hadoop (pseudo distributed) in less than 5 minutes, many developers find inconvenient to &lsquo;live&rsquo; and work inside the VM when deploying, debugging or submitting jobs into a Hadoop cluster.</p>

<p>There is a well documented VM host file configuration on the <a href="http://docs.hortonworks.com/">Hortonworks site</a> describing how to start interacting with the VM sandbox from outside (e.g host machine), but quite soon this will turn into a port-forwarding saga (those who know how many ports does Hadoop and the ecosystem use will know what we mean). An easier and more elegant way is to use a SOCKS5 proxy (which comes with SSL by default).
Check this short goal/problem/resolution and code example snippet if you&rsquo;d like to interact with the Hortonworks Sandbox from your host (outside the VM).</p>

<h2>Goal</h2>

<ul>
<li>accessing the pseudo distributed hadoop cluster from the  host</li>
<li>reading / writing to the  HDFS</li>
<li>submitting  M/R jobs to the RM</li>
</ul>


<h2>Problem(s)</h2>

<ul>
<li>it&rsquo;s hard to reach resources inside the sandbox (e.g. interact with HDFS, or the DataNode)</li>
<li>lots of ports need to be portforwarded</li>
<li>entries to be added to the hosts file of the  host machine</li>
<li>circumstantial configuration of clients  accessing the sandbox</li>
</ul>


<h2>Resolution</h2>

<ul>
<li>use an SSL socks proxy</li>
</ul>


<h2>Example</h2>

<ul>
<li>check the following sample from our <em><a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/hdp-sandbox-access">GitHub page</a></em></li>
</ul>


<p>Start the SOCKS proxy</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>ssh root@127.0.0.1 -p 2222 -D 1099
</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<p>Once the proxy is up and running, edit the core-site.xml</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>      <span class="nt">&lt;name&gt;</span>hadoop.socks.server<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>      <span class="nt">&lt;value&gt;</span>localhost:1099<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>      <span class="nt">&lt;name&gt;</span>hadoop.rpc.socket.factory.class.default<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>      <span class="nt">&lt;value&gt;</span>org.apache.hadoop.net.SocksSocketFactory<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now you can run the test client</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>     
</span><span class='line'><span class="c"># You can use Maven</span>
</span><span class='line'>mvn <span class="nb">exec</span>:java -Dexec.mainClass<span class="o">=</span><span class="s2">&quot;com.sequenceiq.samples.SandboxTester&quot;</span> -Dexec.args<span class="o">=</span><span class="s2">&quot;hdfs sandbox 8020&quot;</span> -Dhadoop.home.dir<span class="o">=</span>/tmp
</span><span class='line'>      
</span><span class='line'><span class="c"># or run from the JAR file</span>
</span><span class='line'>      
</span><span class='line'>java -jar sandbox-playground-1.0.jar hdfs sandbox 8020
</span></code></pre></td></tr></table></div></figure>


<p>As you see it&rsquo;s pretty easy and convenient to use the Hortonworks sandbox as a pre-configured development environment.</p>

<p>In case you&rsquo;d like to use (as we do most of the time) a Hadoop cluster in the cloud (Amazon EC2), check our previous blog post <a href="http://blog.sequenceiq.com/blog/2014/02/07/hdp2-on-amazon/">HDP2 on Amazon</a>.
In case you ever wondered whether it&rsquo;s possible to use Hadoop with Docker please follow this blog.</p>

<p>Hope this helps,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ETL - producing better quality data]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality/"/>
    <updated>2014-02-28T08:12:44+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/02/28/etl-and-data-quality</id>
    <content type="html"><![CDATA[<p>On my way to work this morning I read an interesting article about the quality of data being produced by different systems and applications. While the article was emphasizing that the quality of the data should not be an IT problem (but management), our believe is that at the high volume, velocity and variety (the &ldquo;3Vs&rdquo; of big data) the data is produced today, the process of producing data is a shared responsibility between management and the IT department.</p>

<p>Since the emerging of Hadoop, the TCO of storing large amounts of data in HDFS is lower than ever before &ndash; and now it makes sense to store all the data an enterprise produces in order to find patterns, correlations and break the data silos &ndash; something which was very specific for different departments within an organization. Storing such an amount of data (structured, unstructured, logs, clickstream, etc) inevitable produces a &lsquo;bad&rsquo; data quality &ndash; but this depends on your point of view. For us data is just data &ndash; we don&rsquo;t want to qualify it &ndash; and has it&rsquo;s own intrinsic value, but the quality of it depends on the ETL process. When someone engages with our API and the xTract Spacetime platform, among the first step is the configuration of data sources, and the attached ETL processes. We offer an extremely sophisticated ETL process and the ability to &lsquo;clean&rsquo; the data (batch or streaming) while arrives into xTract Spacetime, but we always suggest our customers to keep the raw data as well.</p>

<p>During the architecture of the xTract Spacetime platform we have tried and PoCd different ETL frameworks and implementations &ndash; and we choose <a href="https://github.com/kite-sdk/kite/tree/master/kite-morphlines">Kite Morphlines</a> being at the core of our ETL process. Morphlines is an open source framework that reduces the time and skills necessary to build and change Hadoop ETL stream processing applications that extract, transform and load data into Apache Solr, HBase, HDFS, Enterprise Data Warehouses, or Analytic Online Dashboards.</p>

<!-- more -->


<p>Since runs on Hadoop, scalability is not a problem &ndash; we have seen enterprises producing 50 terabytes data per day and missing the 24 hour ETL window, by not being able to scale horizontally their ETL processes. Morphlines is built on top of the Kite framework (a great framework for making easier to build systems on top of the Hadoop), and it&rsquo;s extremely easy to extend. We would like to show and give you examples about how to use and create a Morphlines Command to implement your custom transformation (if the many existing ones does not fit your requirement).</p>

<p>The implementation of a Command starts with implementing a CommandBuilder</p>

<p>Actually a new morphline command implementation is not that hard. You have to implement a builder class (see below), define the name of the command and  extend the AbstractCommand base class. That simple.</p>

<figure class='code'><figcaption><span>ToLowerCaseBuilder implements CommandBuilder</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'>  <span class="kd">public</span> <span class="n">Collection</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="nf">getNames</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">Collections</span><span class="o">.</span><span class="na">singletonList</span><span class="o">(</span><span class="s">&quot;toLowerCase&quot;</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span>toLowerCase Morphlines command</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@Override</span>
</span><span class='line'><span class="kd">protected</span> <span class="kt">boolean</span> <span class="nf">doProcess</span><span class="o">(</span><span class="n">Record</span> <span class="n">record</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">ListIterator</span> <span class="n">iter</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">fieldName</span><span class="o">).</span><span class="na">listIterator</span><span class="o">();</span>
</span><span class='line'>  <span class="k">while</span> <span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="na">hasNext</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">iter</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">transformFieldValue</span><span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="na">next</span><span class="o">()));</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>    <span class="k">return</span> <span class="kd">super</span><span class="o">.</span><span class="na">doProcess</span><span class="o">(</span><span class="n">record</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="kd">private</span> <span class="n">Object</span> <span class="nf">transformFieldValue</span><span class="o">(</span><span class="n">Object</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="na">toString</span><span class="o">().</span><span class="na">toLowerCase</span><span class="o">(</span><span class="n">locale</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To configure your new morhline command</p>

<figure class='code'><figcaption><span>toLowerCase config</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">morphlines</span> <span class="o">:</span> <span class="o">[</span>
</span><span class='line'>  <span class="o">{</span>
</span><span class='line'>   <span class="n">id</span> <span class="o">:</span> <span class="n">morphline1</span>
</span><span class='line'>       <span class="n">importCommands</span> <span class="o">:</span> <span class="o">[</span><span class="s">&quot;com.sequenceiq.samples.**&quot;</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>       <span class="n">commands</span> <span class="o">:</span> <span class="o">[</span>
</span><span class='line'>         <span class="o">{</span>
</span><span class='line'>           <span class="n">toLowerCase</span> <span class="o">{</span>
</span><span class='line'>             <span class="n">field</span> <span class="o">:</span> <span class="n">Name</span>
</span><span class='line'>             <span class="n">locale</span> <span class="o">:</span> <span class="n">en_us</span>
</span><span class='line'>           <span class="o">}</span>
</span><span class='line'>         <span class="o">}</span>
</span><span class='line'>       <span class="o">]</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>There is a custom tester tool which waiting a file as an input, a file as a config and an expected output. Among our plans is to build a UI on top of Kite Morphlines as well &ndash; part of our product stack. While we consider the Morhplines configuration, and defining the transformations simple and easy to use, many of our users might prefer a custom UI whee you can define your own ETL process visually.</p>

<p>That’s it. You can find the samples at our <a href="https://github.com/sequenceiq/sequenceiq-samples">GitHub page</a>.
Hope you enjoy it, let us know if you need help or have any questions.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vote for us - 2014 Hadoop Summit San Jose]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/02/26/vote-for-us/"/>
    <updated>2014-02-26T10:17:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/02/26/vote-for-us</id>
    <content type="html"><![CDATA[<p>While we are extremely proud that our abstract came 2nd (out of 107) in the 2014 Hadoop Summit in Amsterdam (see you all there in April 2-3), we will not stop there and our plan is to continue the hard work and we&rsquo;re looking forward to meet you at 2014 Hadoop Summit in San Jose.
We would like to ask for your support by submitting your vote for our session in the largest Hadoop conference in the world.</p>

<p>Please use the following link to vote, or read our abstract below.</p>

<p><a href="http://hadoopsummit.uservoice.com/forums/242807-hadoop-deployment-operations-track/suggestions/5568417-moving-to-hadoop-2-0-yarn-at-sequenceiq">Vote for us</a></p>

<p>Should you have any questions regarding our abstract, the technical solution or implementation detailsm feel free to contact us or
check our <a href="https://github.com/sequenceiq">GitHub</a> page.</p>

<p><strong>Moving to Hadoop 2.0/YARN at SequenceIQ</strong></p>

<p>A showcase of our efforts to bring all our Hadoop based applications under one common cluster management framework &ndash; YARN.
Our deployment consists of MR2, HBase, Mahout and Hive-all running within one single auto-scaling cluster. We have faced many challenges such as load imbalances, SLA misses, cluster scheduling and VM container deployments &ndash; and would like to share our struggle and solution with the community.
As a startup, cost savings is important for us &ndash; switching to Hadoop 2.0 helped us save significant costs through better utilization of our hardware and cloud VMs. Our decision and investment of moving to YARN has paid off &ndash; and opened up new business and technical opportunities.</p>
]]></content>
  </entry>
  
</feed>
