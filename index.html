
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SequenceIQ Blog</title>
  <meta name="author" content="SequenceIQ">

   
  <meta name="description" content="">
  
  <meta name="keywords" content="">

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog.sequenceiq.com/">
  <link href="/favicon.png" rel="icon">
  <link href='http://fonts.googleapis.com/css?family=Quicksand:300,400' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,300' rel='stylesheet' type='text/css'>
  <link href="/stylesheets/bootstrap.css" rel='stylesheet' type='text/css'>
  <link href="/stylesheets/bootstrap-theme.css"rel='stylesheet' type='text/css'>
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">

  <link href="/blog/atom.xml" rel="alternate" title="SequenceIQ Blog" type="application/atom+xml">
  <script src="/js/jquery.js"></script>
  <script src="/js/bootstrap-collapse.js"></script>
  <script src="/js/modernizr-2.0.js"></script>
  <script src="/js/octopress.js" type="text/javascript"></script>
  <script src="/js/application.js"></script>
  <script src="/js/bootstrap.js"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >

  
<div class="jumbotron seq-jumborton">
  <div class="container">
      <a href="/">
        <img src="/images/logo.png" >
      </a>
    <h3 class="tagline">
      
        Our view on big data
      
    </h3>
  </div>
  <div class="container social-jumbotron-container">
      <div class="row">

    
    <div class="col-md-1"><a class="social-link" href="http://github.com/sequenceiq" title="Github Profile"><i class="icon-github-sign social-navbar"></i></a></div>
    
    
    
    <div class="col-md-1"><a class="social-link" href="http://linkedin.com/company/sequenceiq" title="Linkedin Profile"><i class="icon-linkedin-sign social-navbar"></i></a></div>
    
    
    <div class="col-md-1"><a class="social-link" href="http://twitter.com/sequenceiq" title="Twitter Profile"><i class="icon-twitter-sign social-navbar"></i></a></div>
    
    
    
    <div class="col-md-1"><a class="social-link" href="http://facebook.com/sequenceiq" title="Facebook Profile"><i class="icon-facebook-sign social-navbar"></i></a></div>
    
    

    
     </div>
  </div>
</div>


  <div class="container" id="main">
      <div class="row-fluid">
        <div id="content">
          <div class="blog-index">
  
  
  
    <article>
      

  <div class="row-fluid">
    <div class="span2 post-meta">
	  <h5 class="date-time">








  


<i class="icon-calendar-empty"></i> <time datetime="2014-03-07T08:12:44+00:00" pubdate data-updated="true">Mar 7<span>th</span>, 2014</time></h5>
          <div class="row-fluid">
          
          <a href="http://blog.sequenceiq.com//blog/2014/03/07/read-from-hdfs/#disqus_thread">Comments </a> <span class="fui-bubble-16"></span>
          
          </div>
          
          <div class="row-fluid">
          
          <a href="/blog/categories/hdfs/"><span class="badge">HDFS</span></a>
          
          <a href="/blog/categories/hdp2/"><span class="badge">HDP2</span></a>
          
          <a href="/blog/categories/namenode/"><span class="badge">NameNode</span></a>
          
          <a href="/blog/categories/remoteblockreader2/"><span class="badge">RemoteBlockReader2</span></a>
          
          </div>
          
    </div>
    <div class="span10 post-container">
      <h1 class="link">
          <a href="/blog/2014/03/07/read-from-hdfs/">HDFS and java.nio.channels</a>
          <span class="badge name-badge">Janos Matyas</span>
      </h1>
      <p>Many times there is a need to access files or interact with HDFS from Java applications or libraries. Hadoop has built in many tools in order to work or interact with HDFS &ndash; however in case you&rsquo;d like to read into a content of a file remotely (e.g. retrieve the headers of a CSV/TSV file) random exceptions can occurs. One of these remote exceptions coming from the HDFS NameNode is a <em>java.io.IOException: File /user/abc/xyz/ could only be replicated to 0 nodes, instead of 1.</em></p>

<p>Such an exception can be reproduced by the following code snippet:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>java BufferedInputStream bufferedInputStream
</span><span class='line'>
</span><span class='line'>/**
</span><span class='line'> * For the sake of readability, try/cacth/finally blocks are removed 
</span><span class='line'> * Don't Say We Didn't Warn You
</span><span class='line'> */
</span><span class='line'>
</span><span class='line'>FileSystem fs = FileSystem.get(configuration);
</span><span class='line'>          Path filePath = getFilePath(dataPath);
</span><span class='line'>
</span><span class='line'>BufferedInputStream bufferedInputStream = new BufferedInputStream(fs.open(filePath));
</span><span class='line'>  listReader = new CsvListReader(new BufferedReader(new InputStreamReader(bufferedInputStream)),
</span><span class='line'>                      CsvPreference.STANDARD_PREFERENCE);
</span><span class='line'>                     </span></code></pre></td></tr></table></div></figure>




<div><script src='https://gist.github.com/9386987.js'></script>
<noscript><pre><code>ERROR SimpleFeatureSelector:67 - Exception {}
java.lang.IllegalStateException: Must not use direct buffers with InputStream API
    at com.google.common.base.Preconditions.checkState(Preconditions.java:176)
    at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
    at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
    at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
    at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:170)
    at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:135)
    at org.apache.hadoop.hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:642)
    at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:698)
    at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:752)
    at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:793)
    at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
    at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
    at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:264)
    at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:306)
    at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:158)
    at java.io.InputStreamReader.read(InputStreamReader.java:167)
    at java.io.BufferedReader.read1(BufferedReader.java:185)
    at java.io.BufferedReader.read(BufferedReader.java:261)
    at java.io.BufferedReader.fill(BufferedReader.java:136)
    at java.io.BufferedReader.readLine(BufferedReader.java:299)
    at java.io.LineNumberReader.readLine(LineNumberReader.java:182)
    at org.supercsv.io.AbstractTokenizer.readLine(AbstractTokenizer.java:82)
    at org.supercsv.io.Tokenizer.readColumns(Tokenizer.java:95)
    at org.supercsv.io.AbstractCsvReader.readRow(AbstractCsvReader.java:179)
    at org.supercsv.io.CsvListReader.read(CsvListReader.java:69)
    at com.sequenceiq.service.clustering.SimpleFeatureSelector.getAllFeatures(SimpleFeatureSelector.java:57)
    at com.sequenceiq.service.clustering.SimpleFeatureSelector$1.run(SimpleFeatureSelector.java:114)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:335)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1471)
    at com.sequenceiq.service.clustering.SimpleFeatureSelector.main(SimpleFeatureSelector.java:108)
2014-03-06 10:15:03 DEBUG Client:1156 - Stopping client
2014-03-06 10:15:03 DEBUG Client:1107 - IPC Client (343311896) </code></pre></noscript></div>


<p><em>Note: actually all HDFS operations fail in case of the underlying input stream does not have a readable channel (check the java.nio.channels package. RemoteBlockReader2 needs channel based inputstreams to deal with direct buffers.</em></p>

<p>Digging into details and checking the Hadoop 2.2 source code we find the followings:</p>

<p>Through the<code>org.apache.hadoop.hdfs.BlockReaderFactory</code> you can get access to a BlockReader implementation like <code>org.apache.hadoop.hdfs.RemoteBlockReader2</code>, which it is responsible for reading a single block from a single datanode.</p>

<p>The blockreader is retrieved in the following way:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="nd">@SuppressWarnings</span><span class="o">(</span><span class="s">&quot;deprecation&quot;</span><span class="o">)</span>
</span><span class='line'><span class="kd">public</span> <span class="kd">static</span> <span class="n">BlockReader</span> <span class="nf">newBlockReader</span><span class="o">(</span>
</span><span class='line'>                                     <span class="n">Conf</span> <span class="n">conf</span><span class="o">,</span>
</span><span class='line'>                               <span class="n">Socket</span> <span class="n">sock</span><span class="o">,</span> <span class="n">String</span> <span class="n">file</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">ExtendedBlock</span> <span class="n">block</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">Token</span><span class="o">&lt;</span><span class="n">BlockTokenIdentifier</span><span class="o">&gt;</span> <span class="n">blockToken</span><span class="o">,</span>
</span><span class='line'>                                     <span class="kt">long</span> <span class="n">startOffset</span><span class="o">,</span> <span class="kt">long</span> <span class="n">len</span><span class="o">,</span>
</span><span class='line'>                                     <span class="kt">int</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="kt">boolean</span> <span class="n">verifyChecksum</span><span class="o">,</span>
</span><span class='line'>                                     <span class="n">String</span> <span class="n">clientName</span><span class="o">)</span>
</span><span class='line'>                                     <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="n">conf</span><span class="o">.</span><span class="na">useLegacyBlockReader</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">RemoteBlockReader</span><span class="o">.</span><span class="na">newBlockReader</span><span class="o">(</span>
</span><span class='line'>          <span class="n">sock</span><span class="o">,</span> <span class="n">file</span><span class="o">,</span> <span class="n">block</span><span class="o">,</span> <span class="n">blockToken</span><span class="o">,</span> <span class="n">startOffset</span><span class="o">,</span> <span class="n">len</span><span class="o">,</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="n">verifyChecksum</span><span class="o">,</span> <span class="n">clientName</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">RemoteBlockReader2</span><span class="o">.</span><span class="na">newBlockReader</span><span class="o">(</span>
</span><span class='line'>          <span class="n">sock</span><span class="o">,</span> <span class="n">file</span><span class="o">,</span> <span class="n">block</span><span class="o">,</span> <span class="n">blockToken</span><span class="o">,</span> <span class="n">startOffset</span><span class="o">,</span> <span class="n">len</span><span class="o">,</span> <span class="n">bufferSize</span><span class="o">,</span> <span class="n">verifyChecksum</span><span class="o">,</span> <span class="n">clientName</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>In order to avoid the exception and use the right version of the block reader, the followin property <code>conf.useLegacyBlockReader</code> should be TRUE.</p>

<p>Long story short, the configuration set of a job should be set to: <code>configuration.set("dfs.client.use.legacy.blockreader", "true")</code>.</p>

<p>Unluckily in all cases when interacting with HDFS, and the underlying input stream does not have a readable channel, you can&rsquo;t use the <em>RemoteBlockReader2</em> implementation, but you have to fall back to the old legacy <em>RemoteBlockReader</em>.</p>

<p>Hope this helps,
SequenceIQ</p>

      
      
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row-fluid">
    <div class="span2 post-meta">
	  <h5 class="date-time">








  


<i class="icon-calendar-empty"></i> <time datetime="2014-03-05T08:12:44+00:00" pubdate data-updated="true">Mar 5<span>th</span>, 2014</time></h5>
          <div class="row-fluid">
          
          <a href="http://blog.sequenceiq.com//blog/2014/03/05/access-hdp2-sandbox/#disqus_thread">Comments </a> <span class="fui-bubble-16"></span>
          
          </div>
          
          <div class="row-fluid">
          
          <a href="/blog/categories/hdp2/"><span class="badge">HDP2</span></a>
          
          <a href="/blog/categories/hortonworks-sandbox/"><span class="badge">Hortonworks sandbox</span></a>
          
          <a href="/blog/categories/socks-proxy/"><span class="badge">SOCKS proxy</span></a>
          
          <a href="/blog/categories/ssl/"><span class="badge">SSL</span></a>
          
          </div>
          
    </div>
    <div class="span10 post-container">
      <h1 class="link">
          <a href="/blog/2014/03/05/access-hdp2-sandbox/">Accessing HDP2 sandbox from the host</a>
          <span class="badge name-badge">Laszlo Puskas</span>
      </h1>
      <p>During development of a Hadoop project people have many options of where and how to run Hadoop. We at SequenceIQ use different environments as well (cloud based, VM or host) &ndash; and different versions/vendor distributions. A very popular distribution among developers is the Hortonworks Sandbox &ndash; which contains the latest releases across Hadoop (2.2.0) and the key related projects into a single integrated and tested platform.
While using the sandbox gets you going running a single node Hadoop (pseudo distributed) in less than 5 minutes, many developers find inconvenient to &lsquo;live&rsquo; and work inside the VM when deploying, debugging or submitting jobs into a Hadoop cluster.</p>

<p>There is a well documented VM host file configuration on the <a href="http://docs.hortonworks.com/">Hortonworks site</a> describing how to start interacting with the VM sandbox from outside (e.g host machine), but quite soon this will turn into a port-forwarding saga (those who know how many ports does Hadoop and the ecosystem use will know what we mean). An easier and more elegant way is to use a SOCKS5 proxy (which comes with SSL by default).
Check this short goal/problem/resolution and code example snippet if you&rsquo;d like to interact with the Hortonworks Sandbox from your host (outside the VM).</p>

<h2>Goal</h2>

<ul>
<li>accessing the pseudo distributed hadoop cluster from the  host</li>
<li>reading / writing to the  HDFS</li>
<li>submitting  M/R jobs to the RM</li>
</ul>


<h2>Problem(s)</h2>

<ul>
<li>it&rsquo;s hard to reach resources inside the sandbox (e.g. interact with HDFS, or the DataNode)</li>
<li>lots of ports need to be portforwarded</li>
<li>entries to be added to the hosts file of the  host machine</li>
<li>circumstantial configuration of clients  accessing the sandbox</li>
</ul>


<h2>Resolution</h2>

<ul>
<li>use an SSL socks proxy</li>
</ul>


<h2>Example</h2>

<ul>
<li>check the following sample from our <em><a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/hdp-sandbox-access">GitHub page</a></em></li>
</ul>


<p>Start the SOCKS proxy</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>ssh root@127.0.0.1 -p 2222 -D 1099
</span></code></pre></td></tr></table></div></figure>


<p>Once the proxy is up and running, edit the core-site.xml</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>      <span class="nt">&lt;name&gt;</span>hadoop.socks.server<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>      <span class="nt">&lt;value&gt;</span>localhost:1099<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>      <span class="nt">&lt;name&gt;</span>hadoop.rpc.socket.factory.class.default<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>      <span class="nt">&lt;value&gt;</span>org.apache.hadoop.net.SocksSocketFactory<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now you can run the test client</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>     
</span><span class='line'><span class="c"># You can use Maven</span>
</span><span class='line'>mvn <span class="nb">exec</span>:java -Dexec.mainClass<span class="o">=</span><span class="s2">&quot;com.sequenceiq.samples.SandboxTester&quot;</span> -Dexec.args<span class="o">=</span><span class="s2">&quot;hdfs sandbox 8020&quot;</span> -Dhadoop.home.dir<span class="o">=</span>/tmp
</span><span class='line'>      
</span><span class='line'><span class="c"># or run from the JAR file</span>
</span><span class='line'>      
</span><span class='line'>java -jar sandbox-playground-1.0.jar hdfs sandbox 8020
</span></code></pre></td></tr></table></div></figure>


<p>As you see it&rsquo;s pretty easy and convenient to use the Hortonworks sandbox as a pre-configured development environment.</p>

<p>In case you&rsquo;d like to use (as we do most of the time) a Hadoop cluster in the cloud (Amazon EC2), check our previous blog post <a href="http://blog.sequenceiq.com/blog/2014/02/07/hdp2-on-amazon/">HDP2 on Amazon</a>.
In case you ever wondered whether it&rsquo;s possible to use Hadoop with Docker please follow this blog.</p>

<p>Hope this helps,
SequenceIQ</p>

      
      
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row-fluid">
    <div class="span2 post-meta">
	  <h5 class="date-time">








  


<i class="icon-calendar-empty"></i> <time datetime="2014-02-28T08:12:44+00:00" pubdate data-updated="true">Feb 28<span>th</span>, 2014</time></h5>
          <div class="row-fluid">
          
          <a href="http://blog.sequenceiq.com//blog/2014/02/28/etl-and-data-quality/#disqus_thread">Comments </a> <span class="fui-bubble-16"></span>
          
          </div>
          
          <div class="row-fluid">
          
          <a href="/blog/categories/data-cleaning/"><span class="badge">Data cleaning</span></a>
          
          <a href="/blog/categories/etl/"><span class="badge">ETL</span></a>
          
          </div>
          
    </div>
    <div class="span10 post-container">
      <h1 class="link">
          <a href="/blog/2014/02/28/etl-and-data-quality/">ETL - producing better quality data</a>
          <span class="badge name-badge">Richard Doktorics</span>
      </h1>
      <p>On my way to work this morning I read an interesting article about the quality of data being produced by different systems and applications. While the article was emphasizing that the quality of the data should not be an IT problem (but management), our believe is that at the high volume, velocity and variety (the &ldquo;3Vs&rdquo; of big data) the data is produced today, the process of producing data is a shared responsibility between management and the IT department.</p>

<p>Since the emerging of Hadoop, the TCO of storing large amounts of data in HDFS is lower than ever before &ndash; and now it makes sense to store all the data an enterprise produces in order to find patterns, correlations and break the data silos &ndash; something which was very specific for different departments within an organization. Storing such an amount of data (structured, unstructured, logs, clickstream, etc) inevitable produces a &lsquo;bad&rsquo; data quality &ndash; but this depends on your point of view. For us data is just data &ndash; we don&rsquo;t want to qualify it &ndash; and has it&rsquo;s own intrinsic value, but the quality of it depends on the ETL process. When someone engages with our API and the xTract Spacetime platform, among the first step is the configuration of data sources, and the attached ETL processes. We offer an extremely sophisticated ETL process and the ability to &lsquo;clean&rsquo; the data (batch or streaming) while arrives into xTract Spacetime, but we always suggest our customers to keep the raw data as well.</p>

<p>During the architecture of the xTract Spacetime platform we have tried and PoCd different ETL frameworks and implementations &ndash; and we choose <a href="https://github.com/kite-sdk/kite/tree/master/kite-morphlines">Kite Morphlines</a> being at the core of our ETL process. Morphlines is an open source framework that reduces the time and skills necessary to build and change Hadoop ETL stream processing applications that extract, transform and load data into Apache Solr, HBase, HDFS, Enterprise Data Warehouses, or Analytic Online Dashboards.</p>


      
       <a href="/blog/2014/02/28/etl-and-data-quality/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row-fluid">
    <div class="span2 post-meta">
	  <h5 class="date-time">








  


<i class="icon-calendar-empty"></i> <time datetime="2014-02-26T10:17:04+00:00" pubdate data-updated="true">Feb 26<span>th</span>, 2014</time></h5>
          <div class="row-fluid">
          
          <a href="http://blog.sequenceiq.com//blog/2014/02/26/vote-for-us/#disqus_thread">Comments </a> <span class="fui-bubble-16"></span>
          
          </div>
          
          <div class="row-fluid">
          
          <a href="/blog/categories/hadoop-summit/"><span class="badge">Hadoop Summit</span></a>
          
          <a href="/blog/categories/vote/"><span class="badge">Vote</span></a>
          
          </div>
          
    </div>
    <div class="span10 post-container">
      <h1 class="link">
          <a href="/blog/2014/02/26/vote-for-us/">Vote for us - 2014 Hadoop Summit San Jose</a>
          <span class="badge name-badge">Janos Matyas</span>
      </h1>
      <p>While we are extremely proud that our abstract came 2nd (out of 107) in the 2014 Hadoop Summit in Amsterdam (see you all there in April 2-3), we will not stop there and our plan is to continue the hard work and we&rsquo;re looking forward to meet you at 2014 Hadoop Summit in San Jose.
We would like to ask for your support by submitting your vote for our session in the largest Hadoop conference in the world.</p>

<p>Please use the following link to vote, or read our abstract below.</p>

<p><a href="http://hadoopsummit.uservoice.com/forums/242807-hadoop-deployment-operations-track/suggestions/5568417-moving-to-hadoop-2-0-yarn-at-sequenceiq">Vote for us</a></p>

<p>Should you have any questions regarding our abstract, the technical solution or implementation detailsm feel free to contact us or
check our <a href="https://github.com/sequenceiq">GitHub</a> page.</p>

<p><strong>Moving to Hadoop 2.0/YARN at SequenceIQ</strong></p>

<p>A showcase of our efforts to bring all our Hadoop based applications under one common cluster management framework &ndash; YARN.
Our deployment consists of MR2, HBase, Mahout and Hive-all running within one single auto-scaling cluster. We have faced many challenges such as load imbalances, SLA misses, cluster scheduling and VM container deployments &ndash; and would like to share our struggle and solution with the community.
As a startup, cost savings is important for us &ndash; switching to Hadoop 2.0 helped us save significant costs through better utilization of our hardware and cloud VMs. Our decision and investment of moving to YARN has paid off &ndash; and opened up new business and technical opportunities.</p>

      
      
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row-fluid">
    <div class="span2 post-meta">
	  <h5 class="date-time">








  


<i class="icon-calendar-empty"></i> <time datetime="2014-02-22T14:45:48+00:00" pubdate data-updated="true">Feb 22<span>nd</span>, 2014</time></h5>
          <div class="row-fluid">
          
          <a href="http://blog.sequenceiq.com//blog/2014/02/22/custom-flume-source/#disqus_thread">Comments </a> <span class="fui-bubble-16"></span>
          
          </div>
          
          <div class="row-fluid">
          
          <a href="/blog/categories/apache-flume/"><span class="badge">Apache Flume</span></a>
          
          <a href="/blog/categories/custom-source/"><span class="badge">Custom source</span></a>
          
          </div>
          
    </div>
    <div class="span10 post-container">
      <h1 class="link">
          <a href="/blog/2014/02/22/custom-flume-source/">Custom Apache Flume source</a>
          <span class="badge name-badge">Krisztian Horvath</span>
      </h1>
      <p>The process of data analytics starts with collecting the data into a common system, in our case a Hadoop cluster. Flume is an Apache project aiming to help us solve this problem in a very efficient and elegant way.</p>

<p>In Flume terminology a source is responsible to listen and consume events coming from many distributed clients and forwards them to one or more channels. Events can have any arbitrary format, it all depends on what source do we use. Flume provides many sources, but only a few of them is capable to collect data through network.</p>

<p>In this article I will discuss how you can implement your own that meets your demands through creating a websocket source.
There are two types of sources: event driven and pollable. In case of a pollable source, Flume will start a thread to periodically call the following method to check whether there is new data available or not:</p>

<figure class='code'><figcaption><span>PollableSource interface</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="n">Status</span> <span class="nf">process</span><span class="o">()</span> <span class="kd">throws</span> <span class="n">EventDeliveryException</span><span class="o">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>With event driven source you will have to take care yourself of receiving the data from the clients. For our websocket example we will use embedded Jetty 9.1. Extend the AbstractEventDrivenSource class and override the mandatory methods to bootstrap the source. In the doConfigure method you can ask the properties you need from the context. These properties are coming from your agent’s configuration file. More on this later..</p>

<figure class='code'><figcaption><span>protected void doConfigure(Context context)</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">host</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="n">HOST_KEY</span><span class="o">);</span>
</span><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">port</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getInteger</span><span class="o">(</span><span class="n">PORT_KEY</span><span class="o">);</span>
</span><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">path</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getString</span><span class="o">(</span><span class="n">PATH_KEY</span><span class="o">);</span>
</span><span class='line'>    <span class="k">this</span><span class="o">.</span><span class="na">enableSsl</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">getBoolean</span><span class="o">(</span><span class="n">SSL_KEY</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Eventually the doStart will kick off the embedded Jetty as shown:</p>

<figure class='code'><figcaption><span>protected void doStart()</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="k">try</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">JettyWebSocketServer</span> <span class="n">server</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JettyWebSocketServer</span><span class="o">(</span><span class="n">host</span><span class="o">,</span> <span class="n">port</span><span class="o">,</span> <span class="n">path</span><span class="o">,</span> <span class="n">getChannelProcessor</span><span class="o">());</span>
</span><span class='line'>    <span class="n">server</span><span class="o">.</span><span class="na">start</span><span class="o">();</span>
</span><span class='line'><span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">LOGGER</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="s">&quot;Error starting jetty server&quot;</span><span class="o">,</span> <span class="n">e</span><span class="o">);</span>
</span><span class='line'>    <span class="k">throw</span> <span class="k">new</span> <span class="nf">FlumeException</span><span class="o">(</span><span class="n">e</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>



      
       <a href="/blog/2014/02/22/custom-flume-source/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row-fluid">
    <div class="span2 post-meta">
	  <h5 class="date-time">








  


<i class="icon-calendar-empty"></i> <time datetime="2014-02-07T16:17:04+00:00" pubdate data-updated="true">Feb 7<span>th</span>, 2014</time></h5>
          <div class="row-fluid">
          
          <a href="http://blog.sequenceiq.com//blog/2014/02/07/hdp2-on-amazon/#disqus_thread">Comments </a> <span class="fui-bubble-16"></span>
          
          </div>
          
          <div class="row-fluid">
          
          <a href="/blog/categories/ec2/"><span class="badge">EC2</span></a>
          
          <a href="/blog/categories/hdp2/"><span class="badge">HDP2</span></a>
          
          <a href="/blog/categories/hortonworks/"><span class="badge">Hortonworks</span></a>
          
          </div>
          
    </div>
    <div class="span10 post-container">
      <h1 class="link">
          <a href="/blog/2014/02/07/hdp2-on-amazon/">Set up HDP2 on Amazon EC2</a>
          <span class="badge name-badge">Janos Matyas</span>
      </h1>
      <p>During the last years we have seen many blog entries and articles about how to set up Hadoop on Amazon EC2. All these tutorials and articles had one thing in common &ndash; you had to go through a large number of manual (and painful) steps, read screenshots and redo the whole thing all over again, in case you needed a new cluster.</p>

<p>Since we use Amazon EC2 quite a lot, and Hadoop as well (Hortonworks distribution) we have gone through these steps many times &ndash; and have scripted the whole process from the first steps up to launching an N node Hadoop/HDP2 cluster in less then five minutes.</p>

<p>Moreover, the cluster is a &lsquo;production ready&rsquo; setup from infrastructural point of view &ndash; it is provisioned in a logically isolated section of the cloud (Virtual Private Cloud), with his own IP address range, creation of subnets, and configuration of route tables and network gateways.</p>

<p>Once the instances are provisoned, the HDP2 setup is done by Apache Ambari &ndash; for more advanced users we will provide the setup thorugh Ambari&rsquo;s RESTful API &ndash; watch this space or our GitHub page.</p>

<p>All the EC2 instances are tagged with the user name &ndash; thus you can create different clusters for different employees, all under the same AWS account (with IAM).</p>

<p>We believe that this is the right way to provision Hadoop in the cloud &ndash; during development and testing we had to provision Hadoop clusters of different sizes, and going through these steps manually would take lots of time.
This way we are able to provision clusters in the cloud in the matter of minutes &ndash; independently of the size.</p>

<p>The script is available at: <a href="https://github.com/sequenceiq/hadoop-cloud-scripts">https://github.com/sequenceiq/hadoop-cloud-scripts</a></p>

<p>Enjoy,
SequenceIQ</p>

      
      
    </div>
  </div>



    </article>
    
  
  <div class="pagination">
    

    
  </div>
</div>


        </div>
      </div>
      <div class="row-fluid">
        <footer class="footer-page" role="contentinfo">
          <p>
  Copyright &copy; 2014 - SequenceIQ -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>


        </footer>
      </div>
  </div>
  

<script type="text/javascript">
      var disqus_shortname = 'sequenceiqblog';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-48528840-1', 'sequenceiq.com');
  ga('send', 'pageview');

</script>
</html>
