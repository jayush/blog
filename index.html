
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SequenceIQ Blog</title>
  <meta name="author" content="SequenceIQ">

   
  <meta name="description" content="">
  
  <meta name="keywords" content="">

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog.sequenceiq.com">
  <link href="/favicon.png" rel="icon">
  <link href='http://fonts.googleapis.com/css?family=Quicksand:300,400' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,300' rel='stylesheet' type='text/css'>
    <link href="/stylesheets/sequenceiq.css" media="screen, projection" rel="stylesheet" type="text/css">
   <!-- <link href="/stylesheets/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">-->
    <link href="/stylesheets/bootstrap.css" rel='stylesheet' type='text/css'>
  <link href="/stylesheets/bootstrap-theme.css"rel='stylesheet' type='text/css'>
 <!-- <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">-->

  <link href="/blog/atom.xml" rel="alternate" title="SequenceIQ Blog" type="application/atom+xml">
  <script src="/js/jquery.js"></script>
  <script src="/js/bootstrap-collapse.js"></script>
  <script src="/js/modernizr-2.0.js"></script>
  <script src="/js/octopress.js" type="text/javascript"></script>
  <script src="/js/application.js"></script>
  <script src="/js/bootstrap.js"></script>
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >

  <!--<div class="jumbotron seq-jumborton">-->
  <!--<div class="container">
      <a href="/">
        <img src="/images/logo.png" >
      </a>
    <h3 class="tagline">
      
        Our view on big data
      
    </h3>
  </div>-->
  <!--  <div class="navbar-static-top" id="company_div">
        <a href="http://sequenceiq.com/">
            <h5 style="margin: 0; margin-right: 5px;padding-bottom: 2px;padding-top: 2px; padding-right: 50px; font-weight: bolder;color: #003140;font-size: 10px;" class="pull-right" >SEQUENCEIQ.COM</h5>
        </a>
    </div>-->
    <header class="navbar navbar-static-top bs-docs-nav" id="top" role="banner" >
        <div class="container">
            <div class="navbar-header">
                <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".bs-navbar-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a href="http://sequenceiq.com/" class="navbar-brand">
                    <img id="logo" src="http://sequenceiq.com/img/logo@2x.png" width="154" height="39" alt="SequenceIQ">
                </a>
            </div>
            <div class="collapse navbar-collapse" role="navigation" style="/* margin-right: 6.2em; */">
                <ul class="nav navbar-nav navbar-right" id="menu-tag">
                    <li><a href="http://blog.sequenceiq.com/">Blog</a></li>
                    <li><a href="http://blog.sequenceiq.com/archives/">Archives</a></li>
                </ul>

            </div>
        </div>
    </header>
  <div class="container social-jumbotron-container">
      <div class="row">
        
        <div class="col-md-1"><a class="social-link" href="http://github.com/sequenceiq" title="Github Profile"><i class="icon-github-sign social-navbar"></i></a></div>
        
        
        
        <div class="col-md-1"><a class="social-link" href="http://linkedin.com/company/sequenceiq" title="Linkedin Profile"><i class="icon-linkedin-sign social-navbar"></i></a></div>
        
        
        <div class="col-md-1"><a class="social-link" href="http://twitter.com/sequenceiq" title="Twitter Profile"><i class="icon-twitter-sign social-navbar"></i></a></div>
        
        
        
        <div class="col-md-1"><a class="social-link" href="http://facebook.com/sequenceiq" title="Facebook Profile"><i class="icon-facebook-sign social-navbar"></i></a></div>
        
        

        
     </div>
  </div>
<!--</div>-->


  <div id="silent-container">

  </div>
  <div class="container" style="width: 95%;">
      <div class="row" id="main">
              <div class="col-md-9" id="">
                  <div class="">
                   <!-- <div id="content">-->
                      <div class="blog-index">
  
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">

      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/06/11/gs/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>
      
        <div class="row-fluid">
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/06/11/gs/">gs</a>
          <span class="badge name-badge"></span>
      </h1>
      

      
      
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">

      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/06/06/hadoop-summit-slides/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/apache-ambari/"><span class="label label-warning">Apache Ambari</span></a>
            
            <a href="/blog/categories/devops/"><span class="label label-warning">DevOps</span></a>
            
            <a href="/blog/categories/docker/"><span class="label label-warning">Docker</span></a>
            
            <a href="/blog/categories/hadoop/"><span class="label label-warning">Hadoop</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/06/06/hadoop-summit-slides/">Hadoop Summit 2014 - SequenceIQ slides</a>
          <span class="badge name-badge">Janos Matyas</span>
      </h1>
      <p>These are the slides of our presentation from the Hadoop Summit 2014, San Jose. We would like to thank all who have joined the session and the positive feedbacks we have received. This gives us a great confidence and validates our efforts that there is a great need to an easy and seamless Hadoop provisionig &ndash; let it be bare metal, cloud or other virtualizations.</p>

<p>Watch this space as <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> will be open sourced in the coming weeks.</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/35573123" width="640" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning" title="Docker based Hadoop provisioning - Hadoop Summit 2014 " target="_blank">Docker based Hadoop provisioning &ndash; Hadoop Summit 2014 </a> </strong> from <strong><a href="http://www.slideshare.net/JanosMatyas" target="_blank">Janos Matyas</a></strong> </div></p>

      
      
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">

      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/apache-ambari/"><span class="label label-warning">Apache Ambari</span></a>
            
            <a href="/blog/categories/hadoop/"><span class="label label-warning">Hadoop</span></a>
            
            <a href="/blog/categories/spring-shell/"><span class="label label-warning">Spring Shell</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/05/26/ambari-shell/">Apache Ambari + Spring Shell = Ambari Shell</a>
          <span class="badge name-badge">Krisztian Horvath</span>
      </h1>
      <h2>Introduction</h2>

<p><a href="http://ambari.apache.org/">Apache Ambari&rsquo;s</a> goal is to make Hadoop cluster management
as simple as possible. It provides an intuitive easy-to-use web UI backed by its RESTful API.
With only a few clicks you&rsquo;re able to install Hadoop services across any number of hosts
and Ambari will take care of the configurations as well. After the installation is complete
you can monitor your cluster taking leverage of <a href="http://ganglia.sourceforge.net/">Ganglia</a>
and <a href="http://www.nagios.org/">Nagios</a>.</p>

<p>At SequenceIQ we prefer to use command line tools whenever it&rsquo;s possible,
because it&rsquo;s much faster than interacting with a web UI and it&rsquo;s a better candidate for automation.
Here comes <a href="https://github.com/spring-projects/spring-shell#readme">Spring Shell</a> to our rescue.</p>

<p>It&rsquo;s an interactive shell that can be easily extended using a Spring based programming model and battle
tested in various projects like <a href="http://projects.spring.io/spring-roo/">Spring Roo</a>,
<a href="http://docs.spring.io/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/">Spring XD</a>, and
<a href="https://github.com/spring-projects/rest-shell">Spring REST Shell</a> Combine these two projects
and a really powerful tool will come to light.</p>

<h2>Ambari Shell</h2>

<p>The goal is to provide an interactive command line tool which supports:</p>

<ul>
<li>all functionality available through the REST API or Ambari web UI</li>
<li>makes possible complete automation of management task via <strong>scripts</strong></li>
<li>context aware command availability</li>
<li>tab completion</li>
<li>required/optional parameter support</li>
<li><strong>hint</strong> command to guide you on the usual path</li>
</ul>


<p>Since we&rsquo;re open sourcing the project, it should be available and part of the official Ambari repository
<a href="https://issues.apache.org/jira/browse/AMBARI-5482">soon</a>.</p>

<h2>Install Ambari Shell</h2>

<p>For now you have 3 options to give it a try:</p>

<ul>
<li>use our prepared <a href="https://index.docker.io/u/sequenceiq/ambari-shell/">docker image</a></li>
<li>download the latest self-containing executable jar form our maven repo</li>
<li>build it from source</li>
</ul>


<h3>Run via docker</h3>

<p>As we love to dockerize everything, we prepared a <a href="https://index.docker.io/u/sequenceiq/ambari-shell/">docker image</a>
with jdk1.7 on latest ubuntu, ambari-shell preinstalled. Detailed description is available on <a href="https://github.com/sequenceiq/ambari-shell-docker">github</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -it --rm sequenceiq/ambari-shell --ambari.host=&lt;HOSTNAME&gt; --ambari.port=&lt;PORT&gt;</span></code></pre></td></tr></table></div></figure>





      
       <a href="/blog/2014/05/26/ambari-shell/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">

      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/ansible/"><span class="label label-warning">Ansible</span></a>
            
            <a href="/blog/categories/ci/"><span class="label label-warning">CI</span></a>
            
            <a href="/blog/categories/docker/"><span class="label label-warning">Docker</span></a>
            
            <a href="/blog/categories/jenkins/"><span class="label label-warning">Jenkins</span></a>
            
            <a href="/blog/categories/devops/"><span class="label label-warning">devops</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker/">Building the build environment with Ansible and Docker</a>
          <span class="badge name-badge">Marton Sereg</span>
      </h1>
      <p>At SequenceIQ we put a strong emphasis on automating everything we can and this automation starts with our continuous integration &amp; delivery process.</p>

<h2>Introduction</h2>

<p>Lately there is a lot of buzz around continuous integration, development and deployment. More and more companies are moving away from long release cycles towards the &ldquo;release early, release often&rdquo; approach. The advantages of this approach are well known: lower overhead, earlier bug discovery and bug fixing, fewer context switches for the developers to name just a few. There are very good resources to learn about these concepts &ndash; blog posts by different companies (e.g.: by <a href="http://techblog.netflix.com/2013/08/deploying-netflix-api.html">Netflix</a>) and of course the <a href="http://www.amazon.com/dp/0321601912">book</a> &lsquo;Continuous Delivery&rsquo; by Jez Humble and David Farley &ndash; we&rsquo;ll now try to add our own experiences as well.</p>

<p>We&rsquo;ll share two blog posts about our continuous delivery at SequenceIQ: the first one being an introductory post about some tools we use to make the whole process easier and more robust, the second one explains the <a href="http://scottchacon.com/2011/08/31/github-flow.html">flow</a> we use from committing changes to being the changes available in our different environments.</p>

<h2>Tools</h2>

<p>Our CI and CD process at SequenceIQ is based on Ansible, Jenkins and of course Docker.
When we started to build our own process, we decided that we don&rsquo;t want to commit the same mistake that a lot of companies make about their build environment. At these companies the build servers where Jenkins and/or the other build tools are installed are often prepared once in the far past by someone who probably doesn&rsquo;t work there anymore. It quickly becomes something that everyone is afraid to touch and just hope that it will work forever. As the projects improve there will be a lot of different tools with a lot of different versions on the build machine and soon it leads to a small chaos, where the maintenance will involve a lot of hard manual work. To get rid of these problems, we use <a href="http://www.ansible.com/">Ansible</a> to &ldquo;build the build infrastructure&rdquo;, and Docker to run the builds in separated self-sufficient containers.</p>


      
       <a href="/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">

      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/05/01/mapreduce-job-profiling-with-R/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/ambari/"><span class="label label-warning">Ambari</span></a>
            
            <a href="/blog/categories/hadoop/"><span class="label label-warning">Hadoop</span></a>
            
            <a href="/blog/categories/profiling/"><span class="label label-warning">Profiling</span></a>
            
            <a href="/blog/categories/r/"><span class="label label-warning">R</span></a>
            
            <a href="/blog/categories/yarn/"><span class="label label-warning">YARN</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/05/01/mapreduce-job-profiling-with-R/">Job profiling with R</a>
          <span class="badge name-badge">Janos Matyas</span>
      </h1>
      <p>Management of a large Hadoop cluster is not an easy task &ndash; however thanks to projects like <a href="http://ambari.apache.org/">Apache Ambari</a> these tasks are getting easier. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its REST API to provision, manage and monitor a Hadoop cluster. While Ambari helps us a lot to monitor a cluster (leverages <a href="http://ganglia.sourceforge.net/">Ganglia</a> and <a href="http://www.nagios.org/">Nagios</a>), many times we have to profile our MapReduce jobs as well.</p>

<p>At SequenceIQ in order to profile MapReduce jobs, understand (job)internal statistics and create usefull graphs many times we rely on <a href="http://www.r-project.org/">R</a>. The metrics are collected from Ambari and the <a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html">YARN History Server</a>.</p>

<p>In this blog post we would like to explain and guide you through a simple process of collecting MapReduce job metrics, calculate different statistics and generate easy to understand charts.</p>

<p>The MapReduce application is the following:</p>

<ul>
<li>The input set of data is 12 pieces of 1 GB size files. Each file containes the same line of 16 bytes (012345678998765 plus the new line character)</li>
<li>The number of mappers running is 48, because the block size on HDFS is 256 MB and there are 12 files.</li>
<li>We use TextInputFormat (line num, line content) pairs. The output of the mapper function is the same as the input <code>IdentityMapper</code></li>
<li>The number of reducers is 20.</li>
<li>For simplicity we use <code>IdentityReducer</code> as the reducer function.</li>
<li>We use a special partitioner called <code>LinePartitoner</code>. The partitioning is based on line numbers (the key) and it makes sure that each reducer gets the same amount of data (line number <em>modulo</em> reducer number).</li>
</ul>


<h2>How to get the job results with R</h2>

<p>The job id that we are analysing with R is job_1395530889914_0005 (<em>replace this with your job is</em>)</p>

<p>First we load the R functions:</p>

<p><code>source("JobHistory.r")</code></p>

<p>Then we extract/read the job from the HistoryServer. It is actually using the Rest API of HistoryServer, parsing the JSON output.</p>

<p><code>job&lt;-getJob("job_1395530889914_0005","node02:19888")</code></p>

<p>The structure of the job follows the structure that is returned from the HistoryServer except that for example the parameters of all the tasks are converted into vectors so that can be easily handled in R.</p>


      
       <a href="/blog/2014/05/01/mapreduce-job-profiling-with-R/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">

      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/04/22/sql-on-hbase-with-apache-phoenix/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/apache-phoenix/"><span class="label label-warning">Apache Phoenix</span></a>
            
            <a href="/blog/categories/hbase/"><span class="label label-warning">HBase</span></a>
            
            <a href="/blog/categories/jooq/"><span class="label label-warning">Jooq</span></a>
            
            <a href="/blog/categories/sql/"><span class="label label-warning">SQL</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/04/22/sql-on-hbase-with-apache-phoenix/">SQL on HBase with Apache Phoenix</a>
          <span class="badge name-badge">Krisztian Horvath</span>
      </h1>
      <p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we use HBase to store large amounts of high velocity data and interact with them &ndash; many times we use native HBase interfaces but recently there was a need (internal and external) to access the data through an SQL interface.</p>

<h2>Introduction</h2>

<p>HBase is an open-source, distributed, versioned, non-relational database modeled after Google&rsquo;s Bigtable. It&rsquo;s designed to handle
billions of rows and millions of columns. However, using it as a relational database where you would store your data normalized,
split into multiple tables is not easy and most likely you will struggle with it as you would do in any other non-relational database.
Here comes <a href="http://phoenix.incubator.apache.org/">Apache Phoenix</a> in the picture. It&rsquo;s an SQL skin over HBase delivered as a
client-embedded JDBC driver targeting low latency queries. The project is in incubating state and under heavy development, but you
can already start embracing it.</p>

<h2>Installation</h2>

<p>Download the appropriate distribution from <a href="http://xenia.sote.hu/ftp/mirrors/www.apache.org/incubator/phoenix/">here</a>:</p>

<ul>
<li>Phoenix 2.x &ndash; HBase 0.94.x</li>
<li>Phoenix 3.x &ndash; HBase 0.94.x</li>
<li>Phoenix 4.x &ndash; HBase 0.98.1+</li>
</ul>


<p><em>Note the compatibilities between the HBase and Phoenix versions</em></p>

<p>Alternatively you can clone the <a href="https://github.com/apache/incubator-phoenix/tree/4.0">repository</a> and build it yourself (mvn clean install -DskipTests).
It should produce a jar file like this: phoenix-<code>version</code>-incubating-client.jar. Copy it to HBase&rsquo;s classpath (easiest way is to copy into
HBASE_HOME/lib). If you have multiple nodes it has to be there on every node. Restart the RegionServers and you are good to go. That&rsquo;s it?
Yes!</p>

<h2>Sample</h2>

<p>We&rsquo;ve pre-cooked a <a href="https://github.com/sequenceiq/phoenix-docker">Docker</a> image for you so you can follow this sample and play with it:
(the image is based on Hadoop 2.4, HBase 0.98.1, Phoenix 4.1.0-SNAPSHOT) <code>docker run -i -t sequenceiq/phoenix</code>.</p>


      
       <a href="/blog/2014/04/22/sql-on-hbase-with-apache-phoenix/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">

      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/04/17/apache-phoenix-sneak-peak/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/apache-phoenix/"><span class="label label-warning">Apache Phoenix</span></a>
            
            <a href="/blog/categories/hbase/"><span class="label label-warning">HBase</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/04/17/apache-phoenix-sneak-peak/">Apache Phoenix (sneak peak)</a>
          <span class="badge name-badge">Krisztian Horvath</span>
      </h1>
      <script type="text/javascript" src="https://asciinema.org/a/8982.js" id="asciicast-8982" async></script>


      
      
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">

      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/04/14/mapreduce-with-scalding/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/hadoop/"><span class="label label-warning">Hadoop</span></a>
            
            <a href="/blog/categories/scalding/"><span class="label label-warning">Scalding</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/04/14/mapreduce-with-scalding/">Writing MapReduce jobs in Scala</a>
          <span class="badge name-badge">Richard Doktorics</span>
      </h1>
      <p>At SequenceIQ we have many pre-built and configurable MapReduce jobs (complex math algorithms, filtering, sorting and correlation patterns, samplings, top-n, joins, partitioning, etc) &ndash; as building blocks of our job worklow. We needed to find a quick way to build and test these jobs during developement on &lsquo;local&rsquo; mode and be able to push the same jobs to a large test cluster without any modifications.
Though in general we use Java, we always strive for efficiency when we need to solve a problem and we use different  languages (not just JVM based) in our stack (e.g. Groovy, Go and R) &ndash; to write MapReduce jobs we have choosen Scala and the Scalding library. Scalding is a Scala library developed by Twitter that abstracts and makes easy to write Hadoop MapReduce jobs. In many ways Scalding is similar to Pig, but it was writen in Scala, bringing the advantages of Scala to your MapReduce jobs (e.g. type safety &ndash; how many times you have submitted a job to a cluster only to learn 5 hours later that you can&rsquo;t convert a String to Double).</p>

<p>This example will show you how you can use Scalding with Hadoop 2.3 and how easy is to write a MapReduce job with few lines of Scala code.</p>

<h2>Build the project</h2>

<p>In our example we will transform a csv file to an other one with a filter step.
To build the project use:</p>

<p><code>./gradlew clean build</code> in the project library.</p>

<h2>Run the sample</h2>

<p>To run the sample with these parameters in local mode:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>yarn jar scalding-sample-0.1.jar com.sequenceiq.samples.scalding.CsvToCsvFilterJob --local --schema <span class="o">{</span>YOUR_SCHEME<span class="o">}</span> --input <span class="o">{</span>INPUT<span class="o">}</span> --type <span class="o">{</span>TYPE<span class="o">}</span> --operator <span class="o">{</span>OPERATOR<span class="o">}</span> --field <span class="o">{</span>FILTER_FIELD<span class="o">}</span> --operand <span class="o">{</span>OPERAND<span class="o">}</span> --output <span class="o">{</span>OUTPUT_PATH<span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>or if you want to run the exampke using HDFS then use:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>yarn jar scalding-sample-0.1.jar com.sequenceiq.samples.scalding.CsvToCsvFilterJob --hdfs --schema <span class="o">{</span>YOUR_SCHEME<span class="o">}</span> --input <span class="o">{</span>INPUT<span class="o">}</span> --type <span class="o">{</span>TYPE<span class="o">}</span> --operator <span class="o">{</span>OPERATOR<span class="o">}</span> --field <span class="o">{</span>FILTER_FIELD<span class="o">}</span> --operand <span class="o">{</span>OPERAND<span class="o">}</span> --output <span class="o">{</span>OUTPUT_PATH<span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To run the filtering example the parameters are like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>yarn jar scalding-sample-0.1.jar com.sequenceiq.samples.scalding.CsvToCsvFilterJob --hdfs --schema id,name --input /input.csv --type int --operator eq --field id --operand 1 --output /output.csv
</span></code></pre></td></tr></table></div></figure>


<p>The code looks extremely simple:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">validation</span><span class="o">()</span>
</span><span class='line'>  <span class="n">input</span><span class="o">(</span><span class="n">args</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="na">filter</span><span class="o">(</span><span class="n">filterableField</span><span class="o">)</span> <span class="o">{</span><span class="nl">field:</span> <span class="n">String</span> <span class="o">=&gt;</span> <span class="n">createFilterCriterion</span><span class="o">(</span><span class="n">field</span><span class="o">)}</span>
</span><span class='line'>    <span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">output</span><span class="o">(</span><span class="n">args</span><span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>



      
       <a href="/blog/2014/04/14/mapreduce-with-scalding/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">

      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/docker/"><span class="label label-warning">Docker</span></a>
            
            <a href="/blog/categories/hadoop/"><span class="label label-warning">Hadoop</span></a>
            
            <a href="/blog/categories/hadoop-vm/"><span class="label label-warning">Hadoop VM</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/04/04/hadoop-docker-introduction/">Hadoop on Docker introduction</a>
          <span class="badge name-badge">Krisztian Horvath</span>
      </h1>
      <p>In the last few weeks we&rsquo;ve created and published several Docker images (<a href="https://github.com/sequenceiq/hadoop-docker">Hadoop</a>, <a href="https://github.com/sequenceiq/hoya-docker">Hoya</a>, <a href="https://github.com/sequenceiq/tez-docker">Tez</a>) to help you to quick-start with Hadoop and the latest innovations using YARN.
While many people have downloaded and started to use these preconfigured images we&rsquo;ve been asked to give a short introduction of what Docker is, and how one can build Docker images. Also during the Hadoop Summit in Amsterdem we have been inquired in particular about running Hadoop on Docker, so this post is our answer for all the requests we received.</p>

<p>Docker is an open-source engine that automates the deployment of any application as a lightweight, portable, self-sufficient container that will run virtually anywhere.</p>

<h2>Installation</h2>

<p>First install Docker with a package manager. On Ubuntu there is an easy way to start with by running a simple curl script which will do it for you:
<code>curl -s https://get.docker.io/ubuntu/ | sudo sh</code>.
Unfortunately Mac, Windows and some Linux distributions cannot natively run Docker (yet). At <a href="http://sequenceiq.com/">SequenceIQ</a> we develop on OSX and run a 3-6 node Hadoop mini cluster on our laptops. To overcome the limitation of running Docker natively
you will have to install <code>boot2docker</code>. It is a Tiny Core Linux made specifically to run Docker containers and weights less than 24MB memory.
Initialize <em>(boot2docker init)</em> and start <em>(boot2docker up)</em> and you can SSH into the VM <em>(boot2docker ssh, pass: tcuser)</em>.</p>

<p>To verify the installation let&rsquo;s test it: <code>docker run ubuntu /bin/echo hello docker</code>. Docker did a bunch of things within seconds:</p>

<ul>
<li>Downloaded the base image from the docker.io index</li>
<li>Created a new LXC container</li>
<li>Allocated a filesystem for it</li>
<li>Mounted a read-write layer</li>
<li>Allocated a network interface</li>
<li>Setup an IP for it, with network address translation</li>
<li>Executed a process inside the container</li>
<li>Captured the output and printed it</li>
</ul>


<p>You can run an interactive shell as well <code>docker run -i -t ubuntu /bin/bash</code> and use this shell as you would use any other shell.</p>

<p>While there are lots of different Docker images available we would like to share how to create your own images.</p>


      
       <a href="/blog/2014/04/04/hadoop-docker-introduction/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
    <hr>
    
  
  
    <article>
      

  <div class="row">
    <div class="col-md-2 post-meta">

      <div class="row-fluid">
        
            <a href="http://blog.sequenceiq.com/blog/2014/03/31/mahout-on-tez/#disqus_thread">Comments </a> <i class="fa fa-comments-o" style="color: #3ba9c4 !important;"></i>
        
      </div>
      
        <div class="row-fluid">
            
            <a href="/blog/categories/hortonworks/"><span class="label label-warning">Hortonworks</span></a>
            
            <a href="/blog/categories/mahout/"><span class="label label-warning">Mahout</span></a>
            
            <a href="/blog/categories/tez/"><span class="label label-warning">Tez</span></a>
            
            <a href="/blog/categories/yarn/"><span class="label label-warning">YARN</span></a>
            
        </div>
      
    </div>
    <div class="col-md-10 post-container">
      <h1 class="link">
          <a href="/blog/2014/03/31/mahout-on-tez/">Using Mahout with Tez</a>
          <span class="badge name-badge">Marton Sereg</span>
      </h1>
      <p>At SequenceIQ we are always open to the latest innovations in Hadoop, and trying to find a way to offer a better performance and cluster utilization to our customers. We came in close touch with the <a href="http://hortonworks.com/labs/stinger/">Stinger initiative</a> last year at the Hadoop Summit in Amsterdam &ndash; and ever since we have followed up with the project progress (latest <a href="http://hortonworks.com/blog/apache-tez-0-3-released/">release</a> is 0.3). The project was initiated by Hortonworks with the goal of a 100x performance improvement of Hive.
Although Hive is not part of our product stack (we use other ways for SQL on Hadoop), there is one particular key component of the Stinger initiative which was very interesting to us: <a href="https://github.com/apache/incubator-tez">Apache Tez</a>.</p>

<p><a href="http://incubator.apache.org/projects/tez.html">Apache Tez</a> is a new application framework built on top of Hadoop Yarn that can execute complex directed acyclic graphs (DAGs) of general data processing tasks. In many ways it can be thought of as a more flexible and powerful successor of the map-reduce framework. This was exactly what draw our attention and made us start thinking about using Tez as our runtime for map-reduce jobs.</p>

<h2>Tez and MapReduce</h2>

<p>At SequenceIQ we have chains of map-reduce jobs which are scheduled individually and read the output of previous jobs from HBase or HDFS. Many times our map-reduce job flow can be represented as a map-reduce-reduce pattern, however building complex job chains with the current map-reduce framework is not that easy (nor saves on performance) &ndash; we combined the ChainMapper/ChainReducer and IdentityMapper trying to build MRR like DAG job flows.</p>

<p>In Tez data coming from reducers&#8217; output can be pipelined together and eliminates IO/sync barriers, as no temporary HDFS write is required. Jobs can also be chained and represented as MRR steps with no restriction.
In MapReduce disregarding the data size, the shuffle (internal step between the map and reducer) phase writes the sorted partitions to disk, merge-sorts them and feed into the reducers. All these steps are done <em>in memory</em> with Tez and saves on this I/O heavy step, avoiding unnecessary temporary writes and reads.</p>

<h2>Tez and Mahout</h2>

<p>Part of our system is running machine learning algorithms in batch, using Mahout (we do ML on streaming data using Scala, MLlib and Apache Spark as well). To improve the runtime performance of these Mahout algorithms, and decrease the cluster execution time we started to experiment with combining Tez and Mahout, and rewrite a few Mahout drivers in order to build DAGs of MR jobs (MRR in particular where applicable) and submit the jobs in a Tez runtime on a YARN cluster.</p>


      
       <a href="/blog/2014/03/31/mahout-on-tez/">Read on &rarr;</a> 
    </div>
  </div>



    </article>
    
  
  <div class="pagination">
    
    <a class="prev" href="/blog/page/2/">&larr; Older</a>
    

    
  </div>
</div>


                    <!--</div>-->
                  </div>

              </div>
              <div class="col-md-3">
                 <section>
  <h2 class="blue">Recent Posts</h2>
  <ul id="recent_posts" class="list-group">
    
      <li class="list-group-item">
        <a href="/blog/2014/06/11/gs/">gs</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/06/06/hadoop-summit-slides/">Hadoop Summit 2014 - SequenceIQ slides</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/05/26/ambari-shell/">Apache Ambari + Spring Shell = Ambari Shell</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker/">Building the build environment with Ansible and Docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/05/01/mapreduce-job-profiling-with-R/">Job profiling with R</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/04/22/sql-on-hbase-with-apache-phoenix/">SQL on HBase with Apache Phoenix</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/04/17/apache-phoenix-sneak-peak/">Apache Phoenix (sneak peak)</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/04/14/mapreduce-with-scalding/">Writing MapReduce jobs in Scala</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/04/04/hadoop-docker-introduction/">Hadoop on Docker introduction</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/31/mahout-on-tez/">Using Mahout with Tez</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/24/hoya-at-sequenceiq/">Using Hortonworks Hoya at SequenceIQ</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/19/hadoop-2-dot-3-with-docker/">Hadoop 2.3 with docker</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/14/yarn-capacity-scheduler/">YARN Capacity Scheduler</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/11/data-cleaning-with-mapreduce-and-morphlines/">Data cleaning with MapReduce and Morphlines</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/07/read-from-hdfs/">HDFS and java.nio.channels</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/03/05/access-hdp2-sandbox/">Accessing HDP2 sandbox from the host</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/02/28/etl-and-data-quality/">ETL - producing better quality data</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/02/26/vote-for-us/">Vote for us - 2014 Hadoop Summit San Jose</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/02/22/custom-flume-source/">Custom Apache Flume source</a>
      </li>
    
      <li class="list-group-item">
        <a href="/blog/2014/02/07/hdp2-on-amazon/">Set up HDP2 on Amazon EC2</a>
      </li>
    
  </ul>
</section>

              </div>
      </div>
  </div>
  <div class="row-fluid" id="footer-container">
    <div class="container">
        <footer class="footer-page" role="contentinfo">
            <div class="row">
                <div class="col-md-6">
                    <div class="row">
    
    <div class="col-md-1"><a class="social-link" href="http://github.com/sequenceiq" title="Github Profile"><i class="fa fa-github fa-lg"></i></a></div>
    
    
    
    <div class="col-md-1"><a class="social-link" href="http://linkedin.com/company/sequenceiq" title="Linkedin Profile"><i class="fa fa-linkedin fa-lg"></i></a></div>
    
    
    <div class="col-md-1"><a class="social-link" href="http://twitter.com/sequenceiq" title="Twitter Profile"><i class="fa fa-twitter fa-lg"></i></a></div>
    
    
    
    <div class="col-md-1"><a class="social-link" href="http://facebook.com/sequenceiq" title="Facebook Profile"><i class="fa fa-facebook fa-lg"></i></a></div>
    
    

    
</div>
                </div>
                <div class="col-md-5">
                    


<p class="pull-right" >
  <span class="credit">&copy; SequenceIQ Inc. 2014. All rights reserved. </span>
    <br><a href="pp.html" style="color: #508190;">Privacy Policy</a> &nbsp; <a href="tos.html" style="color: #508190;">Terms of Service</a></p>
</p>


                </div>
            </div>
        </footer>
    </div>

  </div>
  

<script type="text/javascript">
      var disqus_shortname = 'sequenceiqblog';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=625149054184531";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>




  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-48528840-1', 'sequenceiq.com');
  ga('send', 'pageview');

</script>
</html>
