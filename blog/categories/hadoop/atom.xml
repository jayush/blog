<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-03-27T10:16:20+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop 2.3 with docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker/"/>
    <updated>2014-03-19T03:54:09+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker</id>
    <content type="html"><![CDATA[<p>You want to try out hadoop 2.3? Go to the zoo and <a href="http://sethgodin.typepad.com/seths_blog/2005/03/dont_shave_that.html">shave a yak</a>.
Or simply just use <a href="https://www.docker.io/">docker</a>.</p>

<p><code>
docker run -i -t sequenceiq/hadoop-docker /etc/bootstrap.sh -bash
</code></p>

<h2>Testing</h2>

<p>```bash</p>

<h1>start ssh and hdfs</h1>

<p>cd $HADOOP_PREFIX</p>

<h1>run the mapreduce</h1>

<p>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.3.0.jar grep input output &lsquo;dfs[a-z.]+&rsquo;</p>

<h1>check the output</h1>

<p>bin/hdfs dfs -cat output/*
```</p>

<!-- more -->


<h2>Yak shaving an elefant</h2>

<p>I had problems installing hadoop 2.3 and by googling i stumbled upon this <a href="http://mail-archives.apache.org/mod_mbox/hadoop-mapreduce-user/201403.mbox/%3C53192FD4.2040003@oss.nttdata.co.jp%3E">email thread</a>,
which references an <a href="http://aajisaka.github.io/hadoop-project/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation">alternative hadoop docs</a> deployed on github.</p>

<p>By following that description i run into an other issue:
hadoop is delivered with 32 bit native libraries. No big deal &hellip;</p>

<h2>Hadoop native libraries</h2>

<p>Of course there is an official <a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries Guide</a> it instructs you
to simple download the sources and <strong>mvn package</strong>. But than you face a new issue: missing <em>protobuf</em>. Eeeasy &hellip;</p>

<h2>Protobuf 2.5</h2>

<p>Unfortunately <strong>yum install protobuf</strong> installs an older 2.3 version, which is close but no cigar.
 So you download protobuf source, and <strong>./configure &amp;&amp; make &amp;&amp; make install</strong></p>

<p>To succeed on that one you have to install a couple of development packages, and there you go.</p>

<h2>Bintray</h2>

<p>I wanted to save you those steps so created a binary distro of the native libs
compiled with 64 bit CentOS. So I created <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.3.0/view/files">Bintray r̨epo</a>. Enjoy</p>

<h2>Automate everything</h2>

<p>As I&rsquo;m an automation fetishist, a Docker file was created, and released in the official <a href="https://index.docker.io/u/sequenceiq/hadoop-docker/">docker repo</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YARN Capacity Scheduler]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/14/yarn-capacity-scheduler/"/>
    <updated>2014-03-14T15:17:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/14/yarn-capacity-scheduler</id>
    <content type="html"><![CDATA[<p>Since the emergence of Hadoop 2 and the YARN based architecture we have a platform where we can run multiple applications (of different types) not constrained only to MapReduce. Different applications or different MapReduce job profiles have different resource needs, however since Hadoop 2.0 is a multi tenant platform the different users could have different access patterns or need for cluster capacity. In Hadoop 2.0 this is achieved through YARN schedulers — to allocate resources to various applications subject to constraints of capacities and queues (for more information on YARN follow this <a href="http://hortonworks.com/hadoop/yarn/">link</a> or feel free to ask us should you have any questions).</p>

<p>In Hadoop 2.0, the scheduler is a pluggable piece of code that lives inside the <em>ResourceManager</em> (the JobTracker in MR1) &ndash; the ultimate authority that arbitrates resources among all the applications in the system. The scheduler in YARN does not perform monitoring or status tracking and offers no guarantees to restart failed tasks — check our sample <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-queue-tests">GitHub</a> project to check how monitoring or progress can be tracked.</p>

<p>The Capacity Scheduler was designed to allow significantly higher cluster utilization while still providing predictability for Hadoop workloads, while sharing resources in a predictable and simple manner, using the common notion of <em>job queues</em>.</p>

<p>In our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-queue-tests">example</a> we show you how to use the Capacity Scheduler, configure queues with different priorities, submit MapReduce jobs into these queues, monitor and track the progress of the jobs &ndash; and ultimately see the differences between execution times and throughput of different queue setups.</p>

<p>First, let’s config the Capacity Scheduler (you can use xml, <a href="http://ambari.apache.org/">Apache Ambari</a> or you can configure queues programmatically). In this example we use a simple xml configuration.</p>

<p><code>xml
&lt;property&gt;
  &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;
  &lt;value&gt;default,highPriority,lowPriority&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;yarn.scheduler.capacity.root.highPriority.capacity&lt;/name&gt;
  &lt;value&gt;70&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;yarn.scheduler.capacity.root.lowPriority.capacity&lt;/name&gt;
  &lt;value&gt;20&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;yarn.scheduler.capacity.root.default.capacity&lt;/name&gt;
  &lt;value&gt;10&lt;/value&gt;
&lt;/property&gt;
</code>
We have 3 queues, with different queue setups/priorities. Each queue is given a <em>minimum</em> guaranteed percentage of total cluster capacity available &ndash; the total guaranteed capacity must equal 100%. In our example the <em>highPriority</em> queue has 70% of the resources, the <em>lowPriority</em> 20%, and the default queue has the remaining 10%. While it is not highlight in the example above, the Capacity Scheduler provides elastic resource scheduling, which means that if there are idle resources in the cluster, then one queue can take up more of the cluster capacity than was minimally allocated . In our case we could allocate a <em>maximum</em> capacity to the <em>lowPriority</em> queue:</p>

<p><code>xml
&lt;property&gt;
  &lt;name&gt;yarn.scheduler.capacity.root. lowPriority.maximum-capacity&lt;/name&gt;
  &lt;value&gt;50&lt;/value&gt;
&lt;/property&gt;
</code></p>

<!-- more -->


<p>Now lets submit some jobs into these queues. We will use the QuasiMonteCarlo.java example (coming with Hadoop) &ndash; a MapReduce job that estimates the value of Pi, and submit the same MapReduce jobs into the low and high priority queues.</p>

<p>``` java</p>

<pre><code>//get a configuration
Configuration priorityConf = new Configuration();
priorityConf.set("mapreduce.job.queuename", queueName);
………………………     
//submit the job
JobID jobID = QuasiMonteCarlo.submitPiEstimationMRApp(“Pi estimation into: "+ queueName, 10, 3, tempDir, priorityConf);
</code></pre>

<p>```
Once the jobs are submitted in the different queues, you can track the MapReduce job progress and monitor the queues through YARN. Using YARNRunner you can get ahold of a job status, and retrieve different informations:</p>

<p>``` java</p>

<pre><code>//print overall job M/R progresses
LOGGER.info("\nJob " + jobStatus.getJobName() + "in queue (" + jobStatus.getQueue() + ")" + " progress M/R: " +                 jobStatus.getMapProgress() + "/" + jobStatus.getReduceProgress());
LOGGER.info("Tracking URL : " + jobStatus.getTrackingUrl());
LOGGER.info("Reserved memory : " + jobStatus.getReservedMem() + ", used memory : "+ jobStatus.getUsedMem() + " and          used slots : "+ jobStatus.getNumUsedSlots());

// list map &amp; reduce tasks statuses and progress        
TaskReport[] reports = yarnRunner.getTaskReports(jobID, TaskType.MAP);
for (int i = 0; i &lt; reports.length; i++) {
LOGGER.info("MAP: Status " + reports[i].getCurrentStatus() + " with task ID " + reports[i].getTaskID() + ", and                 progress " + reports[i].getProgress()); 
}
</code></pre>

<p>```</p>

<p>Same way the queue capacity can be tracked as well:</p>

<p>```java</p>

<pre><code>ArrayNode queues = (ArrayNode) jsonNode.path("scheduler").path("schedulerInfo").path("queues").get("queue");
for (int i = 0; i &lt; queues.size(); i++) {
JsonNode queueNode = queues.get(i);                     
LOGGER.info("queueName / usedCapacity / absoluteUsedCap / absoluteCapacity / absMaxCapacity: " + 
                    queueNode.findValue("queueName") + " / " +
                    queueNode.findValue("usedCapacity") + " / " + 
                    queueNode.findValue("absoluteUsedCapacity") + " / " + 
                    queueNode.findValue("absoluteCapacity") + " / " +
                    queueNode.findValue("absoluteMaxCapacity"));
}
</code></pre>

<p><code>``
You can run the example with</code>hadoop jar yarn-queue-tests-1.0-SNAPSHOT.jar com.sequenceiq.yarntest.client.JobClient<code>, and play with different queue setups. Once you have changed the queue setup you can refresh the settings with</code>yarn rmadmin -refreshQueues`.</p>

<p>You can check the progress through the <a href="https://gist.github.com/matyix/9528220">logs</a>.The <a href="http://sandbox.hortonworks.com:8088/cluster/scheduler">cluster statistics</a> and <a href="http://sandbox.hortonworks.com:8088/cluster/apps">application statistics</a> are available as well (we run this example on Hortonworks HDP2 sandbox, but any other Hadoop 2 distribution works &ndash; you can set your own cluster on Amazon EC2 using SequenceIQ&rsquo;s setup scripts from our <a href="https://github.com/sequenceiq/hadoop-cloud-scripts">GitHub</a> page).</p>

<p>As you expect, the jobs submitted into the  <code>highPriority</code> queue are finished earlier than those submitted into the <code>lowPriority</code> one &ndash; though (in case of submitting into the same queue) the MapReduce jobs should take the same time (as they are the same MapReduce job, have the same job profile).</p>

<p>This is a good way to start experimenting multi-tenancy and parallel jobs submission into a shared cluster (beyond the Fair Scheduler). At <a href="http://sequenceiq.com">SequenceIQ</a> we are working on a heuristic YARN scheduler &ndash; where we can adapt to increased work loads, submit jobs into queues based on different customer QoS profiles, and increase or downsize our cloud based cluster based on load and capacity.</p>

<p>For more details and updates please follow us through our <a href="http://www.linkedin.com/company/sequenceiq/">LinkedIn</a> page.
You can access the code from our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-queue-tests">GitHub</a> repository.</p>

<p>Thanks,
SequenceIQ</p>
]]></content>
  </entry>
  
</feed>
