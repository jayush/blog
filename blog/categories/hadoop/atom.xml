<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-07-13T10:29:19+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Apache Ambari configuration service]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service/"/>
    <updated>2014-07-09T08:20:05+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service</id>
    <content type="html"><![CDATA[<p>At SequenceIQ we use Apache Ambari for provisioning, managing, and monitoring Apache Hadoop clusters on different environments. However Ambari has more useful features than these &ndash; especially for us who automate and frequently build on-demand Hadoop clusters in cloud environments and submit different applications into. These Hadoop clusters carry different components, configurations and services &ndash; think of dev->test->UAT->PROD cluster lifecycles, different settings, SLA&rsquo;s, etc).</p>

<p>Configuration of applications that use dynamically built YARN clusters can be challenging. This is due to the huge amount of configuration properties, some of which needs to be kept in sync on YARN client application side. Think of <em>yarn.resourcemanager.address</em>, <em>fs.defaultFS</em>, <em>yarn.resourcemanager.scheduler.address</em> to name a few. Each time these cluster specific entries change, client applications needs to be reconfigured. Those who ever played with clusters where the default properties are overridden know what this means&hellip;</p>

<p>At Sequenceiq we use Ambari for building on-demand YARN clusters (see the related <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/"> blog post</a>). In our case Ambari not only maintains the configuration of the cluster it manages but also provides access to them through a set of REST resources.</p>

<p>To overcome the configuration maintenance problem in YARN client applications, we implemented an Ambari REST client application that embedded in client applications can dynamically retrieve configuration from an Ambari instance. Thus the only thing needed for an application to have the proper configuration is the access to the Ambari instance.</p>

<p>The Ambari REST client is an open source project we developed and contributed to Apache Ambari &ndash; it&rsquo;s a Groovy REST client used by the <a href="https://github.com/sequenceiq/ambari-shell">Ambari Shell</a> and <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a>.</p>

<!-- more -->


<p>Here is a short example on how to make use of the Ambari client in an arbitrary application:</p>

<p>``` java
public class AmbariConfigurationService {
&hellip;
private AmbariClient ambariClient;</p>

<p>public AmbariConfigurationService(){
  // inject / provide the service with the ambari related properties
  ambariClient = new AmbariClient(ambariHost, ambariPort, ambariUser, ambariPass);
}</p>

<p>// list with the properties needed by the application
private List<String> configList = Arrays.asList(&ldquo;mapreduce.framework.name&rdquo;, &ldquo;yarn.resourcemanager.address&rdquo;, &ldquo;hbase.zookeeper.quorum&rdquo; );</p>

<p>// assembles a Configuration instance with the properties needed by the application
public Configuration getConfiguration() {</p>

<pre><code>    //  use this constructor to avoid loading of properties from the classpath!
    Configuration configuration = new Configuration(false);

    // Map with service specific configuration. The keys are service names: eg.: yarn-site, hbase-site, global ...
    Map&lt;String, Map&lt;String, String&gt;&gt; serviceConfigMap = ambariClient.getServiceConfigMap();

    for (Map.Entry&lt;String, Map&lt;String, String&gt;&gt; serviceEntry : serviceConfigMap.entrySet()) {
        for (Map.Entry&lt;String, String&gt; configEntry : serviceEntry.getValue().entrySet()) {
            if (configList.contains(configEntry.getKey())) {
                configuration.set(configEntry.getKey(), configEntry.getValue());
            }
        }
    }

    // decorate the config with application specific entries, like "dfs.client.use.legacy.blockreader", "mapreduce.job.user.classpath.first"
    decorateConfiguration(configuration);

    return configuration;
}
</code></pre>

<p>}
<code>
_Note: Apart from the</code>getServiceConfigMap() ``` method you&rsquo;ll find a few interesting and useful operations_</p>

<p>You can get the Ambari client code from the <a href="https://github.com/sequenceiq/ambari-rest-client">SequenceIQ GitHub repository</a> &ndash; clone it, build it and add it as a dependency to your project.</p>

<p>If you&rsquo;d like to play with a real multi-node Ambari managed cluster check out <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">this</a> older blog post &ndash; this will set you up with a Hadoop cluster in less than 2 minutes / one-click.</p>

<p>Let us know how it works for you &ndash; for updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Re-prioritize running jobs with YARN schedulers]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues/"/>
    <updated>2014-07-02T08:20:05+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we run different applications all within the same Hadoop YARN cluster. Often the deployed Hadoop stack is a multi-tenant and multi-application and runtime setup &ndash; and as usual for a scenario as such end users will try to use or book as much cluster capacity as possible. A great help for solving these problems are YARN schedulers &ndash; however in our case due to certain SLA and QoS requirements we needed to step further. We have invested a great effort to build custom YARN schedulers, learn about application insights (check our <a href="http://blog.sequenceiq.com/blog/2014/05/01/mapreduce-job-profiling-with-R/">blog post</a> about how we use R to profile running jobs) and we would like to share our experience with the community. Let&rsquo;s dig into technical details.</p>

<p>In YARN, one of the ResourceManager&rsquo;s most important role is the scheduling (allocating available resources in the cluster) between competing applications. It doesn&rsquo;t care about per-application states nor internal flows and optimizations, but the overall resource requirements of
each application. Currently there are 3 different scheduler implementations available: FIFO, Fair, Capacity.</p>

<p>Going back a few weeks in time we blogged about how to configure the
<a href="http://blog.sequenceiq.com/blog/2014/03/14/yarn-capacity-scheduler/">CapacityScheduler</a> and use different queue
setups. Based on the feedbacks we have received we realized that there is a lack of knowledge about how these schedulers work and many people have asked to fill that gap. Good news that we didn&rsquo;t
forget about you. We&rsquo;re going to start a post series where we&rsquo;ll explain them a little bit detailed with fancy diagrams and code examples.</p>

<p>But before doing that, let&rsquo;s visit a concrete problem we encountered while we were developing our product stack.
We wanted to use the CapacityScheduler, but for different reasons (SLA and QoS) move the submitted applications between different queues to set a priority among them &ndash; at runtime (quick reminder: queues are either a composition of other queues or a collection of applications, forming a tree).
Cross application priorites can&rsquo;t be configured yet, only priorities between tasks within the application. The only problem is if you check the code you&rsquo;ll find this:</p>

<p>```java
@Override
  public String moveApplication(ApplicationId appId, String newQueue) throws YarnException {</p>

<pre><code>throw new YarnException(getClass().getSimpleName()
    + " does not support moving apps between queues");
</code></pre>

<p>  }
```</p>

<!-- more -->


<p>Currently this operation is supported only by the FairScheduler. Why is it not implemented? Let us know in a comment and you might receive a surprise present from us :). In the meantime if we&rsquo;d like
to implement it what would be the steps? Lets start with the following queue hierarchy and their capabilities taken from the integration tests:</p>

<p><img class="<a" src="href="http://yuml.me/1fe68e90">http://yuml.me/1fe68e90</a>"></p>

<p>Assume we&rsquo;ve submitted 2 applications, <strong>app1</strong> to <code>b2</code> and <strong>app2</strong> to <code>a2</code> (submitting applications is only allowed to leaf queues). What if <strong>app2</strong> is
pending for so long because of the queue capacity and my friend&rsquo;s friend&rsquo;s dog cannot wait anymore to see his data clustering result? We could play with the queue capacities and max capacities, but then other apps might get scheduled as well and we don&rsquo;t want that.
Then we could move the app to a queue where it can get resources with a much higher chance. To move an app to somewhere
else in the hierarchy we have to consider and update a whole bunch of things. Let&rsquo;s move <strong>app1</strong> to queue <code>b1</code>.</p>

<p>Obviously we have to check if the target queue is a leaf queue and moving the app there does not violate any constraints. But how to do that?
The first part is easy (leaf or parent), but what about the other one? It has to do something with queue capacities, but checking only the target
queue&rsquo;s capacity is not enough, we have to go up in the hierarchy (because the parent queues also keep track the number of applications
and resource usages) but for how deep? The lowest common ancestor of the source and target is enough, because above that nothing changes. In our
case it&rsquo;s the <code>b</code> (b1, b2). Finding it is not that hard since the queues are declared like this:</p>

<ul>
<li>root.a.a1</li>
<li>root.a.a2</li>
<li>root.b.b1</li>
<li>root.b.b2</li>
<li>root.b.b3</li>
</ul>


<p>Going back until <code>b</code> and check the capacities:
```java</p>

<pre><code>CSQueue currentQueue = targetQueue;
while (currentQueue != lowestCommonAncestor) {
  // maxApps
  if (currentQueue.getNumApplications() == this.conf.getMaximumApplicationsPerQueue(currentQueue.getQueueName())) {
    throw new YarnException("Moving app attempt " + appAttId + " to queue "
      + queueName + " would violate queue maxApps constraints on"
      + " queue " + currentQueue.getQueueName());
  }

  // maxCapacity
  float potentialNewCapacity = Resources.divide(calculator, clusterResource, Resources.add(currentQueue.getUsedResources(), consumption), clusterResource);
  if (potentialNewCapacity &gt;= currentQueue.getAbsoluteMaximumCapacity()) {
    throw new YarnException("Moving app attempt " + appAttId + " to queue "
      + queueName + " would violate queue maxCapacity constraints on"
      + " queue " + currentQueue.getQueueName());
  }
  currentQueue = currentQueue.getParent();
}
</code></pre>

<p>```</p>

<p>If everything is fine we can execute the movement.
```java
private void executeMove(SchedulerApplication app, FiCaSchedulerApp attempt, LeafQueue oldQueue, LeafQueue newQueue) {</p>

<pre><code>oldQueue.removeApplicationAttempt(attempt);
attempt.move(newQueue); // This updates all the queue metrics 'til the parent
app.setQueue(newQueue);
newQueue.trackApplications(attempt.getApplicationId(), attempt.getUser());
newQueue.submitApplicationAttempt(attempt, attempt.getUser());
</code></pre>

<p>}
```</p>

<p>There are so many things implemented in these method calls it wouldn&rsquo;t fit here, but it serves the purpose here as pseudo code.</p>

<ul>
<li><p>oldQueue.removeApplicationAttempt(attempt);<br/>
Remove the application from the active and pending list. Notify the parents that an app has been removed.</p></li>
<li><p>attempt.move(newQueue);<br/>
Update the queue metrics upwards to root.</p></li>
<li><p>app.setQueue(newQueue);<br/>
Set the target queue in the app.</p></li>
<li><p>newQueue.trackApplications(attempt.getApplicationId(), attempt.getUser());<br/>
Notify the parents that a new application has been moved here.</p></li>
<li><p>newQueue.submitApplicationAttempt(attempt, attempt.getUser());<br/>
Finally submit the application attempt to the queue.</p></li>
</ul>


<p>As usual we always release the code as well &ndash; you can get the details from our <a href="https://github.com/sequenceiq">GitHub</a> page.</p>

<ul>
<li><p>Move applications between Capacity Scheduler queues <a href="https://github.com/sequenceiq/hadoop-common/blob/branch-2.4.1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/a/ExtendedCapacityScheduler.java#L924">implementation</a>.</p></li>
<li><p>Test case <a href="https://github.com/sequenceiq/hadoop-common/blob/branch-2.4.1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/a/TestExtendedCapacitySchedulerAppMove.java">implementation</a>.</p></li>
</ul>


<p>In case you are interested on the YARN Scheduler series make sure to follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a> or <a href="https://twitter.com/sequenceiq">Twitter</a> for the upcoming posts.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Hadoop 2.4.1 on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/25/hadoop-2-4-0-docker/"/>
    <updated>2014-06-25T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/25/hadoop-2-4-0-docker</id>
    <content type="html"><![CDATA[<p>A few weeks ago we have released an Apache Hadoop 2.3 Docker image &ndash; in a very short time this become the most <a href="https://registry.hub.docker.com/search?q=hadoop&amp;s=downloads">popular</a> Hadoop image in the Docker <a href="https://registry.hub.docker.com/">registry</a>.</p>

<p>Following on the success of our Hadoop 2.3 Docker <a href="https://registry.hub.docker.com/u/sequenceiq/hadoop-docker/">image</a>, the feedbacks and requests we have received and aligning with the Hadoop release cycle, we have released an Apache Hadoop 2.4.1 Docker image &ndash; same as the previous version this is available as a trusted and automated build on the official Docker <a href="https://registry.hub.docker.com/">registry</a>.</p>

<p>Please note that beside this Hadoop image, we have released and maintain a <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/">pseudo-distributed</a> and <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">distributed</a> Hadoop Docker image provisioned with Apache Ambari. As they are provisioned with Ambari you have the option to change, add and remove Hadoop components using cluster blueprints.</p>

<h2>Build the image</h2>

<p>In case you&rsquo;d like to try directly from the <a href="https://github.com/sequenceiq/hadoop-docker">Dockerfile</a> you can build the image as:</p>

<p><code>
docker build  -t sequenceiq/hadoop-docker .
</code></p>

<!-- more -->


<h2>Pull the image</h2>

<p>As it is also released as an official Docker image from Docker&rsquo;s automated build repository &ndash; you can always pull or refer the image when launching containers.</p>

<p><code>
docker pull sequenceiq/hadoop-docker:2.4.1
</code></p>

<h2>Start a container</h2>

<p>In order to use the Docker image you have just build or pulled use:</p>

<p><code>
docker run -i -t sequenceiq/hadoop-docker /etc/bootstrap.sh -bash
</code></p>

<h2>Testing</h2>

<p>You can run one of the stock examples:</p>

<p>```
cd $HADOOP_PREFIX</p>

<h1>run the mapreduce</h1>

<p>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar grep input output &lsquo;dfs[a-z.]+&rsquo;</p>

<h1>check the output</h1>

<p>bin/hdfs dfs -cat output/*
```</p>

<h2>Hadoop native libraries, build, Bintray, etc</h2>

<p>The Hadoop build process is no easy task &ndash; requires lots of libraries and their right version, protobuf, etc and takes some time &ndash; we have simplified all these, made the build and released a 64b version of Hadoop nativelibs on our <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.4.1/view/files">Bintray repo</a>. Enjoy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pearson correlation with Scalding]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/23/scalding-correlation-example/"/>
    <updated>2014-06-23T18:07:18+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/23/scalding-correlation-example</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>At SequenceIQ we are processing data in batch and streaming &ndash; for both we use Scala as our prefered language; for batch processing in particular we use Scalding to build our job and data pipelines. Actually there is <code>Babylon</code> at SequenceIQ as we use Java, Scala, Go, R, Groovy, Ansible, shell, JavaScript and what not &ndash; follow up with us for a post talking about the language heterogeneity.</p>

<p>Scalding is a powerful tool and great choice to simplify the writing and abstracting MapReduce jobs &ndash; an open source project originally developed by Twitter and recently the community.
In the following detailed example we&rsquo;d like show you an example of how to write and test Scalding jobs, running on Hadoop.</p>

<h2>Writing a Pearson correlation job</h2>

<p>In this example, we&rsquo;d like to calculate a Pearson&rsquo;s product-moment coefficient on 2 columns of a given <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/scalding-correlation/data">input</a>.
This is a simple computation and the easiest way to find any dependency between two datasets.
First of all we need all the parameters for the given <a href="http://www.statisticshowto.com/what-is-the-correlation-coefficient-formula/">formula</a>.
In Scala the code would look like this:</p>

<p>``` scala
trait CorrelationOp {
  def calculateCorrelation(size: Long, su1: Double, su2: Double, sq1: Double, sq2: Double, dotProd: Double) : Double = {</p>

<pre><code>val dividend = (size * dotProd) - (su1 * su2)
val divisor = scala.math.sqrt(size * sq1 - su1 * su1) * scala.math.sqrt(size * sq2 - su2 * su2)
dividend / divisor
</code></pre>

<p>  }
}
```</p>

<!-- more -->


<p>In this example we compute all the required parameters for the correlation formula using the <a href="https://github.com/twitter/scalding/wiki/Fields-based-API-Reference">Field API</a> of Scala.
First we obtain the input/output and the two comparable column arguments which comes from command line parameters (usage : &mdash;key value) and provide the schema for the CSV input.
After the input is read we map the two selected fields (product and squares); with the underlined informations, we are able to produce the required parameters (grouping part).
At the end we just need to use the formula on the given fields (second map) and write the results into a TSV file.
``` scala
  val comparableColumn1 = args(&ldquo;column1&rdquo;)
  val comparableColumn2 = args(&ldquo;column2&rdquo;)
  val samplePercent = args.getOrElse(&ldquo;samplePercent&rdquo;,&ldquo;1.00&rdquo;).toDouble</p>

<p>  val scheme = new Fields(&ldquo;id&rdquo;, &ldquo;num1&rdquo;, &ldquo;num2&rdquo;, &ldquo;num3&rdquo;, &ldquo;num4&rdquo;, &ldquo;num5&rdquo;)</p>

<p>  Csv(args(&ldquo;input&rdquo;), fields = scheme, skipHeader = true).read
  .sample(samplePercent)
  .map((comparableColumn1,comparableColumn2) &ndash;> (&lsquo;prod, 'compSq1, 'compSq2)){</p>

<pre><code>values : (Double, Double) =&gt;
  (values._1 * values._2, math.pow(values._1, 2), math.pow(values._2, 2))
</code></pre>

<p>  }
  .groupAll{</p>

<pre><code>_.size
  .sum[Double](comparableColumn1 -&gt; 'compSum1)
  .sum[Double](comparableColumn2 -&gt; 'compSum2)
  .sum[Double]('compSq1 -&gt; 'normSq1)
  .sum[Double]('compSq2 -&gt; 'normSq2)
  .sum[Double]('prod -&gt; 'dotProduct)
</code></pre>

<p>  }
  .limit(1)
  .project(&lsquo;size,'compSum1, 'compSum2, 'normSq1, 'normSq2, 'dotProduct)
  .map(('size, 'compSum1, 'compSum2,'normSq1, 'normSq2, 'dotProduct)</p>

<pre><code>-&gt; ('key, 'correlation)){
fields : (Long, Double, Double, Double, Double, Double) =&gt;
  val (size, sum1, sum2, normSq1, normSq2, dotProduct) = fields
  val corr = calculateCorrelation(size, sum1, sum2, normSq1, normSq2, dotProduct)
  (comparableColumn1 + "-" + comparableColumn2, corr)
</code></pre>

<p>  }
  .project(&lsquo;key, 'correlation)
  .write(Tsv(args(&ldquo;output&rdquo;)))</p>

<p>```</p>

<p>For running the example you will have to run the following command: (<em>you can use &mdash;hdfs instead of &mdash;local</em>)</p>

<p><code>bash
yarn jar scalding-correalation-1.0.jar com.sequenceiq.scalding.correlation.SimpleCorrelationJob --local --input data/data.csv --output data/corr-out.tsv --column1 num1 --column2 num2 --samplePercent 0.1
</code></p>

<h2>Testing Scalding jobs</h2>

<p>In order to test that your data transformations are correct, you can use the
<a href="http://twitter.github.io/scalding/com/twitter/scalding/JobTest.html">JobTest</a> class for unit testing.
``` scala
@RunWith(classOf[JUnitRunner])
class SimpleCorrelationJobTest  extends Specification {
  &ldquo;A SimpleCorrelation Job&rdquo; should {</p>

<pre><code>val input = List((1,2,3,3,4,5),(2,1,2,3,4,5),(3,4,5,3,4,5))
val correctOutputLimit = 0.8

JobTest("com.sequenceiq.scalding.correlation.SimpleCorrelationJob")
  .arg("input", "fakeInput")
  .arg("output", "fakeOutput")
  .arg("column1", "num1")
  .arg("column2", "num2")
  .arg("correlationThreshold", "0.8")
  .source(Csv("fakeInput", ",", new Fields("id","num1","num2","num3","num4","num5"),skipHeader = true), input)
  .sink[(String, Double)](Tsv("fakeOutput", fields = Fields.ALL)) {
  outputBuf =&gt;
    val actualOutput = outputBuf.toList.head._2
    "return greater correlation result than 0.8" in {
      correctOutputLimit must be_&lt; (actualOutput)
    }
}
  .run
  .finish
</code></pre>

<p>  }
}
```</p>

<h2>Writing results to HBase</h2>

<p>In case we&rsquo;d like to store our data in a database (at SequenceIQ we use HBase) we can use a special Cascading Tap for it.
In this example we used <a href="https://github.com/ParallelAI/SpyGlass">Spyglass</a> to store the correlation results in HBase.
``` scala
  val tableName = args(&ldquo;tableName&rdquo;)
  val quorum_name = args(&ldquo;quorum&rdquo;)
  val quorum_port = args(&ldquo;quorumPort&rdquo;).toInt</p>

<p>  val scheme = List(&lsquo;key, 'correlation)
  val familyNames = List(&ldquo;corrCf&rdquo;)</p>

<p>  Tsv(args(&ldquo;input&rdquo;)).read</p>

<pre><code>.toBytesWritable(scheme)
.write(
  new HBaseSource(
    tableName,
    quorum_name + ":" + quorum_port,
    scheme.head,
    familyNames,
    scheme.tail.map((x: Symbol) =&gt; new Fields(x.name)).toList,
    timestamp = Platform.currentTime
  ))
</code></pre>

<p>```</p>

<h2>Build the application</h2>

<p><code>bash
./gradlew clean jar
</code>
or
<code>bash
export GRADLE_OPTS="-XX:MaxPermSize=2048m" # for tests
./gradlew clean build
</code></p>

<h2>Running the example and persisting to HBase</h2>

<p>In order to run the example you&rsquo;ll have to run the following command: (you can use &mdash;hdfs instead of &mdash;local)
<code>bash
yarn jar scalding-correalation-1.0.jar com.sequenceiq.scalding.hbase.HBaseWriterJob --local --input data/corr-out.tsv --tableName corrTable --quorum localhost --quorumPort 2181
</code></p>

<p>Hope this correlation example and introduction into Scalding was useful &ndash; you can get the example project from our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/scalding-correlation">GitHub</a> repository.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Multi-node Hadoop cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/"/>
    <updated>2014-06-19T20:29:10+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker</id>
    <content type="html"><![CDATA[<p>In the <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/">previous post</a>
you saw how easy is to create a single-node Hadoop <em>cluster</em> on your devbox.</p>

<p>Now lets raise the bar and create a multinode Hadoop cluster on Docker. Before we
start, make sure you have the latest Ambari image:</p>

<p><code>
docker pull sequenceiq/ambari:latest
</code></p>

<h2>One-liner</h2>

<p>Once you have the latest image, you can start running Docker containers.
But instead of typing long commands like <code>docker run [options] image [command]</code>,
we have created a couple of <a href="https://github.com/sequenceiq/docker-ambari/blob/master/ambari-functions">shell functions</a> to help you with Docker commands.</p>

<p>Using these functions the impatient can provision a 3 node Hadoop cluster with this one-liner:
<code>
curl -Lo .amb j.mp/docker-ambari &amp;&amp; . .amb &amp;&amp; amb-deploy-cluster
</code></p>

<!-- more -->


<p>Note that you can always alter the default parameters as the blueprint, cluster size, etc &hellip; check the shell <code>j.mp/docker-ambari</code> function&rsquo;s head for the parameters list.</p>

<p>It does the following steps:</p>

<ul>
<li>runs <code>ambari-server start</code> in a daemon Docker (background) container (and also an <code>ambari-agent start</code>)</li>
<li>runs <code>n-1</code> daemon containers with <code>ambari-agent start</code> connecting to the server</li>
<li>runs AmbariShell with attached terminal (to see provision progress)

<ul>
<li>AmbariShell will post the built-in multi-node blueprint to <code>/api/v1/blueprints</code> REST API</li>
<li>AmbariShell auto-assign hosts to host_groups defined in the blueprint</li>
<li>creates a cluster, by posting to the <code>/api/v1/clusters</code> REST API</li>
</ul>
</li>
</ul>


<h2>Custom blueprint</h2>

<p>If you have your own blueprint, put it into a <a href="https://gist.github.com/">gist</a>
and you can use it from AmbariShell. First start AmbariShell:
<code>
amb-start-cluster 2
amb-shell
</code></p>

<p>AmbariShell will wait for:</p>

<ul>
<li>Ambari REST API
Below you will see a happy path to create a multi node Hadoop cluster using the AmbariShell.</li>
</ul>


<p><code>
host list
blueprint add --url https://gist.githubusercontent.com/lalyos/xxx/raw/custum-blueprint.json
cluster build --blueprint custom-blueprint
cluster assign --hostGroup host_group_1 --host amb0.mycorp.kom
cluster assign --hostGroup host_group_2 --host amb1.mycorp.kom
cluster assign --hostGroup host_group_2 --host amb1.mycorp.kom
cluster create
</code></p>

<p>In AmbariShell the <code>hint</code> command will always guide you on the happy path,
and remember that devops are lazy, so instead of typing press <code>&lt;TAB&gt;</code> for autocomplete or suggestions.</p>

<p>Autocomplete will help you to:</p>

<ul>
<li>complete the command in the given context (e.g. without any blueprint, cluster commands are not available)</li>
<li>add required parameters</li>
<li>add optional parameters: press tab after double dash <code>--&lt;TAB&gt;</code></li>
<li>complete parameter arguments, such as blueprint names, hostnames &hellip;</li>
</ul>


<h2>Summary</h2>

<p>Ever since we started to use Docker we are always developing against a multi-node
Hadoop cluster &ndash; as running a 3-4 node cluster in a laptop actually has less overhead
than working on a Sandbox VM.</p>

<p>We are <em>Dockerizing</em> the Hadoop ecosystem and simplifying the provisioning
process &ndash; watch this space or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>
for the latest news about <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> &ndash; the
open source cloud agnostic <em>Hadoop as a Service</em> API built on Docker.</p>

<p>Hope this helps and simplifies your development process &ndash; let us know how it goes
for you or if you need any help with Hadoop on Docker.</p>
]]></content>
  </entry>
  
</feed>
