<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-06-20T08:34:43+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Multi-node Hadoop cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/"/>
    <updated>2014-06-19T20:29:10+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker</id>
    <content type="html"><![CDATA[<p>In the <a href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/">previous post</a>
you saw how easy is to create a single-node Hadoop <em>cluster</em> on your devbox.</p>

<p>Now lets raise the bar and create a multinode Hadoop cluster on Docker. Before we
start, make sure you have the latest ambari image:</p>

<p><code>
docker pull sequenceiq/ambari:latest
</code></p>

<h2>One-liner</h2>

<p>Once you have the latest image, you can start runnin Docker containers.
But instead of typing long commands like <code>docker run [options] image [command]</code>,
we have created a couple of <a href="https://github.com/sequenceiq/docker-ambari/blob/master/ambari-functions">shell functions</a> to help you with Docker commands.</p>

<p>Using these functions the impatient can provision a 3 node Hadoop cluster with this one-liner:
<code>
curl -Lo .amb j.mp/docker-ambari &amp;&amp; . .amb &amp;&amp; amb-deploy-cluster
</code></p>

<!-- more -->


<p>It does the following steps:</p>

<ul>
<li>starts Ambari server in a daemon Docker (background) container (and also an Ambari-agent self connecting)</li>
<li>starts <code>n-1</code> daemon containers with <code>ambari-agent</code> connecting to the server</li>
<li>runs AmbariShell with attached terminal (to see provision progress)

<ul>
<li>AmbariShell will post the built-in multi-node blueprint to <code>/api/v1/blueprints</code> REST API</li>
<li>AmbariShell auto-assign hosts to host_groups defined in the blueprint</li>
<li>creates a cluster, by posting to the <code>/api/v1/clusters</code> REST API</li>
</ul>
</li>
</ul>


<h2>Custom blueprint</h2>

<p>If you have your own blueprint, put it into a <a href="https://gist.github.com/">gist</a>
and you can use it from AmbariShell. First start AmbariShell:
<code>
amb-start-cluster 2
amb-shell
</code></p>

<p>In AmbariShell the <code>hint</code> command will always guide you on the happy path,
and remember that devops are lazy, so instead of typing press <code>&lt;TAB&gt;</code> for autocomplete or suggestions.</p>

<p>Autocomplete will help you to:</p>

<ul>
<li>complete the command in the given context (e.g. without any blueprint, cluster commands are not available)</li>
<li>add required parameters</li>
<li>add optional parameters: pres tab after double dash <code>--&lt;TAB&gt;</code></li>
<li>complete parameter arguments, such as blueprint names, hostnames &hellip;</li>
</ul>


<p>Below you will see a happy path to create a multi node Hadoop cluster using the Ambari shell.</p>

<p><code>
host list
blueprint add --url https://gist.githubusercontent.com/lalyos/xxx/raw/custum-blueprint.json
cluster build --blueprint custom-blueprint
cluster assign --hostGroup host_group_1 --host amb0.mycorp.kom
cluster assign --hostGroup host_group_2 --host amb1.mycorp.kom
cluster assign --hostGroup host_group_2 --host amb1.mycorp.kom
cluster create
</code></p>

<p>Ever since we started to use Docker we are always developing against a multi-node Hadoop cluster &ndash; as running a 3-4 node cluster in a laptop actually has less overhead than working on a Sandbox VM.</p>

<p>We are <em>Dockerizing</em> the Hadoop ecosystem and simplifying the provisioning process &ndash; watch this space or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a> for the latest news about <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> &ndash; the open source cloud agnostic <em>Hadoop as a Service</em> API built on Docker.</p>

<p>Hope this helps and simplifies your development process &ndash; let us know how it goes for you or if you need any help with Hadoop on Docker.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ambari provisioned Hadoop cluster on Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/"/>
    <updated>2014-06-17T08:51:14+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker</id>
    <content type="html"><![CDATA[<p>We are getting close to release and open source our <strong>Docker-based Hadoop Provisioning</strong> project.
The <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">slides</a>
were presented recently at the <a href="http://hadoopsummit.org/san-jose/">Hadoop Summit</a>, and
there is an interest from the community to learn the technical details.</p>

<p>The project &ndash; called <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> &ndash; will provide a REST API to provision a Hadoop cluster &ndash; anywhere. The cluster can be hosted
on AWS EC2, Azure, physical servers or even your laptop &ndash; we are adding more providers &ndash; but always based on the same concept:
<a href="http://ambari.apache.org/">Apache Ambari</a> managed <a href="http://www.docker.com/">Docker</a>
containers.</p>

<p>This blog entry is the first in a series, where we describe the Docker layer step-by-step:</p>

<ul>
<li>Single-node Docker based Hadoop &ldquo;cluster&rdquo; locally</li>
<li>Multi-node Docker based Hadoop cluster</li>
<li>Multi-node Docker based Hadoop cluster on EC2</li>
<li>Cloudbreak</li>
</ul>


<h2>Get Docker</h2>

<p>The only required software is Docker, so if you don&rsquo;t have it yet, jump to the
installation section of the <a href="https://docs.docker.com/installation/">official documentation</a>.</p>

<p>The very basic you need to work with Docker containers, is described in the
<a href="https://docs.docker.com/userguide/dockerizing/">users guide</a>.</p>

<h2>Single-node Cluster</h2>

<p>All setup is based on <a href="https://hub.docker.com/u/sequenceiq/">Docker images</a> only
the glue-code is different. Let&rsquo;s start with the most simple setup:</p>

<ul>
<li>start the first Docker container in the background that runs <strong>ambari-server</strong> and <strong>ambari-agent</strong>.</li>
<li>start the second Docker container which:

<ul>
<li>waits for the agent connecting to the server</li>
<li>starts an <a href="https://github.com/sequenceiq/ambari-shell">ambari-shell</a>, which will instruct ambari-server on its REST API:

<ul>
<li>define an <strong><a href="https://cwiki.apache.org/confluence/display/AMBARI/Blueprints">Ambari Blueprint</a></strong> by posting a JSON to <code>&lt;AMBARI_URL&gt;/api/v1/blueprints</code></li>
<li>create a Hadoop cluster by posting a JSON to <code>&lt;AMBARI_URL&gt;/api/v1/clusters</code> using the blueprint created in the previous step</li>
</ul>
</li>
</ul>
</li>
</ul>


<p><code>
docker run -d -p 8080 -h amb0.mycorp.kom --name ambari-singlenode sequenceiq/ambari --tag ambari-server=true
docker run -e BLUEPRINT=single-node-hdfs-yarn --link ambari-singlenode:ambariserver -t --rm --entrypoint /bin/sh sequenceiq/ambari -c /tmp/install-cluster.sh
</code></p>

<p>or if you want a <strong>twitter-sized</strong> one-liner to start with Hadoop in less than a minute:</p>

<p><code>
curl -LOs j.mp/ambari-singlenode &amp;&amp; . ambari-singlenode
</code></p>

<!-- more -->


<p>When you pull the <code>sequenceiq/ambari</code> image first it will take a couple of minutes (for me it was 4 minutes).
Meanwhile you have started and running the download let&rsquo;s explain all those parameters.</p>

<h2>First container: ambari-server and ambari-agent</h2>

<p>Let&rsquo;s break down the parameters of the first container:
<code>
docker run -d -p 8080 -h amb0.mycorp.kom --name ambari-singlenode sequenceiq/ambari --tag ambari-server=true
</code></p>

<ul>
<li><strong>-d</strong> : Detached mode, container runs in the background</li>
<li><strong>-p 8080</strong> : Publish ambari web and REST API port</li>
<li><strong>-h amb0.mycorp.kom</strong> : hostname</li>
<li><strong>&mdash;name ambari-singlenode</strong> : assign a name to the container</li>
<li><strong>sequenceiq/ambari</strong> : the name of the image</li>
<li><strong>&mdash;tag ambari-server=true</strong> : the <em>command</em> but please note that this is appended to the <em>entrypoint</em>.</li>
</ul>


<p>The default <em>entrypoint</em> of the image is <code>start-serf-agent.sh</code>
<a href="https://github.com/sequenceiq/docker-ambari/blob/master/ambari-server/Dockerfile#L24">see the Dockerfile</a>
so the <code>--tag ambari-server=true</code> command is actually an argument of the <a href="http://www.serfdom.io/">serf agent</a>.</p>

<h3>Serf</h3>

<p>What is <a href="http://www.serfdom.io/">Serf</a>? The definition goes like:</p>

<blockquote><p>Serf is a decentralized solution for cluster membership, failure detection, and orchestration. Lightweight and highly available.</p></blockquote>

<p>Right now it doesn&rsquo;t seem to make any sense to talk about membership and cluster, but remember we want to
have the exact same process/tools for dev env and production.</p>

<p>The only Serf feature we use at this point is that you can define shell scripts based <strong>event-handlers</strong> for
each membership events:</p>

<ul>
<li>member-join</li>
<li>member-failed</li>
<li>member-leave</li>
<li>member-xxx</li>
</ul>


<p>The <strong>member-join</strong> event-handler script will check the Serf tags, defined by <code>--tag name=value</code>
and will start:
 &ndash; ambari-server java process: if the <strong>ambari-server</strong> tag is <strong>true</strong>
 &ndash; ambari-agent python process: if the <strong>ambari-agent</strong> tag is <strong>true</strong></p>

<p>You might noted that only the <strong>ambari-server</strong> tag is defined. The reason is that <strong>ambari-agent</strong> is defined as <strong>true</strong> by default.</p>

<h2>Second container: ambari-shell</h2>

<p><code>
docker run -e BLUEPRINT=single-node-hdfs-yarn --link ambari-singlenode:ambariserver -t --rm --entrypoint /bin/sh sequenceiq/ambari -c /tmp/install-cluster.sh
</code></p>

<ul>
<li><strong>-e BLUEPRINT=single-node-hdfs-yarn</strong> : the template to use for the cluster (single-node-hdfs-yarn/multi-node-hdfs-yarn/lambda-architecture) <a href="https://github.com/sequenceiq/ambari-rest-client/tree/master/src/main/resources/blueprints">see the blueprint JSON on GitHub</a></li>
<li><strong>&mdash;link ambari-singlenode:ambariserver </strong> :  it will make all exposed ports and the private IP of <code>ambari-singlenode</code> available as <code>AMBARISERVER_xxx</code> env variables</li>
<li><strong>-t</strong> : pseudo terminal, to see the progress</li>
<li><strong>&mdash;rm</strong> : remove the container once it&rsquo;s finished</li>
<li><strong>&mdash;entrypoint /bin/sh</strong> : the default entrypoint runs the shell in interactive mode, we want to overwrite it with a script specified as <code>/tmp/install-cluster.sh</code></li>
</ul>


<h1>Install completed</h1>

<p>Once Ambari-shell completed with the installation, you are ready to use it.
To find out the IP of the Ambari server run:</p>

<p><code>
docker inspect -f "" ambari-singlenode
</code></p>

<p>To start with you can browse ambari web ui on <code>port 8080</code>. The default username/password is admin/admin.</p>

<p>or if you can&rsquo;t reach directly the private IP of the container (windows users), use the port exposed to the host:
<code>
docker port ambari-singlenode 8080
</code></p>

<h1>Next steps</h1>

<p>In the upcomming blog posts we will do a multinode Hadoop cluster with the same toolset, so stay tuned &hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Summit 2014 - SequenceIQ slides]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/06/06/hadoop-summit-slides/"/>
    <updated>2014-06-06T13:42:11+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/06/06/hadoop-summit-slides</id>
    <content type="html"><![CDATA[<p>These are the slides of our presentation from the Hadoop Summit 2014, San Jose. We would like to thank all who have joined the session and the positive feedbacks we have received. This gives us a great confidence and validates our efforts that there is a great need to an easy and seamless Hadoop provisionig &ndash; let it be bare metal, cloud or other virtualizations.</p>

<p>Watch this space as <a href="http://docs.cloudbreak.apiary.io/">Cloudbreak</a> will be open sourced in the coming weeks.</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/35573123" width="640" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning" title="Docker based Hadoop provisioning - Hadoop Summit 2014 " target="_blank">Docker based Hadoop provisioning &ndash; Hadoop Summit 2014 </a> </strong> from <strong><a href="http://www.slideshare.net/JanosMatyas" target="_blank">Janos Matyas</a></strong> </div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building the build environment with Ansible and Docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker/"/>
    <updated>2014-05-09T11:51:57+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/05/09/building-the-build-environment-with-ansible-and-docker</id>
    <content type="html"><![CDATA[<p>At SequenceIQ we put a strong emphasis on automating everything we can and this automation starts with our continuous integration &amp; delivery process.</p>

<h2>Introduction</h2>

<p>Lately there is a lot of buzz around continuous integration, development and deployment. More and more companies are moving away from long release cycles towards the &ldquo;release early, release often&rdquo; approach. The advantages of this approach are well known: lower overhead, earlier bug discovery and bug fixing, fewer context switches for the developers to name just a few. There are very good resources to learn about these concepts &ndash; blog posts by different companies (e.g.: by <a href="http://techblog.netflix.com/2013/08/deploying-netflix-api.html">Netflix</a>) and of course the <a href="http://www.amazon.com/dp/0321601912">book</a> &lsquo;Continuous Delivery&rsquo; by Jez Humble and David Farley &ndash; we&rsquo;ll now try to add our own experiences as well.</p>

<p>We&rsquo;ll share two blog posts about our continuous delivery at SequenceIQ: the first one being an introductory post about some tools we use to make the whole process easier and more robust, the second one explains the <a href="http://scottchacon.com/2011/08/31/github-flow.html">flow</a> we use from committing changes to being the changes available in our different environments.</p>

<h2>Tools</h2>

<p>Our CI and CD process at SequenceIQ is based on Ansible, Jenkins and of course Docker.
When we started to build our own process, we decided that we don&rsquo;t want to commit the same mistake that a lot of companies make about their build environment. At these companies the build servers where Jenkins and/or the other build tools are installed are often prepared once in the far past by someone who probably doesn&rsquo;t work there anymore. It quickly becomes something that everyone is afraid to touch and just hope that it will work forever. As the projects improve there will be a lot of different tools with a lot of different versions on the build machine and soon it leads to a small chaos, where the maintenance will involve a lot of hard manual work. To get rid of these problems, we use <a href="http://www.ansible.com/">Ansible</a> to &ldquo;build the build infrastructure&rdquo;, and Docker to run the builds in separated self-sufficient containers.</p>

<!-- more -->


<h2>Ansible</h2>

<p>We have an Ansible script which starts an EC2 instance in the cloud and provisions everything on this server automatically. This script can be easily executed with a single command from a developer laptop:</p>

<p><code>
ansible-playbook -i hosts ci.yml
</code></p>

<p>To run this command Ansible, python and some python modules must be installed on the local machine. To avoid having different version of these tools on the development machines we automated the installation of our development environment too &ndash; maybe the topic of another post in the future.
So let&rsquo;s see how the Ansible script works exactly.</p>

<h3>Creating an instance in the cloud</h3>

<p>First it needs to start an instance in the AWS cloud, so it invokes our <strong>ec2 role</strong> on localhost:
```yaml
&ndash; name: Request and init EC2 instance
  hosts: localhost
  roles:</p>

<pre><code> - ec2
</code></pre>

<p>```</p>

<p>The ec2 role requests an EC2 spot priced instance and associates it with an elastic IP. We can easily use a spot priced instance because if it gets shut down by AWS we can recreate it in a few minutes! Ansible has a few <a href="http://docs.ansible.com/list_of_cloud_modules.html">cloud modules</a> which makes it quite easy to manage EC2 instances. Requesting a spot priced instance looks like this (the placeholders come from Ansible group variables):

<code>yaml
- name: Create an EC2 spot priced instance
  local_action:
  module: ec2
  key_name: "{{ ec2.keypair }}"
  group: "{{ ec2.security_group }}"
  instance_type: "{{ ec2.instance_type }}"
  spot_price: "{{ ec2.spot_price }}"
  image: "{{ ec2.image }}"
  wait: yes
  region: "{{ ec2.region }}"
  id: "{{ ec2.idempotent_id }}"
  register: ec2result
</code>
</p>

<h3>Provisioning the build server</h3>

<p>After the EC2 instance is running and accepting SSH connections, the script can go on and start to install the tools needed. Because almost everything is running in separated Docker containers, we only need 3 things: Docker, Nginx and Jenkins. Installing Docker is pretty easy as the new Amazon Linux AMIs are <a href="http://aws.amazon.com/amazon-linux-ami/2014.03-release-notes/">prepared</a> to run Docker. We only need to install it from Amazon&rsquo;s provided Software Repository, and start the service. It looks like this in the Ansible script:</p>

<p>```yaml
&ndash; name: Install Docker on Amazon Linux AMI
  when: ansible_os_family == &ldquo;RedHat&rdquo;
  yum: name=docker state=present</p>

<ul>
<li>name: Start Docker service
service: name=docker state=started
```</li>
</ul>


<p>After Docker is installed we can start some containers that are used by some Jenkins builds later (e.g.: a SonarQube server and a MySQL database that holds the results &ndash; we&rsquo;ve created publicly available <a href="https://index.docker.io/u/sequenceiq/sonar-server/">containers</a> on our Github page.</p>

<p>To install and configure Nginx we use an <a href="https://galaxy.ansible.com/list#/roles/466">existing role</a> from Ansible Galaxy that is well prepared and easily configurable. We are configuring Nginx to forward requests from port 80 to either Jenkins or Sonar.</p>

<p>For example the configuration for Jenkins is the following in the Ansible group variables:</p>

<p>```yaml
nginx_sites:
  default:</p>

<pre><code>- listen 80
- server_name jenkins.sequenceiq.com
- location / {
   proxy_pass http://jenkins;
   proxy_redirect off;
   proxy_set_header Host $host;
   proxy_set_header X-Forwarded-Host $server_name;
  }
</code></pre>

<p>nginx_configs:
  proxy:</p>

<pre><code>- proxy_set_header X-Real-IP $remote_addr
- proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for
</code></pre>

<p>  upstream:</p>

<pre><code>- upstream jenkins { server 127.0.0.1:8080 weight=10; }
</code></pre>

<p>```</p>

<p>The most difficult thing to install is Jenkins: we want our configurations and jobs to be instantly available as well. Jenkins has a <a href="https://wiki.jenkins-ci.org/display/JENKINS/Jenkins+CLI">command-line interface</a> that allows access from a script. It has a lot of built-in commands to manage jobs and other configurations and it even has a command to execute a Groovy script on the server. We use these features extensively to prepare the whole Jenkins environment from sketch. Our Jenkins role (that will be available soon on Ansible Galaxy) is able to do the following:
&ndash; Install Jenkins and its dependencies, and get the Jenkins CLI jar from the specified URL.
&ndash; Configure the global Jenkins properties like the mail server, or the properties needed for the Github pull request builder plugin &ndash; it is simply achieved by copying a global config.xml to the Jenkins home directory using Ansible&rsquo;s copy module.
&ndash; Install and update plugins through the Jenkins CLI. Installing plugins looks like this:</p>

<p>
<code>yaml
- name: Install plugins
  sudo: yes
  shell: java -jar {{ jenkins.cli_dest }} -s http://localhost:8080/ install-plugin {{item.item}}
  when: item.stdout.find('false') != -1
  with_items: check_plugins.results
  notify:
  - 'Restart Jenkins'
</code>
</p>

<ul>
<li>Configure security (we use Github OAuth). The Jenkins CLI doesn&rsquo;t have any dedicated commands for setting security, but it can be configured with a Groovy script that can be invoked from the CLI:</li>
</ul>


<p>```groovy
def githubSecurityRealm = new org.jenkinsci.plugins.GithubSecurityRealm(&ldquo;<a href="https://github.com">https://github.com</a>&rdquo;, &ldquo;<a href="https://api.github.com">https://api.github.com</a>&rdquo;, clientId, clientSecret)
def authorizationStrategy = new org.jenkinsci.plugins.GithubAuthorizationStrategy(&ldquo;admin1,admin2&rdquo;,true,&ldquo;organization name&rdquo;,true,false,false)
jenkins.model.Jenkins.instance.setSecurityRealm(githubSecurityRealm)
jenkins.model.Jenkins.instance.setAuthorizationStrategy(authorizationStrategy)
jenkins.model.Jenkins.instance.save()</p>

<p>```</p>

<ul>
<li><p>Copy private keys for Github builds &ndash; it simply copies the predefined private SSH keys from a local directory to the <code>~/.ssh</code> directory of the Jenkins user. We use a dedicated Github user to communicate with Github from Jenkins.</p></li>
<li><p>Creating jobs from XML configuration. The Jenkins CLI supports the creation of Jenkins jobs through the create-job command that accepts an XML file as input that defines the Jenkins job. Currently our Jenkins role works by invoking this command for every job that is defined in the variables and has a corresponding XML file in a predefined directory.  We are planning to later modify this role to have a template that holds the structure of a Jenkins job XML so it won&rsquo;t be needed to create the whole XML file manually, only the required parameters among the Ansible variables.</p></li>
</ul>


<p>
<code>yaml
- name: Create jenkins jobs
  shell: java -jar {{ jenkins.cli_dest }} -s http://localhost:8080/ create-job {{ item }} &lt; {{ jenkins.dest }}/{{item}}.xml
  with_items: jenkins_jobs
  when: existing_jobs.changed and existing_jobs.stdout.find('{{ item }}') == -1
</code>
</p>

<h2>Docker</h2>

<p>The other tool besides Ansible that we use extensively in our build environment is Docker. Docker is a quickly expanding technology that enables the creation of lightweight application containers. If you don&rsquo;t know about Docker yet, check out the official <a href="https://www.docker.io/gettingstarted/">Getting Started guide</a> or our own <a href="http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction/">blog post</a> about it.
With the help of Docker we don&rsquo;t need to worry about the tools needed for the builds or its dependencies on the continuous integration server as they are packaged in separate containers. Every one of our builds on Jenkins are only a few lines that runs a container, maybe copies something out of it and removes the container after it finished. We provide a few environment variables, some shared directories or some links between containers where needed. One of our jobs in Jenkins that builds the master branch looks like this:
```bash</p>

<h1>!/bin/bash</h1>

<p>docker run -i &mdash;name $BUILD_TAG \
-v &ldquo;/var/lib/jenkins/.gradle-api:/root/.gradle:rw&rdquo; \
-e &ldquo;SONAR_USERNAME=$SONAR_USERNAME&rdquo; \
-e &ldquo;SONAR_PW=$SONAR_PW&rdquo; \
-e &ldquo;BUILD_NUMBER=$BUILD_NUMBER&rdquo; \
-e &ldquo;KEY=$(cat /var/lib/jenkins/.ssh/id_rsa| base64 -w 0)&rdquo; \
-e &ldquo;REPO=$REPO_ADDRESS&rdquo; \
-e &ldquo;BRANCH=master&rdquo; \
-e &ldquo;BUILD_TASKS=clean build sonarRunner uploadArchives&rdquo; \
-e &ldquo;BUILD_ENV=jenkins&rdquo; \
-e &ldquo;GRADLE_OPTS=-XX:MaxPermSize=512m&rdquo; \
&mdash;link sonar_server:sonar \
&mdash;link sonar_mysql:sonar_db \
sequenceiq/build /etc/build-project.sh
sleep 5
docker cp $BUILD_TAG:/tmp/prj/build/build.info $WORKSPACE
docker rm $BUILD_TAG</p>

<p>```</p>

<p>And not only our builds run in Docker, some other tools we use on the build environment also run in containers. For instance our code quality management tool, SonarQube and the MySQL database it uses also runs in separate containers. This way we don&rsquo;t need to install them on the EC2 instance directly, we only need to link them where needed &ndash; see the example above!</p>

<p>In our next blog post about continuous integration we&rsquo;ll explain the process we use at SequenceIQ to continuously deliver the new features to production using the Github flow with Jenkins and Docker.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop on Docker introduction]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction/"/>
    <updated>2014-04-04T18:24:17+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction</id>
    <content type="html"><![CDATA[<p>In the last few weeks we&rsquo;ve created and published several Docker images (<a href="https://github.com/sequenceiq/hadoop-docker">Hadoop</a>, <a href="https://github.com/sequenceiq/hoya-docker">Hoya</a>, <a href="https://github.com/sequenceiq/tez-docker">Tez</a>) to help you to quick-start with Hadoop and the latest innovations using YARN.
While many people have downloaded and started to use these preconfigured images we&rsquo;ve been asked to give a short introduction of what Docker is, and how one can build Docker images. Also during the Hadoop Summit in Amsterdem we have been inquired in particular about running Hadoop on Docker, so this post is our answer for all the requests we received.</p>

<p>Docker is an open-source engine that automates the deployment of any application as a lightweight, portable, self-sufficient container that will run virtually anywhere.</p>

<h2>Installation</h2>

<p>First install Docker with a package manager. On Ubuntu there is an easy way to start with by running a simple curl script which will do it for you:
<code>curl -s https://get.docker.io/ubuntu/ | sudo sh</code>.
Unfortunately Mac, Windows and some Linux distributions cannot natively run Docker (yet). At <a href="http://sequenceiq.com/">SequenceIQ</a> we develop on OSX and run a 3-6 node Hadoop mini cluster on our laptops. To overcome the limitation of running Docker natively
you will have to install <code>boot2docker</code>. It is a Tiny Core Linux made specifically to run Docker containers and weights less than 24MB memory.
Initialize <em>(boot2docker init)</em> and start <em>(boot2docker up)</em> and you can SSH into the VM <em>(boot2docker ssh, pass: tcuser)</em>.</p>

<p>To verify the installation let&rsquo;s test it: <code>docker run ubuntu /bin/echo hello docker</code>. Docker did a bunch of things within seconds:</p>

<ul>
<li>Downloaded the base image from the docker.io index</li>
<li>Created a new LXC container</li>
<li>Allocated a filesystem for it</li>
<li>Mounted a read-write layer</li>
<li>Allocated a network interface</li>
<li>Setup an IP for it, with network address translation</li>
<li>Executed a process inside the container</li>
<li>Captured the output and printed it</li>
</ul>


<p>You can run an interactive shell as well <code>docker run -i -t ubuntu /bin/bash</code> and use this shell as you would use any other shell.</p>

<p>While there are lots of different Docker images available we would like to share how to create your own images.</p>

<!-- more -->


<h2>Dockerfile</h2>

<p>The <code>Dockerfile</code> describes the build steps and it can be viewed as an image representation. They provide a simple syntax for building images and
they are a great way to automate and script the images creation. Dockerfile instructions look like this:
<code>
INSTRUCTION arguments
</code></p>

<h3>FROM</h3>

<p>Every Dockerfile has to start with the <code>FROM image</code> instruction which sets the base image for subsequent instructions (e.g. in our <a href="https://github.com/sequenceiq/hoya-docker">Hoya</a> and <a href="https://github.com/sequenceiq/tez-docker">Tez</a> images we used our <a href="https://github.com/sequenceiq/hadoop-docker">Hadoop</a> image as a base, while the Hadoop image was built on top of the <code>tianon/centos</code> base image).
A base image is built from a trusted build (more on this later) and in case of Hoya and Tez the base image was: <code>sequenceiq/hadoop-docker</code>. You can browse the available containers in the
<a href="https://index.docker.io/">Docker index</a>.</p>

<h3>RUN</h3>

<p>The next instruction is usually the <code>RUN command</code>. This will execute any commands on the current image and commit the results. The resulting committed image
will be used for the next step in the Dockerfile. Example: RUN yum install -y openssh-server. One important thing to keep in mind is that the
following set of instructions will not act as we would like:
<code>
RUN cd /usr/local  
RUN mkdir apple  
</code>
This will create an apple folder in the root directory. Surprised, huh? The reason of this that the RUN command is equivalent to the docker commands:
docker run image command + docker commit container_id, where the image would be replaced automatically with the current image,
and container_id would be the result of the previous RUN instruction. But it doesn&rsquo;t mean it can&rsquo;t be done:
<code>
RUN cd /usr/local &amp;&amp; mkdir apple
</code></p>

<h3>ADD</h3>

<p>The <code>ADD from to</code> command will copy the specified file into the container. Example:
ADD data.xml /usr/local/data.xml. In this case the data.xml is in the same directory as the Dockerfile. After this command you can rely on that this file
is present in the container and you can use it as well: RUN rm /usr/local/data.xml.</p>

<h3>EXPOSE</h3>

<p>The <code>EXPOSE port</code> instruction sets ports to be exposed to the host when running the image. Example: EXPOSE 8080 80 22 50070</p>

<h3>ENV</h3>

<p>Setting an environment variable by running a RUN export KEY=value won&rsquo;t work in dockerland. Instead you can use the <code>ENV key value</code> instruction.
Example: ENV JAVA_HOME /usr/java/default</p>

<h3>ENTRYPOINT</h3>

<p>The <code>ENTRYPOINT [command]</code> instruction permits you to trigger a command as soon as the container starts. Example: ENTRYPOINT [&ldquo;echo&rdquo;, &ldquo;Whale you be my container&rdquo;]</p>

<p>There are more instructions, but these are enough to start with and build your own images.</p>

<h2>Build &amp; Trusted build</h2>

<p>Once the Dockerfile is ready you can build it. If the file is in the current directory build it with <code>docker build .</code> (-t name to TAG the image). It&rsquo;s possible
to create trusted builds. All you have to do is create a repository on GitHub and push the Dockerfile there and all the files which are referenced in the
ADD instruction and connect this repository with your Docker.io account. Docker.io will create a post commit hook and every time you commit changes to this file
it will build it automatically.</p>

<h2>Usage</h2>

<p>Use this environment variable to make things easier: export DOCKER_HOST=tcp://localhost:4243. Few frequently used commands:</p>

<ul>
<li>List of your local images: docker images</li>
<li>List of running containers: docker ps</li>
<li>List of all containers: docker ps -a</li>
</ul>


<p>After you built your image it should show in the image list, and ready to use. Run it with <code>docker run -i -t -P image_name /bin/bash</code>. The -P variable will
publish all exposed ports to the host interfaces.</p>

<h2>Complete example</h2>

<p>As a reference check out our Hadoop 2.3 based <a href="https://github.com/sequenceiq/hadoop-docker">Dockerfile</a>.</p>

<h2>OSX Tweaks</h2>

<h3>Passwordless ssh</h3>

<p>On OSX it&rsquo;s quite tedious to always type tcuser password when you ssh into boot2docker. You can install your public key with a oneliner. You have to set the
KEYCHAIN variable to your <a href="http://keychain.io">Keychain.io</a> registered email.
<code>
(export KEYCHAIN=&lt;email&gt;; curl -L j.mp/chain2docker|bash)
</code>
If you restart boot2docker, you have to run this command again, for a passwordless ssh. To install your public ssh key into keychain is as simple as:
<code>
curl -s ssh.keychain.io/&lt;email&gt;/upload | bash
</code>
than you will receive a confirmation email, that&rsquo;s all.</p>

<h3>Expose ports from boot2docker to host</h3>

<p>Let&rsquo;s say you have a docker image starting Hadoop Name Node on port 50070. When you start 3 images you will get something like this:</p>

<ul>
<li>instance1: 50070 &ndash;> 49153</li>
<li>instance1: 50070 &ndash;> 49154</li>
<li>instance1: 50070 &ndash;> 49155</li>
</ul>


<p>But all those 4915X ports are only available when you are inside of boot2docker. Now if you forward all 49XXX ports straight to to your host,
you can reach the namenodes in your browser running on your mac as: <a href="http://localhost:4915X">http://localhost:4915X</a>
<code>
boot2docker stop
for i in {49000..49900}; do
 echo -n .
 VBoxManage modifyvm "boot2docker-vm" --natpf1 "tcp-port$i,tcp,,$i,,$i";
 VBoxManage modifyvm "boot2docker-vm" --natpf1 "udp-port$i,udp,,$i,,$i";
done
boot2docker up
</code>
That&rsquo;s it. Hope this helps you to start with building your own Docker images. Let us know how it goes, we are happy to help you quick start Hadoop on Docker.</p>
]]></content>
  </entry>
  
</feed>
