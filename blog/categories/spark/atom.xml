<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Spark | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-09-03T10:05:28+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Submit a Spark job to YARN from code]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java/"/>
    <updated>2014-08-22T08:09:28+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java</id>
    <content type="html"><![CDATA[<p>In our previous Apache Spark related post we showed you how to write a simple machine learning job. In this post we’d like to show you how to submit a Spark job from code. At SequenceIQ we submit jobs to different clusters &ndash; based on load, customer profile, associated SLAs, etc. Doing this the <code>documented</code> way was cumbersome so we needed a way to submit Spark jobs (and in general all of our jobs running in a YARN cluster) from code. Also due to the <code>dynamic</code> clusters, and changing job configurations we can’t use hardcoded parameters &ndash; in a previous <a href="http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service/">blog post</a> we highlighted how are we doing all these.</p>

<h2>Business as usual</h2>

<p>Basically as you from the <a href="https://spark.apache.org/docs/1.0.1/submitting-applications.html">Spark documentation</a>, you have to use the <a href="https://github.com/apache/spark/blob/master/bin/spark-submit">spark-submit</a> script to submit a job. In nutshell SparkSubmit is called
by the <a href="https://github.com/apache/spark/blob/master/bin/spark-class">spark-class</a> script with a lots of decorated arguments. In our example we examine only the YARN part of the submissions.
As you can see in <a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala">SparkSubmit.scala</a> the YARN <a href="https://github.com/apache/spark/blob/master/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala">Client</a> is loaded and its main method invoked (based on the arguments of the script).</p>

<p>```scala</p>

<pre><code>// If we're deploying into YARN, use yarn.Client as a wrapper around the user class
if (!deployOnCluster) {
  childMainClass = args.mainClass
  if (isUserJar(args.primaryResource)) {
    childClasspath += args.primaryResource
  }
} else if (clusterManager == YARN) {
  childMainClass = "org.apache.spark.deploy.yarn.Client"
  childArgs += ("--jar", args.primaryResource)
  childArgs += ("--class", args.mainClass)
}

...
// Here we invoke the main method of the Client
val mainClass = Class.forName(childMainClass, true, loader)
val mainMethod = mainClass.getMethod("main", new Array[String](0).getClass)
try {
  mainMethod.invoke(null, childArgs.toArray)
} catch {
  case e: InvocationTargetException =&gt; e.getCause match {
    case cause: Throwable =&gt; throw cause
    case null =&gt; throw e
}
</code></pre>

<p>```
It’s a pretty straightforward way to submit a Spark job to a YARN cluster, though you will need to change manually the parameters which as passed as arguments.</p>

<!--more-->


<h2>Submitting the job from Java code</h2>

<p>In case if you would like to submit a job to YARN from Java code, you can just simply use this Client class directly in your application.
(but you have to make sure that every environment variable what you will need is set properly).</p>

<h3>Passing Configuration object</h3>

<p>In the main method the org.apache.hadoop.conf.Configuration object is not passed to the Client class. A <code>Configuration</code> is created explicitly in the constructor, which is actually okay (then client configurations are loaded from $HADOOP_CONF_DIR/core-site.xml and $HADOOP_CONF_DIR/yarn-site.xml).
But what if you want to use (for example) an <a href="http://blog.sequenceiq.com/blog/2014/07/09/ambari-configuration-service/">Ambari Configuration Service</a> for retrieve your configuration, instead of using hardcoded ones?</p>

<p>```scala</p>

<pre><code>... // Client class - constructor
  def this(clientArgs: ClientArguments, spConf: SparkConf) =
    this(clientArgs, new Configuration(), spConf)

... // Client object - main method
System.setProperty("SPARK_YARN_MODE", "true")
val sparkConf = new SparkConf()

try {
  val args = new ClientArguments(argStrings, sparkConf)
  new Client(args, sparkConf).run()
} catch {
  case e: Exception =&gt; {
    Console.err.println(e.getMessage)
    System.exit(1)
  }
}

System.exit(0)
</code></pre>

<p>```</p>

<p>Fortunately, the configuration can be passed here (there is a <code>Configuration</code> field in the Client), but you have to write your own main method.</p>

<h3>Code example</h3>

<p>In our example we also use the 2 client XMLs as configuration (for demonstration purposes only), the main difference here is that we read the properties from the XMLs and filling them in the Configuration. Then we pass the Configuration object to the Client (which is directly invoked here).</p>

<p>```scala
 def main(args: Array[String]) {</p>

<pre><code>val config = new Configuration()
fillProperties(config, getPropXmlAsMap("config/core-site.xml"))
fillProperties(config, getPropXmlAsMap("config/yarn-site.xml"))

System.setProperty("SPARK_YARN_MODE", "true")

val sparkConf = new SparkConf()
val cArgs = new ClientArguments(args, sparkConf)

new Client(cArgs, config, sparkConf).run()
</code></pre>

<p>  }
```</p>

<p>To build the project use this command from the spark-submit directory:</p>

<p><code>bash
./gradlew clean build
</code></p>

<p>After building it you find the required jars in spark-submit-runner/build/libs (<code>uberjar</code> with all required dependencies) and spark-submit-app/build/libs. Put them in the same directory (do this also with this <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/spark-submit/spark-submit-runner/src/main/resources">config folder</a> too). After that run this command:</p>

<p><code>bash
java -cp spark-submit-runner-1.0.jar com.sequenceuq.spark.SparkRunner \
  --jar spark-submit-app-1.0.jar \
  --class com.sequenceiq.spark.Main \
  --driver-memory 1g \
  --executor-memory 1g \
  --executor-cores 1 \
  --arg hdfs://sandbox:9000/input/sample.txt \
  --arg /output \
  --arg 10 \
  --arg 10
</code></p>

<p>During the submission note that: not just the app jar, but the spark-submit-runner jar is also uploaded (which is an <code>uberjar</code>) to the HDFS. To avoid this, you have to upload it to the HDFS manually and set the <strong>SPARK_JAR</strong> environment variable.</p>

<p><code>bash
export SPARK_JAR="hdfs:///spark/spark-submit-runner-1.0.jar"
</code></p>

<p>If you get &ldquo;Permission denied&rdquo; exception on submit, you should set the <strong>HADOOP_USER_NAME</strong> environment variable to root (or something with proper rights).</p>

<p>As usual for us we ship the code &ndash; you can get it from our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/spark-submit">GitHub</a> samples repository; the sample input is available <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/spark-clustering/data/input.txt">here</a>.</p>

<p>If you would like to play with Spark, you can use our <a href="https://registry.hub.docker.com/u/sequenceiq/spark/">Spark Docker container</a> available as a trusted build on Docker.io repository.</p>

<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark - MLlib Introduction]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/31/spark-mllib/"/>
    <updated>2014-07-31T07:47:32+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/31/spark-mllib</id>
    <content type="html"><![CDATA[<h3>Introduction</h3>

<p>In one of our earlier posts we have mentioned that we use Scalding (among others) for writing MR jobs. Scala/Scalding simplifies the implementation of many MR patterns and makes it easy to implement quite complex jobs like machine learning algorithms. Map Reduce is a mature and widely used framework and it is a good choice for processing large amounts of data &ndash; but not as great if you’d like to use it for fast iterative algorithms/processing. This is a use case where <a href="https://spark.apache.org/">Apache Spark</a> can be quite handy. Spark is fit for these kind of algorithms, because it tries to keep everything in memory (in case of you run out of memory, you can switch to another <a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence">storage levels</a>).</p>

<h3>Apache Spark &ndash; MLlib library</h3>

<p><a href="https://spark.apache.org/docs/latest/mllib-guide.html">MLlib</a> is a machine learning library which ships with Apache Spark, and can run on any Hadoop2/YARN cluster without any pre-installation. At SequenceIQ we use MLlib in Scala &ndash; but you could use it from Java and Python as well. Let us quickly show you an MLlib clustering algorithm with code examples.</p>

<h3>KMeans example</h3>

<p>K-Means (Lloyd&rsquo;s algorithm) is a simple NP-hard unsupervised learning algorithm that solve well known clustering problems. The essence of the algorithm is to separate your data into K cluster. In simple terms it needs 4 steps. First of all you have to vectorize your data. (you can do that with text values too). The code looks like this:</p>

<p>```scala</p>

<pre><code>val data = context.textFile(input).map {
  line =&gt; Vectors.dense(line.split(',').map(_.toDouble))
}.cache()
</code></pre>

<p>```</p>

<!-- more -->


<p>The second step is to choose K center points (centroids). The third one is to assign each vector to the group that has the closest centroid. After all this is done, next thing you will need to do is to recalculate the positions of the centroids. You have to repeat the third and fourth steps until the centroids are not moving (<code>the iterative stuff</code>). The <a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/clustering/KMeans.scala">KMeans</a> MLlib model is doing that for you.</p>

<p>```scala</p>

<pre><code>val clusters: KMeansModel = KMeans.train(data, K, maxIteration, runs)

val vectorsAndClusterIdx = data.map{ point =&gt;
  val prediction = clusters.predict(point)
  (point.toString, prediction)
}
</code></pre>

<p>```
After you have your model result, you can utilize it in your RDD object.</p>

<h3>Running Spark job on YARN</h3>

<p>In order to run this Spark application on YARN first of all you will need a Hadoop YARN cluster. For that you could use our Hadoop as a Service API called <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a> &ndash; using a <code>multi-node-hdfs-yarn</code> blueprint will set you up a Spark ready Hadoop cluster in less than 2 minutes on your favorite cloud provider. Give it a try at our hosted <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> instance.</p>

<p>Once your cluster it’s up and ready you can run the following command:</p>

<p><code>bash
./bin/spark-submit --class com.sequenceiq.spark.Main --master \
yarn-client --driver-memory 1g --executor-memory 1g --executor-cores 1 \
/root/spark-clustering-1.0.jar hdfs://sandbox:9000/input/input.txt /output 10 10 1
</code>
Alternatively you can run this in our free Docker based Apache Spark container as well. You can get a Spark container from the official <a href="https://registry.hub.docker.com/u/sequenceiq/spark/">Docker registry</a> or from our <a href="https://github.com/sequenceiq/docker-spark">GitHub</a> repository.
As always we are making the source code available at <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/spark-clustering">SequenceIQ&rsquo;s GitHub repository</a> (check the other interesting examples as well).  You can find 2 simple input datasets for testing purposes.</p>

<p>The result of the clustering looks like this (generated with R):</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/spark-clustering/data/spark-clustering_1.jpeg" alt="" /></p>

<p>While there is a loud buzz about what’s faster than the other and there are huge numbers thrown in as the <em>X</em> multiplier factor we don’t really want to enter that game &ndash; as a fact we’d like to mention that both example performs better than Mahout KMeans (2-3x faster with 20 iterations), but these are really small datasets. We have seen larger datasets in production where the performances are quite the same, or can go the other way (especially that Spark is new and people don’t always get the configuration right).</p>

<p>In one of our next post we will show you metrics for a much larger dataset and other ML algorithms &ndash; follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a> for updates.</p>

<h3>Apache Tez</h3>

<p>We can’t finish this blog post before not talking about <a href="http://tez.apache.org/">Apache Tez</a> &ndash; the project is aimed at building an application framework which allows for a complex directed-acyclic-graph of tasks for processing data &ndash; fast. We (and many others) believe that this can be a good alternative for Spark &ndash; especially for machine learning. The number of frameworks which are adding or moving the MR runtime to Tez is increasing &ndash; among the few to mention are Cascading, Summingbird, Conjecture &ndash; including us as well.</p>

<p>Note that Apache Tez has already showed <strong>awesome</strong> result. Being the key building block of the <a href="http://hortonworks.com/labs/stinger/">Stinger inititive</a> &ndash; led by Hortonworks &ndash; managed to bring near real time queries and speed up Hive with 100x.</p>

<h3>Other promising machine learning frameworks</h3>

<p>If you are interested in machine learning frameworks, you have to check  <a href="https://github.com/etsy/Conjecture">Conjecture</a> or <a href="https://github.com/tresata/ganitha">ganitha</a> &ndash; they both show great fueatures and have promising results.</p>
]]></content>
  </entry>
  
</feed>
