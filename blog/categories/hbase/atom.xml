<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: HBase | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/hbase/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-06-20T05:52:42+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[SQL on HBase with Apache Phoenix]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/04/22/sql-on-hbase-with-apache-phoenix/"/>
    <updated>2014-04-22T12:24:11+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/04/22/sql-on-hbase-with-apache-phoenix</id>
    <content type="html"><![CDATA[<p>At <a href="http://sequenceiq.com/">SequenceIQ</a> we use HBase to store large amounts of high velocity data and interact with them &ndash; many times we use native HBase interfaces but recently there was a need (internal and external) to access the data through an SQL interface.</p>

<h2>Introduction</h2>

<p>HBase is an open-source, distributed, versioned, non-relational database modeled after Google&rsquo;s Bigtable. It&rsquo;s designed to handle
billions of rows and millions of columns. However, using it as a relational database where you would store your data normalized,
split into multiple tables is not easy and most likely you will struggle with it as you would do in any other non-relational database.
Here comes <a href="http://phoenix.incubator.apache.org/">Apache Phoenix</a> in the picture. It&rsquo;s an SQL skin over HBase delivered as a
client-embedded JDBC driver targeting low latency queries. The project is in incubating state and under heavy development, but you
can already start embracing it.</p>

<h2>Installation</h2>

<p>Download the appropriate distribution from <a href="http://xenia.sote.hu/ftp/mirrors/www.apache.org/incubator/phoenix/">here</a>:</p>

<ul>
<li>Phoenix 2.x &ndash; HBase 0.94.x</li>
<li>Phoenix 3.x &ndash; HBase 0.94.x</li>
<li>Phoenix 4.x &ndash; HBase 0.98.1+</li>
</ul>


<p><em>Note the compatibilities between the HBase and Phoenix versions</em></p>

<p>Alternatively you can clone the <a href="https://github.com/apache/incubator-phoenix/tree/4.0">repository</a> and build it yourself (mvn clean install -DskipTests).
It should produce a jar file like this: phoenix-<code>version</code>-incubating-client.jar. Copy it to HBase&rsquo;s classpath (easiest way is to copy into
HBASE_HOME/lib). If you have multiple nodes it has to be there on every node. Restart the RegionServers and you are good to go. That&rsquo;s it?
Yes!</p>

<h2>Sample</h2>

<p>We&rsquo;ve pre-cooked a <a href="https://github.com/sequenceiq/phoenix-docker">Docker</a> image for you so you can follow this sample and play with it:
(the image is based on Hadoop 2.4, HBase 0.98.1, Phoenix 4.1.0-SNAPSHOT) <code>docker run -i -t sequenceiq/phoenix</code>.</p>

<!-- more -->


<h3>Create tables</h3>

<p>The downloaded or built distribution&rsquo;s bin directory contains a pure-Java console based utility called sqlline.py. You can use this
to connect to HBase via the Phoenix JDBC driver. You need to specify the Zookeeper&rsquo;s QuorumPeer&rsquo;s address. If the default (2181) port is
used then type <em>sqlline.py localhost</em> (to quit type: !quit). Let&rsquo;s create two different tables:
<code>mysql
CREATE TABLE CUSTOMERS (ID INTEGER NOT NULL PRIMARY KEY, NAME VARCHAR(40) NOT NULL, AGE INTEGER NOT NULL, CITY CHAR(25));
CREATE TABLE ORDERS (ID INTEGER NOT NULL PRIMARY KEY, DATE DATE, CUSTOMER_ID INTEGER, AMOUNT DOUBLE);
</code>
It&rsquo;s worth checking which <a href="http://phoenix.incubator.apache.org/language/datatypes.html">datatypes</a> and
<a href="http://phoenix.incubator.apache.org/language/index.html">functions</a> are currently supported. These tables will be translated into
HBase tables and the metadata is stored along with it and versioned, such that snapshot queries over prior versions will automatically
use the correct schema. You can check with HBase shell as <code>describe 'CUSTOMERS'</code>
```
DESCRIPTION                                                                                                                         ENABLED
 &lsquo;CUSTOMERS&rsquo;, {TABLE_ATTRIBUTES => {coprocessor$1 => &lsquo;|org.apache.phoenix.coprocessor.ScanRegionObserver|1|&rsquo;, coprocessor$2 => &lsquo;|or true
 g.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|1|&rsquo;, coprocessor$3 => &lsquo;|org.apache.phoenix.coprocessor.GroupedAggreg
 ateRegionObserver|1|&rsquo;, coprocessor$4 => &lsquo;|org.apache.phoenix.coprocessor.ServerCachingEndpointImpl|1|&rsquo;, coprocessor$5 => &lsquo;|org.apa
 che.phoenix.hbase.index.Indexer|1073741823|index.builder=org.apache.phoenix.index.PhoenixIndexBuilder,org.apache.hadoop.hbase.inde
 x.codec.class=org.apache.phoenix.index.PhoenixIndexCodec&rsquo;}, {NAME => &lsquo;0&rsquo;, DATA_BLOCK_ENCODING => &lsquo;FAST_DIFF&rsquo;, BLOOMFILTER => &lsquo;ROW&rsquo;
 , REPLICATION_SCOPE => &lsquo;0&rsquo;, VERSIONS => &lsquo;1&rsquo;, COMPRESSION => &lsquo;NONE&rsquo;, MIN_VERSIONS => &lsquo;0&rsquo;, TTL => &lsquo;2147483647&rsquo;, KEEP_DELETED_CELLS =</p>

<blockquote><p>&lsquo;true&rsquo;, BLOCKSIZE => &lsquo;65536&rsquo;, IN_MEMORY => &lsquo;false&rsquo;, BLOCKCACHE => &lsquo;true&rsquo;}
<code>``
As you can see there are bunch of co-processors. Co-processors were introduced in version 0.92.0 to push arbitrary computation out
to the HBase nodes and run in parallel across all the RegionServers. There are two types of them:</code>observers<code>and</code>endpoints`.
Observers allow the cluster to behave differently during normal client operations. Endpoints allow you to extend the clusterâ€™s
capabilities, exposing new operations to client applications. Phoenix uses them to translate the SQL queries to scans and that&rsquo;s
why it can operate so quickly. It is also possible to map an existing HBase table to a Phoenix table. In this case the binary
representation of the row key and key values must match one of the Phoenix data types.</p></blockquote>

<h3>Load data</h3>

<p>After the tables are created fill them with data. For this purpose we&rsquo;ll use the <a href="http://www.jooq.org/">Jooq</a> library&rsquo;s fluent API.
The related sample project (Spring based) can be found in our
<a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/phoenix-jooq">GitHub</a> repository. To connect you&rsquo;ll need Phoenix&rsquo;s
JDBC driver on your classpath (org.apache.phoenix.jdbc.PhoenixDriver). At the moment it is not available anywhere, but temporary we
deployed into our <a href="https://github.com/sequenceiq/sequenceiq-maven-repo">maven</a> repository, so use it if you&rsquo;d like, but don&rsquo;t rely on that
it will be always there in this form. The url to connect to should be familiar as it uses the same Zookeeper QuorumPeer&rsquo;s address:
<code>jdbc:phoenix:localhost:2181</code>. Unfortunately Jooq&rsquo;s insert statement is not suitable for us since the JDBC driver only supports the
upsert statement so we cannot make use of the fluent API here.
```java
String userSql = String.format(&ldquo;upsert into customers values (%d, &lsquo;%s&rsquo;, %d, &lsquo;%s&rsquo;)&rdquo;,</p>

<pre><code>                i,
                escapeSql(names.get(random.nextInt(names.size() - 1))),
                random.nextInt(40) + 18,
                escapeSql(locales[random.nextInt(locales.length - 1)].getDisplayCountry()));
</code></pre>

<p>String orderSql = String.format(&ldquo;upsert into orders values (%d, CURRENT_DATE(), %d, %d)&rdquo;,</p>

<pre><code>                i,
                i,
                random.nextInt(1_000_000));
</code></pre>

<p>dslContext.execute(userSql);
dslContext.execute(orderSql);
```</p>

<h3>Query</h3>

<p>On the generated data let&rsquo;s create queries:
```java
dslContext</p>

<pre><code>      .select()
      .from(tableByName("customers").as("c"))
      .join(tableByName("orders").as("o")).on("o.customer_id = c.id")
      .where(fieldByName("o.amount").lessThan(amount))
      .orderBy(fieldByName("c.name").asc())
      .fetch();
</code></pre>

<p><code>
This query resulted the following:
</code>
+&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;+
|C.ID|C.NAME      |C.AGE|C.CITY     |O.ID|O.DATE    |O.CUSTOMER_ID|O.AMOUNT|
+&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;+
| 976|Bogan, Elias|   26|Japan      | 976|2014-04-20|          976|  8664.0|
| 827|Constrictor |   29|{null}     | 827|2014-04-20|          827|  7856.0|
| 672|Hardwire    |   31|Tunisia    | 672|2014-04-20|          672|  9292.0|
| 746|Lady Killer |   37|Cyprus     | 746|2014-04-20|          746|  1784.0|
| 242|Lifeforce   |   35|Switzerland| 242|2014-04-20|          242|  5406.0|
| 487|Topspin     |   48|{null}     | 487|2014-04-20|          487|  6512.0|
+&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;+&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;+
<code>
The same thing could've been achieved with sqlline also.
</code>mysql
select c.name as name, o.amount as amount, o.date as date from customers as c inner join orders as o on o.id = c.id where o.amount &lt; 10000;
```
Nested queries are not supported yet, but it will come soon.</p>

<h2>Summary</h2>

<p>As you saw it is pretty easy to get started with Phoenix both command line and programmatically. There are lots of lacking features, but
the contributors are dedicated and working hard to make this project moving forward. Next step? ORM for HBase? It is also ongoing.. :)</p>

<p>Follow up with <a href="https://www.linkedin.com/company/sequenceiq/">us</a> if you are interested in HBase and building an SQL interface on top.
Don&rsquo;t hesitate to contact us should you have any questions.</p>

<p><a href="http://sequenceiq.com/">SequenceIQ</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Phoenix (sneak peak)]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/04/17/apache-phoenix-sneak-peak/"/>
    <updated>2014-04-17T13:51:08+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/04/17/apache-phoenix-sneak-peak</id>
    <content type="html"><![CDATA[<script type="text/javascript" src="https://asciinema.org/a/8982.js" id="asciicast-8982" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Hortonworks Hoya at SequenceIQ]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/24/hoya-at-sequenceiq/"/>
    <updated>2014-03-24T03:54:09+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/24/hoya-at-sequenceiq</id>
    <content type="html"><![CDATA[<p>With this blog post we are starting a series of articles where we&rsquo;d like to describe how we use YARN, why it is central to our product stack and why we believe that Hortonworks Hoya will be a determining building block in the Hadoop ecosystem.</p>

<p>While we don&rsquo;t want to get in details about <a href="http://hortonworks.com/hadoop/yarn/">YARN</a>, we&rsquo;d like to briefly explain the advantages of running an application on YARN, and introduce you to <a href="https://github.com/hortonworks/hoya">Hortonworks Hoya</a>.</p>

<p>At SequenceIQ we are building a multi-tenant, scale on demand data platform, with unpredictable batch and streaming workloads.
Before YARN we have tried different cluster management frameworks with Hadoop and managed to have pretty good results with <a href="http://aws.amazon.com/autoscaling/">Amazon EC2 Autoscaling groups</a>. Being true believers in open source and the need to diversify of provisioning Hadoop on different environments we needed to find an open source and &lsquo;standardised&rsquo; solution &ndash; welcome YARN.
(for example we provision Hadoop on <a href="http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker/">Docker</a>).</p>

<p>YARN separates the processing engine from the resource management &ndash; and acting effectively as an OS for Hadoop.
With YARN, you can now run multiple applications in Hadoop, all sharing a common resource management and improving cluster utilisation (we will release some metrics soon).
YARN also provides the following features out of the box:</p>

<ul>
<li>Multi-tenancy</li>
<li>Management and monitoring</li>
<li>High availability</li>
<li>Security</li>
<li>Failover and recovery</li>
</ul>


<p>All of the above and the effort of the Hadoop community and the wide adoption convinced us to start implementing our platform to run on top of YARN.
During our proof of concepts we went as far as starting all our non Hadoop (and not YARN compatible) applications on YARN &ndash; using <a href="https://github.com/hortonworks/hoya">Hoya</a>.</p>

<p>Hoya was introduced by Hortonworks mid last year &ndash; with the purpose to create Apache HBase clusters on YARN (since than it supports Apache Accumulo as well).
The code evolved pretty fast and now Hoya is a framework/application which allows you to deploy existing distributed applications on YARN &ndash; and benefit all the nice features of YARN.</p>

<p>In order to support different applications Hoya has a plugin provider architecture (supported plugins are in the <em>org.apache.hoya.providers</em> package).
Once a plugin is implemented (pretty straightforward, took us a few days only to understand and build a Flume and Tomcat plugin), the application is started in a YARN container and is monitored and controlled by YARN/Hoya.
The clusters can be started, stopped, frozen and re-sized dynamically &ndash; and in case of container failures Hoya deploys a replacement.</p>

<p>For a better architectural understanding of Hoya please read the following blog <a href="http://hortonworks.com/blog/hoya-hbase-on-yarn-application-architecture/">post</a>.</p>

<p>In this first post we would like to help you to get familiar with the benefits offered by Hoya and start an HBase cluster, re-scale it dynamically, freeze and stop.
First and foremost you will need an installation of Hadoop (2.3), the latest Hoya release (0.13.1) and HBase (0.98).
For your convenience we have put together an automated install script which lets you start with Hoya in a few minutes.</p>

<p>The script is available from our <a href="https://github.com/sequenceiq/hoya-docker/blob/master/hoya-centos-install.sh">GitHub page</a>. Also an official docker.io image is available at <a href="https://index.docker.io/u/sequenceiq/hoya-docker/">hoya-docker</a>, and the Dockerfile can be downloaded from our <a href="https://github.com/sequenceiq/hoya-docker">GitHub</a> page.</p>

<p>Once Hadoop, HBase and Hoya are installed you can create an HBase cluster.
``` bash
create-hoya-cluster() {
  hoya create hbase &mdash;role master 1 &mdash;role worker 1</p>

<pre><code>--manager localhost:8032
--filesystem hdfs://localhost:9000 --image hdfs://localhost:9000/hbase.tar.gz
--appconf file:///tmp/hoya-master/hoya-core/src/main/resources/org/apache/hoya/providers/hbase/conf
--zkhosts localhost
</code></pre>

<p>}
```
This will launch a 2 node HBase cluster (1 Master and 1 RegionServer). Now lets increase the number of RegionServers.</p>

<p><code>bash
flex-hoya-cluster() {
  num_of_workers=$1
  hoya flex hbase --role worker $num_of_workers --manager localhost:8032 --filesystem hdfs://localhost:9000
}
</code></p>

<!-- more -->


<p>This will start as many RegionServers as specified &ndash; in new YARN containers. Also the size of the cluster can be decreased if the load on the system does not demand for a larger number of RegionServers. The cluster can also be freezed (Hoya takes care about persisting the state).</p>

<p><code>bash
freeze-hoya-cluster() {
  hoya freeze hbase --manager localhost:8032 --filesystem hdfs://localhost:9000
}
</code></p>

<p>Finally when you&rsquo;d like to destroy the cluster and the state associated with the application you can use:</p>

<p><code>bash
destroy-hoya-cluster() {
  hoya destroy hbase --manager localhost:8032 --filesystem hdfs://localhost:9000
}
</code>
As you see installing Hoya and starting different applications (HBase in this case) is very simple &ndash; and all the nice features of YARN are instantly available for any clustered applications.
In our next post we will drive you through the steps of creating your own Hoya provider, deploy it and run on a YARN cluster.</p>
]]></content>
  </entry>
  
</feed>
