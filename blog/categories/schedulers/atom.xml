<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Schedulers | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/schedulers/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-09-09T12:18:21+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[SLA policies for autoscaling Hadoop clusters]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/09/01/sla-samples-periscope/"/>
    <updated>2014-09-01T16:37:37+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/09/01/sla-samples-periscope</id>
    <content type="html"><![CDATA[<p>Last week we have <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/">announced</a> and open sourced <a href="http://sequenceiq.com/periscope/">Periscope</a> &ndash; the industry’s first SLA policy based autoscaling API for Hadoop YARN clusters. In this post we’d like to come up with some examples, setting up alarms and attach scaling policies to your Hadoop cluster.</p>

<p>Periscope is built on existing (and coming/contributed by us) features provided by Apache Hadoop, YARN, Ambari, Docker containers and SequenceIQ’s <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>. Just FYI, <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> is our open source and cloud agnostic Hadoop as a Service API, built on Docker containers. While Periscope can attach scaling policies to <code>static</code> and <code>dynamic</code> clusters &ndash; in this post we’d like to emphasize Periscope’s capabilities while working with &mdash; `dynamic &ndash; cloud based Hadoop deployments  &ndash; such as Hadoop clusters deployed with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a>.</p>

<p>SLAs policies are configured based on <code>alarms</code>, whereas an alarm is created based on <code>metrics</code> &ndash; these entities are explained below.</p>

<h2>Alarms</h2>

<p>An alarm watches a <code>metric</code> over a specified time period, and used by one or more action or scaling policy based on the value of the metric relative to a given threshold over the time period. A few of the supported <code>metrics</code> are listed below:</p>

<p>*<code>PENDING_CONTAINERS</code>&ndash; pending YARN containers</p>

<p>*<code>PENDING_APPLICATIONS</code> &ndash; pending/queued YARN applications</p>

<p>*<code>LOST_NODES</code> &ndash; cluster nodes lost</p>

<p>*<code>UNHEALTHY_NODES</code> &ndash; unhealthy cluster nodes</p>

<p>*<code>GLOBAL_RESOURCES</code> &ndash; global resources</p>

<!--more-->


<p>Measured <code>metrics</code> are compared with pre-configured values using operators. The <code>comparison operators</code> are: <code>LESS_THAN</code>, <code>GREATER_THAN</code>, <code>LESS_OR_EQUAL_THAN</code>, <code>GREATER_OR_EQUAL_THAN</code>, <code>EQUALS</code>.
In order to avoid reacting for sudden spikes in the system and apply policies only in case of a sustained system stress, <code>alarms</code> have to be sustained over a <code>period</code> of time.  The <code>period</code> specifies the time period in minutes during the alarm has to be sustained. Also a <code>threshold</code> can be configured, which specifies the variance applied by the operator for the selected <code>metric</code>.</p>

<p>For the <code>alarm</code> related REST operations you can check the <a href="http://docs.periscope.apiary.io/reference/alarms">API</a> documentation. Alarms can issue <code>notifications</code> as well &ndash; for example if a metric is reached for the configured time and threshold a notification event is raised &ndash; in the given example below this notification is an email.</p>

<p>```</p>

<h1>set metric alarms</h1>

<p>curl -X POST -H &ldquo;Content-Type: application/json&rdquo; -d &lsquo;{&ldquo;alarms&rdquo;:[{&ldquo;alarmName&rdquo;:&ldquo;pendingContainerHigh&rdquo;,&ldquo;description&rdquo;:&ldquo;Number of pending containers is high&rdquo;,&ldquo;metric&rdquo;:&ldquo;PENDING_CONTAINERS&rdquo;,&ldquo;threshold&rdquo;:10,&ldquo;comparisonOperator&rdquo;:&ldquo;GREATER_THAN&rdquo;,&ldquo;period&rdquo;:1},{&ldquo;alarmName&rdquo;:&ldquo;freeGlobalResourcesRateLow&rdquo;,&ldquo;description&rdquo;:&ldquo;Low free global resource rate&rdquo;,&ldquo;metric&rdquo;:&ldquo;GLOBAL_RESOURCES&rdquo;,&ldquo;threshold&rdquo;:1,&ldquo;comparisonOperator&rdquo;:&ldquo;EQUALS&rdquo;,&ldquo;period&rdquo;:1,&ldquo;notifications&rdquo;:[{&ldquo;target&rdquo;:[“<a href="&#109;&#97;&#105;&#x6c;&#x74;&#111;&#x3a;&#109;&#105;&#99;&#107;&#x2e;&#x66;&#x61;&#110;&#x6e;&#105;&#x6e;&#103;&#64;&#x61;&#x73;&#112;&#119;&#x6f;&#x72;&#108;&#x64;&#x74;&#111;&#x75;&#x72;&#46;&#x63;&#x6f;&#109;">&#109;&#105;&#99;&#107;&#46;&#102;&#x61;&#110;&#x6e;&#105;&#x6e;&#x67;&#x40;&#97;&#x73;&#112;&#x77;&#111;&#114;&#108;&#x64;&#116;&#x6f;&#117;&#x72;&#46;&#x63;&#x6f;&#109;</a>"],&ldquo;notificationType&rdquo;:&ldquo;EMAIL&rdquo;}]}]}&rsquo; localhost:8081/clusters/1/alarms | jq .
curl -X PUT -H &ldquo;Content-Type: application/json&rdquo; -d &lsquo;{&ldquo;alarmName&rdquo;:&ldquo;unhealthyNodesHigh&rdquo;,&ldquo;description&rdquo;:&ldquo;Number of unhealthy nodes is high&rdquo;,&ldquo;metric&rdquo;:&ldquo;UNHEALTHY_NODES&rdquo;,&ldquo;threshold&rdquo;:5,&ldquo;comparisonOperator&rdquo;:&ldquo;GREATER_OR_EQUAL_THAN&rdquo;,&ldquo;period&rdquo;:5}&rsquo; localhost:8081/clusters/1/alarms | jq .
```</p>

<h2>SLA scaling policies</h2>

<p>Scaling is the ability to increase or decrease the capacity of the Hadoop cluster or application.  When scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined.
Periscope will do the heavy lifting and based on the alarms and the scaling policy linked to them it executes the associated policy. By default a fully configured and running <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> cluster contains no SLA policies.  An SLA scaling policy can contain multiple <code>alarms</code>.</p>

<p>As an alarm is triggered a <code>scalingAdjustment</code> is applied, however to keep the cluster size within boundaries a <code>minSize</code> and <code>maxSize</code> is attached to the cluster &ndash; thus a scaling policy can never over or undersize a cluster. Also in order to avoid stressing the cluster we have introduced a <code>cooldown</code> period (minutes) &ndash; though an alarm is raised and there is an associated scaling policy, the system will not apply the policy within the configured timeframe. In an SLA scaling policy the triggered policies are applied in order.</p>

<p>Hosts can be added or removed from specific <code>hostgroups</code>. Periscope and Cloudbreak uses Apache Ambari to provision a Hadoop cluster. Ambari host groups are a set of machines with the same Hadoop “components” installed. You can set up a cluster having different hostgroups &ndash; and run different services, thus having a heterogenous cluster.</p>

<p>In the following example we downscale a cluster when the unused resources are high.</p>

<p>```</p>

<h1>set scaling policy</h1>

<p>curl -X POST -H &ldquo;Content-Type: application/json&rdquo; -d &lsquo;{&ldquo;minSize&rdquo;:2,&ldquo;maxSize&rdquo;:10,&ldquo;cooldown&rdquo;:30,&ldquo;scalingPolicies&rdquo;:[{&ldquo;name&rdquo;:&ldquo;downScaleWhenHighResource&rdquo;,&ldquo;adjustmentType&rdquo;:&ldquo;NODE_COUNT&rdquo;,&ldquo;scalingAdjustment&rdquo;:2,&ldquo;hostGroup&rdquo;:&ldquo;slave_1&rdquo;,&ldquo;alarmId&rdquo;:&ldquo;101&rdquo;},{&ldquo;name&rdquo;:&ldquo;upScaleWhenHighPendingContainers&rdquo;,&ldquo;adjustmentType&rdquo;:&ldquo;PERCENTAGE&rdquo;,&ldquo;scalingAdjustment&rdquo;:40,&ldquo;hostGroup&rdquo;:&ldquo;slave_1&rdquo;,&ldquo;alarmId&rdquo;:&ldquo;100&rdquo;}]}&rsquo; localhost:8081/clusters/1/policies | jq .
```</p>

<p>For the <code>policy</code> related REST operations you can check the <a href="http://docs.periscope.apiary.io/reference/scaling-policy">API</a> documentation.</p>

<p>Let us know how Periscope works for you &ndash; and for updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Periscope - autoscaling for Hadoop YARN]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/"/>
    <updated>2014-08-27T16:37:37+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope</id>
    <content type="html"><![CDATA[<p><em>Periscope is a powerful, fast, thick and top-to-bottom right-hander, eastward from Sumbawa&rsquo;s famous west-coast. Timing is critical, as needs a number of elements to align before it shows its true colors.</em></p>

<p><em>Periscope brings QoS and autoscaling to Hadoop YARN. Built on cloud resource management and YARN schedulers, allows to associate SLA policies to applications.</em></p>

<p>After the very positive reception of <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> &ndash; the first open source and cloud agnostic Hadoop as a Service API &ndash; today we are releasing the <code>public beta</code> version of our open source <strong>SLA policy based autoscaling API</strong> for Hadoop YARN clusters.</p>

<h2>Overview</h2>

<p>The purpose of Periscope is to bring QoS and autoscaling to a multi-tenant Hadoop YARN cluster, while allowing to apply SLA policies to individual applications.
At <a href="http://sequenceiq.com">SequenceIQ</a> working with multi-tenant Hadoop clusters for quite a while, we have always seen the same frustration and fight for resource between users.
The <strong>FairScheduler</strong> was partially solving this problem &ndash; bringing in fairness based on the notion of <a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Ghodsi.pdf">Dominant Resource Fairness</a>.
With the emergence of Hadoop 2 YARN and the <strong>CapacityScheduler</strong> we had the option to maximize throughput and utilization for a multi-tenant cluster in an operator-friendly manner.
The scheduler works around the concept of queues. These queues are typically setup by administrators to reflect the economics of the shared cluster.
While this is a pretty good abstraction and brings some level of SLA for predictable workloads, it often needs proper design ahead.
The queue hierarchy and resource allocation needs to be changed when new tenants and workloads are moved to the cluster.</p>

<p>Periscope was designed around the idea of <code>autoscaling</code> clusters &ndash; without any need to preconfigure queues, cluster nodes or apply capacity planning ahead.</p>

<!--more-->


<h2>How it works</h2>

<p>Periscope monitors the application progress, the number of YARN containers/resources and their allocation, queue depths, the number of available cluster nodes and their health.
Since we have switched to YARN a while ago (been among the first adopters) we have run an open source <a href="https://github.com/sequenceiq/yarn-monitoring">monitoring project</a>, based on R.
We have been collecting metrics from the YARN Timeline server, Hadoop Metrics2 and Ambari&rsquo;s Nagios/Ganglia &ndash; and profiling the applications and correlating with these metrics.
One of the key findings was that while low level metrics are good to understand the cluster health &ndash; they might not necessarily help on making decisions when applying different SLA policies on a multi-tenant cluster.
Focusing on higher level building blocks as queue depth, YARN containers, etc actually brings in the same quality of service, while not being lost in low level details.</p>

<p>Periscope works with two types of Hadoop clusters: <code>static</code> and <code>dynamic</code>. Periscope does not require any pre-installation &ndash; the only thing it requires is to be <code>attached</code> to an Ambari server&rsquo;s REST API.</p>

<h2>Clusters</h2>

<h3>Static clusters</h3>

<p>From Periscope point of view we consider a cluster <code>static</code> when the cluster capacity can&rsquo;t be increased horizontally.
This means that the hardware resources are already given &ndash; and the throughput can&rsquo;t be increased by adding new nodes.
Periscope introspects the job submission process, monitors the applications and applies the following SLAs:</p>

<ol>
<li> Application ordering &ndash; can guarantee that a higher priority application finishes before another one (supporting parallel or sequential execution)</li>
<li> Moves running applications between priority queues</li>
<li> <em>Attempts</em> to enforce time based SLA (execution time, finish by, finish between, recurring)</li>
<li> <em>Attempts</em> to enforce guaranteed cluster capacity requests ( x % of the resources)</li>
<li> Support for distributed (but not YARN ready) applications using Apache Slider</li>
<li> Attach priorities to SLAs</li>
</ol>


<p><em>Note: not all of the features above are supported in the first <code>public beta</code> version. There are dependencies we contributed to Hadoop, YARN and Ambari and they will be included in the next releases (2.6 and 1.7)</em></p>

<h3>Autoscaling clusters</h3>

<p>From Periscope point of view we consider a cluster <code>dynamic</code> when the cluster capacity can be increased horizontally.
This means that nodes can be added or removed on the fly &ndash; thus the cluster’s throughput can be increased or decreased based on the cluster load and scheduled applications.
Periscope works with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> to add or remove nodes from the cluster based on the SLA policies and thus continuously provide a high <em>quality of service</em> for the multi-tenand Hadoop cluster.
Just to refresh memories &ndash; <a href="http://sequenceiq.com/products.html">Cloudbreak</a> is <a href="http://sequenceiq.com">SequenceIQ&rsquo;s</a> open source, cloud agnostic Hadoop as a Service API.
Given the option of provisioning or decommissioning cluster nodes on the fly, Periscope allows you to use the following set of SLAs:</p>

<ol>
<li> Application ordering &ndash; can guarantee that a higher priority application finishes before another one (supporting parallel or sequential execution)</li>
<li> Moves running applications between priority queues</li>
<li> <em>Enforce</em> time based SLA (execution time, finish by, finish between, recurring) by increasing cluster capacity and throughput</li>
<li> Smart decommissioning &ndash; avoids HDFS storms, keeps <code>paid</code> nodes alive till the last minute</li>
<li> <em>Enforce</em> guaranteed cluster capacity requests ( x % of the resources)</li>
<li> <em>Private</em> cluster requests &ndash; supports provisioning of short lived private clusters with the possibility to merge them.</li>
<li> Support for distributed (but not YARN ready) applications using Apache Slider</li>
<li> Attach priorities to SLAs</li>
</ol>


<p><em>Note: not all of the features above are supported in the first <code>public beta</code> version. There are dependencies we contributed to Hadoop, YARN and Ambari and they will be included in the next releases (2.6 and 1.7)</em></p>

<h3>High level technical details</h3>

<p>When we have started to work on Periscope we checked different solutions &ndash; and we quickly realized that there are no any open source solutions available.
Apache YARN in general, and the scheduler API&rsquo;s in particular have solved few of the issues we had &ndash; and they have certainly bring some level of SLA to Hadoop.
At <a href="https://sequenceiq.com">SequenceIQ</a> we run all our different applications on YARN &ndash; and when we decided to create a heuristic scheduler we knew from very beginning that it has to be built on the functionality given by YARN.
In order to create Periscope we had to contribute code to YARN, Hadoop and Ambari &ndash; and were trying to add all the low level features directly into the YARN codebase.
Periscope has a <a href="http://docs.periscope.apiary.io/">REST API</a> and supports pluggable SLA policies.
We will follow up with technical details in coming blog posts, so make sure you subscribe to on of our social channels.</p>

<h3>Resources</h3>

<p>Get the code : <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></p>

<p>Documentation: <a href="http://sequenceiq.com/periscope">http://sequenceiq.com/periscope</a></p>

<p>API documentation: <a href="http://docs.periscope.apiary.io/">http://docs.periscope.apiary.io/</a></p>

<h3>What&rsquo;s next, etc</h3>

<p>This is the first <code>public beta</code> release of Periscope made available on our <a href="https://github.com/sequenceiq/periscope">GitHub</a> page.
While we are already using this internally we would like the community to help us battle test it, let us know if you find issues or raise feature requests. We are always happy to help.</p>

<p>Further releases will bring tighter integration with Ambari (especially around cluster resources), an enhanced (or potentially new) YARN scheduler and a Machine learning based job classification model.</p>

<p>We would like to say a big <em>thank you</em> for the YARN team &ndash; this effort would have not been possible without their contribution. Also we would like to thank them by supporting us with our contributions as well.
At SequenceIQ we are 100% committed to open source &ndash; and releasing Periscope under an <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache 2 licence</a> was never a question.</p>

<p>Stay tuned and make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>

<p>Enjoy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YARN Schedulers demystified - Part 1: Capacity]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/22/schedulers-part-1/"/>
    <updated>2014-07-22T12:56:39+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/22/schedulers-part-1</id>
    <content type="html"><![CDATA[<p>After our first <a href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues/">post</a> about re-prioritizing already submitted and running jobs on different queues we have received many questions and feedbacks about the Capacity Scheduler internals. While there is <code>some</code> documentation available, there is no extensive and deep documentation about how it actually works internally. Since it&rsquo;s all event based it&rsquo;s pretty hard to understand the flow &ndash; let alone debugging it. At <a href="http://sequenceiq.com/">SequenceIQ</a> we are working on a heuristic cluster scheduler &ndash; and understanding how YARN schedulers work was essential. This is part of a larger piece of work &ndash; which will lead to a fully dynamic Hadoop cluster &ndash; orchestrating <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the first open source and Docker based <strong>Hadoop as a Service API</strong>. As usual for us, this work and what we have already done around Capacity and Fair schedulers will be open sourced (or already contributed back to Apache YARN project).</p>

<h2>The Capacity Scheduler internals</h2>

<p>The CapacityScheduler is the default scheduler used with Hadoop 2.x. Its purpose is to allow multi-tenancy and share resources between multiple organizations and applications on the same cluster. You can read about the high level abstraction
<a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">here</a>. In this blog entry we&rsquo;ll examine it
from a deep technical point of view (the implementation can be found <a href="https://github.com/apache/hadoop-common/tree/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity">here</a>
as part of the ResourceManager). I&rsquo;ll try to keep it short and deal with the most important aspects, preventing to write a book about it (would be still better than twilight).</p>

<p><img class="<a" src="href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-queue-tests/src/main/resources/event-flow.gif">https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-queue-tests/src/main/resources/event-flow.gif</a>"></p>

<p>The animation shows a basic application submission event flow, but don&rsquo;t worry if you don&rsquo;t understand it yet, hopefully you will when you&rsquo;re done with the reading.</p>

<!-- more -->


<h2>Configuration</h2>

<p>It all begins with the configuration. The scheduler consists of a queue hierarchy, something like this
(except it’s xml ah.. capacity-scheduler.xml):
<code>
yarn.scheduler.capacity.maximum-am-resource-percent=0.2
yarn.scheduler.capacity.maximum-applications=10000
yarn.scheduler.capacity.node-locality-delay=40
yarn.scheduler.capacity.root.acl_administer_queue=*
yarn.scheduler.capacity.root.capacity=100
yarn.scheduler.capacity.root.default.acl_administer_jobs=*
yarn.scheduler.capacity.root.default.acl_submit_applications=*
yarn.scheduler.capacity.root.default.capacity=80
yarn.scheduler.capacity.root.default.maximum-capacity=80
yarn.scheduler.capacity.root.default.state=RUNNING
yarn.scheduler.capacity.root.default.user-limit-factor=1
yarn.scheduler.capacity.root.low.acl_administer_jobs=*
yarn.scheduler.capacity.root.low.acl_submit_applications=*
yarn.scheduler.capacity.root.low.capacity=20
yarn.scheduler.capacity.root.low.maximum-capacity=40
yarn.scheduler.capacity.root.low.state=RUNNING
yarn.scheduler.capacity.root.low.user-limit-factor=1
yarn.scheduler.capacity.root.queues=default,low
</code></p>

<p><img class="<a" src="href="http://yuml.me/9d7e9977">http://yuml.me/9d7e9977</a>"></p>

<p>Generally, queues are for to prevent applications to consume more resources then they should.
Be careful when determining the capacities, because if you mess it up the <code>ResourceManager</code> won&rsquo;t start:
<code>
Service RMActiveServices failed in state INITED cause: java.lang.IllegalArgumentException: Illegal capacity of 1.1 for children of queue root.
</code>
The <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java#L255">initScheduler</a>
will parse the configuration file and create either <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java">parent</a>
or <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java">leaf</a>
queues and compute their capabilities.</p>

<p><code>
capacity.LeafQueue (LeafQueue.java:setupQueueConfigs(312)) - Initializing default
capacity = 0.8 [= (float) configuredCapacity / 100 ]
asboluteCapacity = 0.8 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 0.8 [= configuredMaxCapacity ]
absoluteMaxCapacity = 0.8 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 8000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 8000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
maxActiveApplications = 1 [= max((int)ceil((clusterResourceMemory / minimumAllocation) * maxAMResourcePerQueuePercent * absoluteMaxCapacity),1) ]
maxActiveAppsUsingAbsCap = 1 [= max((int)ceil((clusterResourceMemory / minimumAllocation) *maxAMResourcePercent * absoluteCapacity),1) ]
maxActiveApplicationsPerUser = 1 [= max((int)(maxActiveApplications * (userLimit / 100.0f) * userLimitFactor),1) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.2 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.75 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = ADMINISTER_QUEUE: SUBMIT_APPLICATIONS:* [= configuredAcls ]
nodeLocalityDelay = 40
</code>
Although it does not imply, but application submission is only allowed to leaf queues. By default all application is submitted to
a queue called <code>default</code>. One interesting property is the <code>schedule-asynchronously</code> about which I&rsquo;ll talk later.</p>

<h2>Messaging</h2>

<p>Once the ResourceManager is up and running, the messaging starts. Mostly everything happens via events. These events are distributed with
a <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java">dispatcher</a>
among the registered event handlers. The down side of the event driven architecture that it&rsquo;s hard to follow the flow because
events can come from everywhere. The CapacityScheduler itself is registered for many events, and act based on these events.
Code snippets are from branch <code>trunk</code> aka <code>3.0.0-SNAPSHOT</code>.
```java
 @Override
  public void handle(SchedulerEvent event) {</p>

<pre><code>switch(event.getType()) {
case NODE_ADDED:
{
  NodeAddedSchedulerEvent nodeAddedEvent = (NodeAddedSchedulerEvent)event;
  addNode(nodeAddedEvent.getAddedRMNode());
  recoverContainersOnNode(nodeAddedEvent.getContainerReports(),
    nodeAddedEvent.getAddedRMNode());
}
break;
case NODE_REMOVED:
{
  NodeRemovedSchedulerEvent nodeRemovedEvent = (NodeRemovedSchedulerEvent)event;
  removeNode(nodeRemovedEvent.getRemovedRMNode());
}
break;
case NODE_UPDATE:
{
  NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;
  RMNode node = nodeUpdatedEvent.getRMNode();
  nodeUpdate(node);
  if (!scheduleAsynchronously) {
    allocateContainersToNode(getNode(node.getNodeID()));
  }
}
break;
case APP_ADDED:
{
  AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent) event;
  addApplication(appAddedEvent.getApplicationId(),
    appAddedEvent.getQueue(), appAddedEvent.getUser());
}
break;
case APP_REMOVED:
{
  AppRemovedSchedulerEvent appRemovedEvent = (AppRemovedSchedulerEvent)event;
  doneApplication(appRemovedEvent.getApplicationID(),
    appRemovedEvent.getFinalState());
}
break;
case APP_ATTEMPT_ADDED:
{
  AppAttemptAddedSchedulerEvent appAttemptAddedEvent =
      (AppAttemptAddedSchedulerEvent) event;
  addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),
    appAttemptAddedEvent.getTransferStateFromPreviousAttempt(),
    appAttemptAddedEvent.getShouldNotifyAttemptAdded());
}
break;
case APP_ATTEMPT_REMOVED:
{
  AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent =
      (AppAttemptRemovedSchedulerEvent) event;
  doneApplicationAttempt(appAttemptRemovedEvent.getApplicationAttemptID(),
    appAttemptRemovedEvent.getFinalAttemptState(),
    appAttemptRemovedEvent.getKeepContainersAcrossAppAttempts());
}
break;
case CONTAINER_EXPIRED:
{
  ContainerExpiredSchedulerEvent containerExpiredEvent =
      (ContainerExpiredSchedulerEvent) event;
  ContainerId containerId = containerExpiredEvent.getContainerId();
  completedContainer(getRMContainer(containerId),
      SchedulerUtils.createAbnormalContainerStatus(
          containerId,
          SchedulerUtils.EXPIRED_CONTAINER),
      RMContainerEventType.EXPIRE);
}
break;
default:
  LOG.error("Invalid eventtype " + event.getType() + ". Ignoring!");
}
</code></pre>

<p>  }
```</p>

<h3>NODE_ADDED</h3>

<p>Each time a node joins the cluster the <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java#L237">ResourceTrackerService</a>
registers the <code>NodeManager</code> and as part of the transition sends a <code>NodeAddedSchedulerEvent</code>. The scheduler keeps track of
the global cluster resources and adds the node&rsquo;s resources to the global.
<code>
Added node amb1.mycorp.kom:45454 clusterResource: &lt;memory:5120, vCores:8&gt;
Added node amb2.mycorp.kom:45454 clusterResource: &lt;memory:10240, vCores:16&gt;
</code>
It is also needed to update all the queue metrics since the cluster got bigger, thus the queue capacities also change. More likely to happen
that a new application can be scheduled. If the <code>isWorkPreservingRecoveryEnabled</code> is enabled on the <code>ResourceManager</code> it can recover
containers on a re-joining node.</p>

<h3>NODE_REMOVED</h3>

<p>There can be many reasons that a node is being removed from the cluster, but the scenario is almost the same as adding one. A
<code>NodeRemovedSchedulerEvent</code> is sent and the scheduler subtracts the node&rsquo;s resources from the global and updates all the queue metrics.
Things can be a little bit complicated since the node was active part of the resource scheduling and can have running containers and
reserved resources. The scheduler will kill these containers and notify the applications so they can request new containers and
unreserve the resources.
<code>
rmnode.RMNodeImpl (RMNodeImpl.java:transition(569)) - Deactivating Node amb4.mycorp.kom:45454 as it is now DECOMMISSIONED
rmnode.RMNodeImpl (RMNodeImpl.java:handle(385)) - amb4.mycorp.kom:45454 Node Transitioned from RUNNING to DECOMMISSIONED
capacity.CapacityScheduler (CapacityScheduler.java:removeNode(980)) - Removed node amb4.mycorp.kom:45454 clusterResource: &lt;memory:15360, vCores:24&gt;
</code></p>

<h3>APP_ADDED</h3>

<p>On application <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java#L266">submission</a>
an <code>AppAddedSchedulerEvent</code> is made and the scheduler will decide to accept the application or not. It depends whether it
was submitted to a leaf queue and the user have the appropriate rights (ACL) to submit to this queue and the queue can have more applications. If
any of these fails the scheduler will reject the application by sending an <code>RMAppRejectedEvent</code>. Otherwise it will register a new
<code>SchedulerApplication</code> and notify the target queue&rsquo;s parents about it and updates the queue metrics.
<code>
capacity.ParentQueue (ParentQueue.java:addApplication(495)) - Application added - appId: application_1405323437551_0001 user: hdfs leaf-queue of parent: root #applications: 1
capacity.CapacityScheduler (CapacityScheduler.java:addApplication(544)) - Accepted application application_1405323437551_0001 from user: hdfs, in queue: default
</code></p>

<h3>APP_REMOVED</h3>

<p>The analogy is the same as between <code>NODE_ADDED</code> and <code>NODE_REMOVED</code>. Updates the queue metrics and notifies the parent&rsquo;s that an application finished, removes the application and sets its final state.</p>

<h3>APP_ATTEMPT_ADDED</h3>

<p>After the <code>APP_ADDED</code> event the application is in <code>inactive</code> mode. It means it won<code>t get any resources scheduled for - only an attempt to run it. One application can have many attempts as it can fail for many reasons.
```
rmapp.RMAppImpl (RMAppImpl.java:handle(639)) - application_1405323437551_0001 State change from SUBMITTED to ACCEPTED
resourcemanager.ApplicationMasterService (ApplicationMasterService.java:registerAppAttempt(611)) - Registering app attempt : appattempt_1405323437551_0001_000001
attempt.RMAppAttemptImpl (RMAppAttemptImpl.java:handle(659)) - appattempt_1405323437551_0001_000001 State change from NEW to SUBMITTED
capacity.LeafQueue (LeafQueue.java:activateApplications(763)) - Application application_1405323437551_0001 from user: hdfs activated in queue: default
capacity.LeafQueue (LeafQueue.java:addApplicationAttempt(779)) - Application added - appId: application_1405323437551_0001 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@46a224a4, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
capacity.CapacityScheduler (CapacityScheduler.java:addApplicationAttempt(567)) - Added Application Attempt appattempt_1405323437551_0001_000001 to scheduler from user hdfs in queue default
```
Attempt states are transferred from one to another. By sending an</code>AppAttemptAddedSchedulerEvent<code>the scheduler actually tries to allocate resources. First, the application goes into the pending applications list of the queue and if the queue limits allows it, it goes into the active applications list. This active application list is the one that the queue uses when trying to allocate resources.
It works in FIFO order, but I'll elaborate on it in the</code>NODE_UPDATE` part.</p>

<h3>APP_ATTEMPT_REMOVED</h3>

<p>On <code>AppAttemptRemovedSchedulerEvent</code> the scheduler cleans up after the application. Releases all the allocated, acquired, running containers
(in case of <code>ApplicationMaster</code> restart the running containers won&rsquo;t get killed), releases all reserved containers,
cleans up pending requests and informs the queues.
<code>
rmapp.RMAppImpl (RMAppImpl.java:handle(639)) - application_1405323437551_0001 State change from FINISHING to FINISHED
capacity.CapacityScheduler (CapacityScheduler.java:doneApplicationAttempt(598)) - Application Attempt appattempt_1405323437551_0001_000001 is done. finalState=FINISHED
scheduler.AppSchedulingInfo (AppSchedulingInfo.java:clearRequests(108)) - Application application_1405323437551_0001 requests cleared
capacity.LeafQueue (LeafQueue.java:removeApplicationAttempt(821)) - Application removed - appId: application_1405323437551_0001 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
amlauncher.AMLauncher (AMLauncher.java:run(262)) - Cleaning master appattempt_1405323437551_0001_000001
</code></p>

<h3>NODE_UPDATE</h3>

<p>Normally <code>NodeUpdateSchedulerEvents</code> arrive every second from every node. By setting the earlier mentioned <code>schedule-asynchronously</code> to true
the behavior of this event handling can be altered in a way that container allocations happen asynchronously from these events. Meaning
the CapacityScheduler tries to allocate new containers in every 100ms on a different <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java#L361">thread</a>.
Before going into details let&rsquo;s discuss another important aspect.</p>

<h4>Allocate</h4>

<p>The <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java#L675">allocate</a>
method is used as a heartbeat. This is the most important call between the <code>ApplicationMaster</code> and the scheduler. Instead of using a simple empty message, the heartbeat can contain resource requests of the application. The reason it is important here is that the scheduler, more precisely the queue &ndash; when tries to allocate resources &ndash; it will check the active applications in FIFO order and see their resource requests.
<code>
{Priority: 20, Capability: &lt;memory:1024, vCores:1&gt;, # Containers: 2, Location: *, Relax Locality: true}
{Priority: 20, Capability: &lt;memory:1024, vCores:1&gt;, # Containers: 2, Location: /default-rack, Relax Locality: true}
{Priority: 20, Capability: &lt;memory:1024, vCores:1&gt;, # Containers: 2, Location: amb1.mycorp.kom, Relax Locality: true}
</code>
Let&rsquo;s examine these requests:</p>

<ul>
<li>Priority: requests with higher priority gets served first (mappers got 20, reducer 10, AM 0)</li>
<li>Capability: denotes the required container&rsquo;s size</li>
<li>Location: where the AM would like to get the containers. There are 3 types of locations: node-local, rack-local, off-switch.
The location is based on the location of the blocks of the data. It is the <code>ApplicationMaster</code>&rsquo;s duty to locate them. Off-switch means
that any node in the cluster is good enough. Typically the <code>ApplicationMaster</code>&rsquo;s container request is an off-switch request.</li>
<li>Relax locality: in case it is set to false, only node-local container can be allocated. By default it is set to true.</li>
</ul>


<h4>NODE_UPDATE</h4>

<p>Let&rsquo;s get back to <code>NODE_UPDATE</code>. What happens at every node update and why it happens so frequently? First of all, nodes are running the containers. Containers start and stop all the time thus the scheduler needs an update about the state of the nodes. Also it updates their resource capabilities as well. After the updates are noted, the scheduler tries to allocate containers on the nodes. The same applies to every node in the cluster. At every <code>NODE_UPDATE</code> the scheduler checks if an application reserved a container on this node. If there is reservation then tries to fulfill it. Every node can have only one reserved container. After the reservation it tries to allocate more containers by going through all the queues starting from <code>root</code>. The node can have one more container if it has at least the minimum allocation&rsquo;s resource capability. Going through the child queues of <code>root</code> it checks the queue&rsquo;s active applications and its resource requests by priority order. If the application has request for this node it tries to allocate it. If the relax locality is set to true it could also allocate container even though there is no explicit request for this node, but that&rsquo;s not what&rsquo;s going to happen first. There is another term called delay scheduling. The scheduler tries to delay any non-data-local
request, but cannot delay it for so long. The <code>localityWaitFactor</code> will determine how long to wait until fall back to rack-local then the end to off-switch requests. If everything works out it allocates a container and tracks its resource usage. If there is not enough resource capability one application can reserve a container on this node, and at the next update may can use this reservation. After the allocation is made the <code>ApplicationMaster</code> will submit a request to the node to launch this container and assign a task to it to run.
The scheduler does not need to know what task the AM will run in the container.</p>

<h3>CONTAINER_EXPIRED</h3>

<p>The <a href="https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/ContainerAllocationExpirer.java">ContainerAllocationExpirer&rsquo;s</a>
responsibility to check if a container expires and when it does it sends an <code>ContainerExpiredSchedulerEvent</code> and the scheduler will notify the application to remove the container. The value of how long to wait until a container is considered dead can be configured.</p>

<h2>Animation</h2>

<p>The animation on top shows a basic event flow starting from adding 2 nodes and submitting 2 applications with attempts. Eventually the node updates tries to allocate resources to those applications. After reading this post I hope it makes sense now.</p>

<h2>What&rsquo;s next?</h2>

<p>In the next part of this series I&rsquo;ll compare it with FairScheduler, to see the differences.</p>

<p>For updates and further blog posts follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>

<p>Enjoy.</p>
]]></content>
  </entry>
  
</feed>
