<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloud | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/cloud/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-09-01T13:45:13+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Infrastructure management with CloudFormation]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/29/aws-cloudformation-makes-everything-easier/"/>
    <updated>2014-08-29T14:13:06+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/29/aws-cloudformation-makes-everything-easier</id>
    <content type="html"><![CDATA[<p>Our Hadoop as a Service solution, <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> integrates with multiple cloud providers to deploy Hadoop clusters in the cloud. It means that every time a cluster is requested, Cloudbreak goes to the selected cloud provider and creates a new, separated infrastructure through the provider’s API. Building this infrastructure can be a real pain and can cause a lot of problems &ndash; it involves a lot of API calls, the polling of created building blocks, the management of failures and the necessary rollbacks to name a few. With the help of <a href="http://aws.amazon.com/cloudformation/">AWS CloudFormation</a> we were able to avoid most of these problems when integrating AWS in Cloudbreak.</p>

<h3>Problems with the traditional approach</h3>

<p>When Cloudbreak creates a Hadoop cluster it should first create the underlying infrastructure on the cloud provider. The building blocks are a bit different on every provider, the following resources are created on AWS:</p>

<ul>
<li>a virtual private cloud (VPC)</li>
<li>a subnet</li>
<li>an internet gateway</li>
<li>a route table</li>
<li>an auto scaling group and its launch configuration</li>
<li>a security group</li>
</ul>


<p>Although AWS has a pretty good API and great SDKs to communicate with it, we needed to deal with the above described problems if we would like to create all of these elements one by one through the Java SDK. The code would start with something like this with the creation of the VPC:</p>

<p>```java
AmazonEC2Client amazonEC2Client = new AmazonEC2Client(basicSessionCredentials);
amazonEC2Client.setRegion(region);</p>

<p>CreateVpcRequest vpcRequest = new CreateVpcRequest().withCidrBlock(10.0.0.0/24);
CreateVpcResponse vpcResponse = amazonEC2Client.createVpc(createVpcRequest);</p>

<p>//poll vpc creation until it’s state is available
waitForVPC(amazonEC2Client, vpcResponse.getVpc());</p>

<p>ModifyVpcAttributeRequest modifyVpcRequest = new ModifyVpcAttributeRequest().withEnableDnsHostnames(true).withEnableDnsSupport(true);
amazonEC2Cient.modifyVpcAttribute(modifyVpcRequest);
```</p>

<!--more-->


<p>The above code is only a taste of the whole thing. The VPC is one of the most simple resources with very few attributes to set. Also the polling of the creation process isn’t detailed here as well as failure handling. In addition the different resources would be scattered around the code making it impossible to have an overview of the whole stack and making it much harder to find bugs or to modify some attributes. With CloudFormation all of the above problems can be solved very easily.</p>

<h3>Introduction to CloudFormation</h3>

<p>According to the <a href="http://aws.amazon.com/cloudformation/">AWS CloudFormation documentation</a> it was designed to create and manage a collection of related AWS resources easily and provisioning and updating them in an orderly and predictable fashion. What it really means is that the resources can be described declaratively in a JSON document (a <em>template</em>) and the whole <em>stack</em> can be created/updated/deleted with a simple API call. AWS also handles failures, and rollbacks the whole stack if something goes wrong. Furthermore it is able to send notifications to <em>SNS topics</em> when some event occurs (e.g.: a resource creation started or the resource is completed), making the polling of resource creations unnecessary.</p>

<h3>Template structure</h3>

<p>We don’t want to give a detailed introduction on how the structure of a CloudFormation template look like, the <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html">AWS documentation</a> covers it really well and there are also a lot of <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/sample-templates-services-us-west-2.html">samples</a>.
Instead we’re trying to focus on the advantages that CloudFormation gave us while using it, so let’s jump in the middle and start with a simple example. The declaration of a VPC in a template looks like this:</p>

<p>```json
&ldquo;Resources&rdquo; : {
  &ldquo;MY_VPC&rdquo; : {</p>

<pre><code>"Type" : "AWS::EC2::VPC",
"Properties" : {
  "CidrBlock" : { "10.0.0.0/16" },
  "EnableDnsSupport" : "true",
  "EnableDnsHostnames" : "true",
  "Tags" : [
    { "Key" : "Application", "Value" : { "Ref" : "AWS::StackId" } },
    { "Key" : "Network", "Value" : "Public" }
  ]
}
</code></pre>

<p>  }
}
```</p>

<p>The JSON syntax can be a bit complicated sometimes, especially when dealing with a lot of references to other properties with the <em>&ldquo;Ref&rdquo;</em> keyword or some other built-in CloudFormation <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html">functions</a>, but it is much clearer than the Java code above.
Other than resources, there are other parts of a CloudFormation template (<em>Conditions</em>, <em>Mappings</em>, <em>Outputs</em>, <em>Intrinsic Functions</em>) but here we will cover only one more: <em>Parameters</em>.</p>

<p>Declaring a parameter means that there is no hard-coded value for a given attribute, rather it is given to the template when creating the stack. If you’d like to have an EC2 Instance  in your template but you don’t want to hardcode its type, you can have a parameter like this:</p>

<p>```json
&ldquo;Parameters&rdquo; : {
  &ldquo;InstanceType&rdquo; : {</p>

<pre><code>"Description" : "EC2 instance type",
"Type" : "String",
"Default" : "m3.medium",
"AllowedValues" : [ "m3.medium","m3.large","m3.xlarge","m3.2xlarge"],
"ConstraintDescription" : "must be a valid EC2 instance type."
</code></pre>

<p>  }
}
```</p>

<p>After it’s declared, you can reference it from a resource with the <em>Ref</em> keyword:</p>

<p>```json
&ldquo;EC2Instance&rdquo; : {
  &ldquo;Type&rdquo; : &ldquo;AWS::EC2::Instance&rdquo;,
  &ldquo;Properties&rdquo; : {</p>

<pre><code>"SecurityGroups" : [ { "Ref" : "InstanceSecurityGroup" } ],
"InstanceType" : { "Ref" : "InstanceType" },
"KeyName" : { "Ref" : "KeyName" },
"ImageId" : "ami-123456",
"EbsOptimized" : "true"
</code></pre>

<p>  }
}
```</p>

<p>You can reference not only parameters, but other resources as well. In the above code example there is a reference to <em>InstanceSecurityGroup</em> that is an <em>AWS::EC2::SecurityGroup</em> type resource and that is declared in an other part of the template.</p>

<h3>Creating the stack</h3>

<p>So we’ve declared a few resources, how can we tell AWS to create the stack? Let’s see how it looks like with the Java SDK (two parameters are passed to the template):</p>

<p>```java
CreateStackRequest createStackRequest = new CreateStackRequest()</p>

<pre><code>.withStackName(“MyCFStack")
.withTemplateBody(templateAsString)
.withNotificationARNs(notificationTopicARN)
.withParameters(
    new Parameter().withParameterKey("InstanceCount").withParameterValue(“3"),
    new Parameter().withParameterKey("InstanceType").withParameterValue(“m3.large"));
</code></pre>

<p>client.createStack(createStackRequest);
```</p>

<p>And that’s it. It’s every code that should written in Java to create the complete stack. It is pretty straightforward, the only thing that needs to be explained is the <em>notification ARN</em> part. It is the identifier of an <em>SNS topic</em> and it is detailed below.</p>

<h3>Callbacks</h3>

<p>CloudFormation is able to send notifications to SNS <em>topics</em> when an event occurs. An event is when a resource creation is started, finished or failed (and the same with delete). SNS is Amazon’s Simple Notification Service that enables endpoints to subscribe to a topic, and when a message is sent to a topic every subscriber receives that message. AWS supports a lot of endpoint types. It can send notifications by email or text message, to Amazon Simple Queue Service (SQS) queues, or to any HTTP/HTTPS endpoint. In the Cloudbreak project we’re using HTTP endpoints as callback URLs. We’re also creating topics and subscriptions from code but that could fill up another full blog post.</p>

<p>If you just like to try SNS, you can create a topic and a subscription from the AWS Console. After you have a confirmed subscription of an HTTP endpoint (e.g.: <em>example.com/sns</em>), you can very easily create an HTTP endpoint in Java (with some help from <a href="http://spring.io/">Spring</a>):</p>

<p><code>java
@RequestMapping(value="sns", method = RequestMethod.POST)
@ResponseBody
public ResponseEntity&lt;String&gt; receiveSnsMessage(@RequestBody String request) {
  // parse and handle request
}
</code></p>

<p>For a more detailed example see the <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/java/com/sequenceiq/cloudbreak/controller/AmazonSnsController.java">controller class</a> in Cloudbreak.
So every time a CloudFormation stack event occurs, Cloudbreak receives a message that is parsed and handled correctly &ndash; there is no need to poll the creation of resources and dealing with timeouts.</p>

<h3>Failures and rollbacks</h3>

<p>It is always possible that something will go wrong when creating a stack with a lot of resources. With the traditional approach you must keep track of the resources that were created and you will have to implement some rollback logic that gets called when something unexpected happens and that rolls back the already created elements somehow. With CloudFormation these things are completely done by AWS.</p>

<p>The resources in the stack are tracked so the only thing you have to save is the identifier of the stack. If one of the resources fails to be created AWS rolls back every other resource and puts the stack in <em>ROLLBACK_COMPLETED</em> state. It also sends the failure message to the SNS topic with the exact cause of the failure.
The same is true if you’d like to delete the stack. The only call that you will have to send to the AWS API is the deletion of the stack (very similar to the creation in Java). CloudFormation will delete every resource one by one and will take care of failures.</p>

<h3>Notes</h3>

<p>The template we used in Cloudbreak is available <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/resources/templates/aws-cf-stack.ftl">here</a>. It is not a pure CloudFormation template because of some limitations &ndash; the number of attached volumes cannot be specified dynamically and it is not possible to specify it as a parameter if spot priced instances are needed or not &ndash; we ended up generating the template with Freemarker.</p>

<h3>Terraform</h3>

<p>The <a href="http://www.hashicorp.com/products">company</a> that brought us <a href="http://www.vagrantup.com/">Vagrant</a>, <a href="http://www.packer.io/">Packer</a> and a few more useful things has recently announced a new project named <a href="http://www.terraform.io/intro/index.html">Terraform</a>. Terraform is inspired by tools like CloudFormation or <a href="https://wiki.openstack.org/wiki/Heat">OpenStack’s Heat</a>, but goes further as it supports multiple cloud platforms and their services can also be combined. If you’re interested in managing infrastructure from code and configuration you should check out that project too, we’ll keep an eye on it for sure.</p>

<p>Stay tuned and make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Periscope - autoscaling for Hadoop YARN]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope/"/>
    <updated>2014-08-27T16:37:37+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope</id>
    <content type="html"><![CDATA[<p><em>Periscope is a powerful, fast, thick and top-to-bottom right-hander, eastward from Sumbawa&rsquo;s famous west-coast. Timing is critical, as needs a number of elements to align before it shows its true colors.</em></p>

<p><em>Periscope brings QoS and autoscaling to Hadoop YARN. Built on cloud resource management and YARN schedulers, allows to associate SLA policies to applications.</em></p>

<p>After the very positive reception of <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> &ndash; the first open source and cloud agnostic Hadoop as a Service API &ndash; today we are releasing the <code>public beta</code> version of our open source <strong>SLA policy based autoscaling API</strong> for Hadoop YARN clusters.</p>

<h2>Overview</h2>

<p>The purpose of Periscope is to bring QoS and autoscaling to a multi-tenant Hadoop YARN cluster, while allowing to apply SLA policies to individual applications.
At <a href="http://sequenceiq.com">SequenceIQ</a> working with multi-tenant Hadoop clusters for quite a while, we have always seen the same frustration and fight for resource between users.
The <strong>FairScheduler</strong> was partially solving this problem &ndash; bringing in fairness based on the notion of <a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Ghodsi.pdf">Dominant Resource Fairness</a>.
With the emergence of Hadoop 2 YARN and the <strong>CapacityScheduler</strong> we had the option to maximize throughput and utilization for a multi-tenant cluster in an operator-friendly manner.
The scheduler works around the concept of queues. These queues are typically setup by administrators to reflect the economics of the shared cluster.
While this is a pretty good abstraction and brings some level of SLA for predictable workloads, it often needs proper design ahead.
The queue hierarchy and resource allocation needs to be changed when new tenants and workloads are moved to the cluster.</p>

<p>Periscope was designed around the idea of <code>autoscaling</code> clusters &ndash; without any need to preconfigure queues, cluster nodes or apply capacity planning ahead.</p>

<!--more-->


<h2>How it works</h2>

<p>Periscope monitors the application progress, the number of YARN containers/resources and their allocation, queue depths, the number of available cluster nodes and their health.
Since we have switched to YARN a while ago (been among the first adopters) we have run an open source <a href="https://github.com/sequenceiq/yarn-monitoring">monitoring project</a>, based on R.
We have been collecting metrics from the YARN Timeline server, Hadoop Metrics2 and Ambari&rsquo;s Nagios/Ganglia &ndash; and profiling the applications and correlating with these metrics.
One of the key findings was that while low level metrics are good to understand the cluster health &ndash; they might not necessarily help on making decisions when applying different SLA policies on a multi-tenant cluster.
Focusing on higher level building blocks as queue depth, YARN containers, etc actually brings in the same quality of service, while not being lost in low level details.</p>

<p>Periscope works with two types of Hadoop clusters: <code>static</code> and <code>dynamic</code>. Periscope does not require any pre-installation &ndash; the only thing it requires is to be <code>attached</code> to an Ambari server&rsquo;s REST API.</p>

<h2>Clusters</h2>

<h3>Static clusters</h3>

<p>From Periscope point of view we consider a cluster <code>static</code> when the cluster capacity can&rsquo;t be increased horizontally.
This means that the hardware resources are already given &ndash; and the throughput can&rsquo;t be increased by adding new nodes.
Periscope introspects the job submission process, monitors the applications and applies the following SLAs:</p>

<ol>
<li> Application ordering &ndash; can guarantee that a higher priority application finishes before another one (supporting parallel or sequential execution)</li>
<li> Moves running applications between priority queues</li>
<li> <em>Attempts</em> to enforce time based SLA (execution time, finish by, finish between, recurring)</li>
<li> <em>Attempts</em> to enforce guaranteed cluster capacity requests ( x % of the resources)</li>
<li> Support for distributed (but not YARN ready) applications using Apache Slider</li>
<li> Attach priorities to SLAs</li>
</ol>


<p><em>Note: not all of the features above are supported in the first <code>public beta</code> version. There are dependencies we contributed to Hadoop, YARN and Ambari and they will be included in the next releases (2.6 and 1.7)</em></p>

<h3>Autoscaling clusters</h3>

<p>From Periscope point of view we consider a cluster <code>dynamic</code> when the cluster capacity can be increased horizontally.
This means that nodes can be added or removed on the fly &ndash; thus the cluster’s throughput can be increased or decreased based on the cluster load and scheduled applications.
Periscope works with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> to add or remove nodes from the cluster based on the SLA policies and thus continuously provide a high <em>quality of service</em> for the multi-tenand Hadoop cluster.
Just to refresh memories &ndash; <a href="http://sequenceiq.com/products.html">Cloudbreak</a> is <a href="http://sequenceiq.com">SequenceIQ&rsquo;s</a> open source, cloud agnostic Hadoop as a Service API.
Given the option of provisioning or decommissioning cluster nodes on the fly, Periscope allows you to use the following set of SLAs:</p>

<ol>
<li> Application ordering &ndash; can guarantee that a higher priority application finishes before another one (supporting parallel or sequential execution)</li>
<li> Moves running applications between priority queues</li>
<li> <em>Enforce</em> time based SLA (execution time, finish by, finish between, recurring) by increasing cluster capacity and throughput</li>
<li> Smart decommissioning &ndash; avoids HDFS storms, keeps <code>paid</code> nodes alive till the last minute</li>
<li> <em>Enforce</em> guaranteed cluster capacity requests ( x % of the resources)</li>
<li> <em>Private</em> cluster requests &ndash; supports provisioning of short lived private clusters with the possibility to merge them.</li>
<li> Support for distributed (but not YARN ready) applications using Apache Slider</li>
<li> Attach priorities to SLAs</li>
</ol>


<p><em>Note: not all of the features above are supported in the first <code>public beta</code> version. There are dependencies we contributed to Hadoop, YARN and Ambari and they will be included in the next releases (2.6 and 1.7)</em></p>

<h3>High level technical details</h3>

<p>When we have started to work on Periscope we checked different solutions &ndash; and we quickly realized that there are no any open source solutions available.
Apache YARN in general, and the scheduler API&rsquo;s in particular have solved few of the issues we had &ndash; and they have certainly bring some level of SLA to Hadoop.
At <a href="https://sequenceiq.com">SequenceIQ</a> we run all our different applications on YARN &ndash; and when we decided to create a heuristic scheduler we knew from very beginning that it has to be built on the functionality given by YARN.
In order to create Periscope we had to contribute code to YARN, Hadoop and Ambari &ndash; and were trying to add all the low level features directly into the YARN codebase.
Periscope has a <a href="http://docs.periscope.apiary.io/">REST API</a> and supports pluggable SLA policies.
We will follow up with technical details in coming blog posts, so make sure you subscribe to on of our social channels.</p>

<h3>Resources</h3>

<p>Get the code : <a href="https://github.com/sequenceiq/periscope">https://github.com/sequenceiq/periscope</a></p>

<p>Documentation: <a href="http://sequenceiq.com/periscope">http://sequenceiq.com/periscope</a></p>

<p>API documentation: <a href="http://docs.periscope.apiary.io/">http://docs.periscope.apiary.io/</a></p>

<h3>What&rsquo;s next, etc</h3>

<p>This is the first <code>public beta</code> release of Periscope made available on our <a href="https://github.com/sequenceiq/periscope">GitHub</a> page.
While we are already using this internally we would like the community to help us battle test it, let us know if you find issues or raise feature requests. We are always happy to help.</p>

<p>Further releases will bring tighter integration with Ambari (especially around cluster resources), an enhanced (or potentially new) YARN scheduler and a Machine learning based job classification model.</p>

<p>We would like to say a big <em>thank you</em> for the YARN team &ndash; this effort would have not been possible without their contribution. Also we would like to thank them by supporting us with our contributions as well.
At SequenceIQ we are 100% committed to open source &ndash; and releasing Periscope under an <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache 2 licence</a> was never a question.</p>

<p>Stay tuned and make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>

<p>Enjoy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create Hadoop clusters in the cloud using a CLI]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/07/clodubreak-shell/"/>
    <updated>2014-08-07T19:43:01+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/07/clodubreak-shell</id>
    <content type="html"><![CDATA[<p>Few weeks back we have announced <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">Cloudbreak</a> &ndash; the open source Hadoop as a Service API. Included in the release we open sourced a REST API, REST client, UI and a CLI/shell. In this post we’d like to show you how easy is to use <a href="https://github.com/sequenceiq/cloudbreak-shell">Cloudbreak shell</a> in order to create on demand Hadoop clusters on your favorite cloud provider.</p>

<p>While it’s up to everybody&rsquo;s personal preference whether to use a UI, a command line interface or the REST API directly, at SequenceIQ we prefer to use command line tools whenever it’s possible because it’s much faster than interacting with a web UI and it’s a better candidate for automation. Yeah &ndash; we have mentioned this many times &ndash; we are <code>obsessed with automation</code>; any step which is a candidate of doing it twice we script and automate it.</p>

<p>This <code>thing</code> with the automation does not affect the effort and quality standards we put on building the UI &ndash; <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> has an extremely intuitive and clean <strong>responsive</strong> UI and it’s built on the latest and greatest web UI framework &ndash; <a href="https://angularjs.org/">Angular JS</a>. We will have a post about the UI, however we consider it so simple to use that we ask you to go ahead and give it a try. You are a signup and a few clicks away from your Hadoop cluster.</p>

<p>Now back to the CLI. Remember one of our Apache contribution &ndash; the <a href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/">Ambari shell and REST API</a>? Well, the Cloudbreak shell is built on the same technology &ndash; Spring Shell. It’s an interactive shell that can be easily extended using a Spring based programming model and battle tested in various projects like Spring Roo, Spring XD, and Spring REST Shell Combine these two projects to create a powerful tool.</p>

<h2>Cloudbreak Shell</h2>

<p>The goal with the CLI was to provide an interactive command line tool which supports:</p>

<ul>
<li>all functionality available through the REST API or Cloudbreak web UI</li>
<li>makes possible complete automation of management task via <strong>scripts</strong></li>
<li>context aware command availability</li>
<li>tab completion</li>
<li>required/optional parameter support</li>
<li><strong>hint</strong> command to guide you on the usual path</li>
</ul>


<h2>Install Cloudbreak Shell</h2>

<p>You have 3 options to give it a try:</p>

<ul>
<li>use our prepared <a href="https://registry.hub.docker.com/u/sequenceiq/cloudbreak/">docker image</a></li>
<li>download the latest self-containing executable jar form our maven repo</li>
<li>build it from source</li>
</ul>


<p>We will follow up with the first two, in this post we’d like to guide you through the third option.</p>

<h3>Build from source</h3>

<p>If want to use the code or extend it with new commands follow the steps below. You will need:
&ndash; jdk 1.7
&ndash; maven 3.x.x</p>

<p><code>
git clone https://github.com/sequenceiq/cloudbreak-shell.git
cd cloudbreak-shell
mvn clean package
</code></p>

<!--more-->


<h2>Connect to Cloudbreak</h2>

<p>In order to use the shell you will have to have a Cloudbreak account. You can get one by subscribing to our hosted and free <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> instance. Alternatively you can build your own Cloudbreak and deploy it within your organization &ndash; for that just follow up with the steps in the Cloudbreak <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">documentation</a>. We suggest to try our hosted solution as in case you have any issues we can always help you with. Please feel free to create bugs, ask for enhancements or just give us feedback by either using our <a href="https://github.com/sequenceiq/cloudbreak">GitHub repository</a> or the other channels highlighted in the product documentation (Google Groups, email or social channels).
The shell is built as a single executable jar with the help of <a href="http://projects.spring.io/spring-boot/">Spring Boot</a>.</p>

<p>```
Usage:
  java -jar cloudbreak-shell-0.1-SNAPSHOT.jar                  : Starts Cloudbreak Shell in interactive mode.
  java -jar cloudbreak-shell-0.1-SNAPSHOT.jar &mdash;cmdfile=<FILE> : Cloudbreak executes commands read from the file.</p>

<p>Options:
  &mdash;cloudbreak.host=<HOSTNAME>       Hostname of the Cloudbreak REST API Server [use:cloudbreak-api.sequenceiq.com].
  &mdash;cloudbreak.port=<PORT>           Port of the Cloudbreak REST API Server [use:80].
  &mdash;cloudbreak.user=<USER>           Username of the Cloudbreak user [use:your user name ].
  &mdash;cloudbreak.password=<PASSWORD>   Password of the Cloudbreak admin [use: your password].</p>

<p>Note:
  All options are mandatory.
<code>``
Once you are connected you can start to create a cluster. If you are lost and need guidance through the process you can use</code>hint<code>. You can always use</code>TAB<code>for completion. Note that all commands are</code>context aware` &ndash; they are available only when it makes sense &ndash; this way you are never confused and guided by the system on the right path.</p>

<h3>Create a cloud credential</h3>

<p>In order to start using Cloudbreak you will need to have a cloud user, for example an Amazon AWS account. Note that Cloudbreak <strong>does not</strong> store you cloud user details &ndash; we work around the concept of <a href="http://aws.amazon.com/iam/">IAM</a> &ndash; on Amazon (or other cloud providers) you will have to create an IAM role, a policy and associate that with your Cloudbreak account &ndash; for further documentation please refer to the <a href="http://sequenceiq.com/cloudbreak/#accounts">documentation</a>.</p>

<p><code>
credential createEC2 --description “description" --name “myCredentialName" --roleArn "arn:aws:iam::NUMBER:role/cloudbreak-ABC" --sshKeyUrl “URL towards your AWS public key"
</code></p>

<p>Alternatively you can upload your public key from a file as well, by using the <code>—sshKeyPath</code> switch. You can check whether the credential was creates successfully by using the <code>credential list</code> command. You can switch between your cloud credential &ndash; when you’d like to use one and act with that you will have to use:</p>

<p><code>
credential select --id #ID of the credential
</code></p>

<h3>Create a template</h3>

<p>A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion. A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).</p>

<p><code>
template createEC2 --description "awstemplate" --name "awstemplate" --region EU_WEST_1 --instanceType M3Large --sshLocation 0.0.0.0/0
</code>
You can check whether the template was created successfully by using the <code>template list</code> command. Check the template with or select if you are happy with:</p>

<p>```
template show &mdash;id #ID of the template</p>

<p>template select &mdash;id #ID of the template
```</p>

<h3>Create a stack</h3>

<p>Stacks are template <code>instances</code> &ndash; a running cloud infrastructure created based on a template. Use the following command to create a stack to be used with your Hadoop cluster:</p>

<p><code>
stack create --name “myStackName" --nodeCount 20
</code></p>

<h3>Select a blueprint</h3>

<p>We ship default Hadoop cluster blueprints with Cloudbreak. You can use these blueprints or add yours. To see the available blueprints and use one of them please use:</p>

<p>```
blueprint list</p>

<p>blueprint select &mdash;id #ID of the blueprint
```</p>

<h3>Create a Hadoop cluster</h3>

<p>You are almost done &ndash; one more command and this will create your Hadoop cluster on your favorite cloud provider. Same as the API, or UI this will use your <code>template</code>, and by using CloudFormation will launch a cloud <code>stack</code> &ndash; once the <code>stack</code> is up and running (cloud provisioning is done) it will use your selected <code>blueprint</code> and install your custom Hadoop cluster with the selected components and services. For the supported list of Hadoop components and services please check the <a href="http://sequenceiq.com/cloudbreak/#supported-components">documentation</a>.</p>

<p><code>
cluster create --description “my cluster desc"
</code>
You are done &ndash; you can check the progress through the Ambari UI. If you log back to <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak UI</a> you can check the progress over there as well, and learn the IP address of Ambari.</p>

<h3>Automate the process</h3>

<p>Each time you start the shell the executed commands are logged in a file line by line and later either with the <code>script</code> command or specifying an <code>—cmdfile</code> option the same commands can be executed again.</p>

<h2>Commands</h2>

<p>For the full list of available commands please check below. Please note that all commands are context aware, and you can always use <code>TAB</code> for command completion.</p>

<pre><code>* blueprint add - Add a new blueprint with either --url or --file
* blueprint defaults - Adds the default blueprints to Cloudbreak
* blueprint list - Shows the currently available blueprints
* blueprint select - Select the blueprint by its id
* blueprint show - Shows the blueprint by its id
* cluster create - Create a new cluster based on a blueprint and template
* cluster show - Shows the cluster by stack id
* credential createAzure - Create a new Azure credential
* credential createEC2 - Create a new EC2 credential
* credential defaults - Adds the default credentials to Cloudbreak
* credential list - Shows all of your credentials
* credential select - Select the credential by its id
* credential show - Shows the credential by its id
* exit - Exits the shell
* help - List all commands usage
* hint - Shows some hints
* quit - Exits the shell
* script - Parses the specified resource file and executes its commands
* stack create - Create a new stack based on a template
* stack list - Shows all of your stack
* stack select - Select the stack by its id
* stack show - Shows the stack by its id
* stack terminate - Terminate the stack by its id
* template create - Create a new cloud template
* template createEC2 - Create a new EC2 template
* template defaults - Adds the default templates to Cloudbreak
* template list - Shows the currently available cloud templates
* template select - Select the template by its id
* template show - Shows the template by its id
* version - Displays shell version
</code></pre>

<p>As usual for us &ndash; being committed to 100% open source &ndash; we are always open sourcing everything thus you can get the details on our <a href="https://github.com/sequenceiq/cloudbreak-shell">GitHub</a> repository.
Should you have any questions feel free to engage with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Launch Docker containers on Azure]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/08/04/launch-docker-containers-on-azure/"/>
    <updated>2014-08-04T19:43:01+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/08/04/launch-docker-containers-on-azure</id>
    <content type="html"><![CDATA[<p>Two weeks ago we have open sourced our cloud agnostic and Docker based Hadoop as a Service API &ndash; called <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a>.
The first public beta version supports Amazon’s AWS and Microsoft’s Azure, while we are already wrapping up a few new cloud provider integrations.</p>

<p>While there is some documentation about running Docker containers on Amazon, there is no detailed description about running Docker on the Azure cloud.
With this blog post we would like to shed some light on it &ndash; recently there have been lots of announcements from Microsoft about Docker support (Azure CLI, Kubernetes, libswarm) but they are either not finished yet or are not ready to build a robust platform on top.
We are eagerly waiting for the <a href="http://azure.microsoft.com/blog/2014/07/10/azure-collaboration-with-google-and-docker/">Kubernetes integration</a>.</p>

<p>In the meantime, if you are interested in running a <code>cluster</code> of Docker container, or do some more complex stuff then read on.</p>

<p>Just to briefly recap &ndash; with Cloudbreak we are launching on demand Hadoop clusters (check our <a href="http://blog.sequenceiq.com/blog/2014/07/25/cloudbreak-technology/">blog</a> for further technical details) in Docker containers. These containers are <code>shipped</code> to different cloud VMsm and dynamically find and join each other &ndash; they form a fully functional Hadoop cluster without the need to do anything manually on the host, or apply any manual pre-configuration.
So how are we doing this?</p>

<!--more-->


<h3>Docker ready base VM image</h3>

<p>First of all you need a base image with Docker installed &ndash; thus for that we have built and made available an Ubuntu 14.04 image with Docker installed. Apart from Docker, to build a fully dynamic and <code>service discovery</code> aware Docker cluster we needed <a href="http://stedolan.github.io/jq/">jq</a> and <a href="http://www.linuxfromscratch.org/blfs/view/svn/basicnet/bridge-utils.html"> bridge-utils</a>.</p>

<p>Once this base image is created you will need to make it public and re-usable. In order to do that the image has to be published in <a href="http://vmdepot.msopentech.com/List/Index">VMdepot</a>. When you are about to use an image from VM depot, and create a VM based on that you will need to copy it in your own storage account &ndash; note that doing it at first time this can be a slow process (20-25 minutes, copying the 30 GB image).</p>

<h3>Dynamic networking</h3>

<p>Now you have an image based on that you can launch your own VMs, and the Docker container inside your VM. While there are a few options to do that, we needed to find a unified way to do so &ndash; note that  <a href="http://sequenceiq.com/cloudbreak">Cloudbreak</a> is a cloud agnostic solution &ndash; and we do not want to create init scripts for each and every cloud environment we use. Amazon’s AWS has a feature so called <code>userdata</code> &ndash; an option of passing data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon AWS: shell scripts and cloud-init directives. In order to keep the launch process unified everywhere we are using <a href="https://help.ubuntu.com/community/CloudInit">cloud-init</a> on Azure as well.</p>

<p>You can use/start Docker with different networking setup &ndash; using a bridged network or using the host network. You can check the init scripts in our <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/resources/azure-init.sh">GitHub</a> repository.</p>

<p><strong>Bridged network</strong></p>

<p>```bash</p>

<h1>set bridge0 in docker opts</h1>

<p>sh -c &ldquo;cat > /etc/default/docker&rdquo; &lt;&lt;&ldquo;EOF&rdquo;
DOCKER_OPTS=&ldquo;-b bridge0 -H unix:// -H tcp://0.0.0.0:2375&rdquo;
EOF</p>

<p>CMD=&ldquo;docker run -d -p SOURCE_PORT:DESTINATION_PORT 0 -e SERF_JOIN_IP=$SERF_JOIN_IP -e SERF_ADVERTISE_IP=$MY_IP &mdash;dns 127.0.0.1 &mdash;name ${NODE_PREFIX}${INSTANCE_IDX} -h ${NODE_PREFIX}${INSTANCE_IDX}.${MYDOMAIN} &mdash;entrypoint /usr/local/serf/bin/start-serf-agent.sh  $IMAGE $AMBARI_ROLE&rdquo;</p>

<p>```
<strong>Host network</strong></p>

<p><code>bash
CMD="docker run -d -e SERF_JOIN_IP=$AMBARI_SERVER_IP --net=host --name ${NODE_PREFIX}${INSTANCE_IDX} --entrypoint /usr/local/serf/bin/start-serf-agent.sh  $IMAGE $AMBARI_ROLE"
</code></p>

<p><em>Note: for cloud based clusters we are giving up on the bridged based network &ndash; mostly due to Azure&rsquo;s networking limitations &ndash; and will use the <code>net=host</code> solution in the next release. The bridged network will still be a supported solution, though we are using it mostly with bare metal or multi container/host solutions.</em></p>

<p>Azure has (comparing with Amazon’s AWS or Google’s Cloud compute) an <code>uncommon</code> network setup and supports limited flexibility &ndash; in order to overcome these, and still have a dynamic Hadoop cluster different scenarios / use cases requires different Docker networking &ndash; that is quite a large <strong>undocumented</strong> topic which we will cover in our next blog posts &ndash; in particular the issues, differences and solutions to use Docker on different cloud providers. While we have briefly talked about <a href="http://sequenceiq.com/cloudbreak/#technology">Serf</a> in the <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> documentation, we will enter in deep technical details in one of our next posts as well. Should you be interested in these, make sure you follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a> for updates.</p>

<h3>SequenceIQ’s Azure REST API &ndash; open sourced</h3>

<p>At <a href="htp://sequenceiq.com">SequenceIQ</a> we always automate everything &ndash; and in order to launch VM instances, configure networks, start containers, etc we needed a REST client which we can use it from our JAVA and Scala codebase. Since the Microsoft API is XML based &ndash; <em>yo, it’s 2014</em> &ndash; we have created and open sourced a Groovy based <a href="https://github.com/sequenceiq/azure-rest-client">Azure REST API</a> &ndash; wrapping the XML calls into a nice, easy to use and clean REST API. Feel free to use it &ndash; it’s open sourced under an Apache 2 license. Note that <a href="https://cloudbreak.sequenceiq.com">Cloudbreak</a> does not store your Azure user credential &ndash; whereas with the defulat Azue CLI that would have been possible &ndash; the only thing we need from your side to work is your subscription id. The process is documented here: <a href="http://sequenceiq.com/cloudbreak/#accounts.">http://sequenceiq.com/cloudbreak/#accounts.</a></p>

<h3>Metadata service for Azure</h3>

<p>The another nice feature we have created for Azure VMs is a <code>metadata service</code>. While a service as such does exists on Amazon’s AWS it’s missing from Microsoft Azure &ndash; note that our Cloudbreak solution is a cloud agnostic one, and we always strive to use identical solution on all cloud providers. The instance metadata is data about your instance that you can use to configure or manage the running instances &ndash; and available via a REST call. We have developed a service as such for Azure &ndash; <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/java/com/sequenceiq/cloudbreak/service/stack/connector/azure/AzureMetadataSetup.java">AzureMetadataSetup</a>. As you can see we collect the metadata, and make it available under a <code>unique hash</code> for each cluster by calling the following resource: <code>/metadata/{hash}</code></p>

<p>```java
private Set<CoreInstanceMetaData> collectMetaData(Stack stack, AzureClient azureClient, String name) {</p>

<pre><code>... try {
            CoreInstanceMetaData instanceMetaData = new CoreInstanceMetaData(vmName,
                    getPrivateIP((String) virtualMachine),
                    getVirtualIP((String) virtualMachine));
            instanceMetaDatas.add(instanceMetaData);
        } catch (IOException e) { ...
</code></pre>

<p>}
```
This service is used in a few cases &ndash; for example to learn different network setups as the hosts are using different network options than the Docker containers.</p>

<p>As usual for us &ndash; being committed to 100% open source &ndash; we are always open sourcing everything thus you can get the details on our <a href="https://github.com/sequenceiq/cloudbreak">GitHub</a> repository.
Should you have any questions feel free to engage with us on our <a href="http://blog.sequenceiq.com/">blog</a> or follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook">Facebook</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker ships Hadoop to the cloud]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/07/25/cloudbreak-technology/"/>
    <updated>2014-07-25T12:56:39+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/07/25/cloudbreak-technology</id>
    <content type="html"><![CDATA[<p>A week ago we have <a href="http://blog.sequenceiq.com/blog/2014/07/18/announcing-cloudbreak/">announced</a> and open sourced <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a>, the first Docker based Hadoop as a Service API. In this post we&rsquo;d like to introduce you into the technical details and the building blocks of the architecture.
Cloudbreak is built on the foundation of cloud providers APIs, Apache Ambari, Docker containers, Serf and dnsmasq. It is a cloud agnostic solution &ndash; as all the Hadoop services and components are running inside Docker containers &ndash; and these containers are shipped across different cloud providers.</p>

<p>Cloudbreak product documentation: <a href="http://sequenceiq.com/cloudbreak">http://sequenceiq.com/cloudbreak</a></p>

<p>Cloudbreak API documentation: <a href="http://docs.cloudbreak.apiary.io/">http://docs.cloudbreak.apiary.io/</a></p>

<h2>How it works</h2>

<p>From Docker containers point of view we have two kind of containers &ndash; based on their Ambari role &ndash; server and agent. There is one Docker container running the Ambari server, and there are many Docker containers running the Ambari agents. The used Docker image is always the same: <code>sequenceiq/ambari</code> and
the Ambari role is decided based on the <code>$AMBARI_ROLE</code> variable.</p>

<p>For example on Amazon EC2 this is how we start the containers:</p>

<p><code>bash
docker run -d -p &lt;LIST of ports&gt; -e SERF_JOIN_IP=$SERF_JOIN_IP --dns 127.0.0.1 --name ${NODE_PREFIX}${INSTANCE_IDX} -h ${NODE_PREFIX}${INSTANCE_IDX}.${MYDOMAIN} --entrypoint /usr/local/serf/bin/start-serf-agent.sh  $IMAGE $AMBARI_ROLE
</code></p>

<p>As we are starting up the instances and the Docker containers on the host, we&rsquo;d like them to join each other and be able to communicate &ndash; though we don&rsquo;t know the IP addresses beforehand. This can be challanging on cloud environments &ndash; where your IP address and DNS name is dynamically allocated &ndash; however you don&rsquo;t want to collect these imformations beforehand launching the Docker containers.
For that we use Serf &ndash; and pass along the IP address <code>SERF_JOIN_IP=$SERF_JOIN_IP</code> of the first container. Using a gossip protocol Serf will automatically discover each other, set the DNS names, and configure the routing between the nodes.
Serf reconfigures the DNS server <code>dnsmasq</code> running inside the container, and keeps it up to date with the joining or leaving nodes information.
As you can see at startup we always pass a <code>--dns 127.0.0.1</code> dns server for the container to use.</p>

<p>As you see there is no cloud specific code at the Docker containers level, the same technology can be used on bare metal as well.
Check our previous blog posts about a <a href="http://blog.sequenceiq.com/blog/2014/06/19/multinode-hadoop-cluster-on-docker/">multi node Hadoop cluster on any host</a>.</p>

<p>Obliviously there is some configuration on the host as well &ndash; for that and to handle early initialization of a cloud instance we use <a href="https://help.ubuntu.com/community/CloudInit">CloudInit</a>. We will write a blog post about these for every cloud provider we support.</p>

<p>For additional information you can check our slides from the <a href="http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning">Hadoop Summit 2014</a>.</p>

<p>Once Ambari is started it will install the selected components based on the passed Hadoop blueprint &ndash; and start the desired services.</p>

<!-- more -->


<h2>Used Technologies</h2>

<h3>Apache Ambari</h3>

<p>The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/ambari-overview.png" alt="" /></p>

<p>Ambari enables System Administrators to:</p>

<ol>
<li>Provision a Hadoop Cluster</li>
<li>provides a step-by-step wizard for installing Hadoop services across any number of hosts.</li>
<li><p>handles configuration of Hadoop services for the cluster.</p></li>
<li><p>Manage a Hadoop Cluster</p></li>
<li><p>provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.</p></li>
<li><p>Monitor a Hadoop Cluster</p></li>
<li>provides a dashboard for monitoring health and status of the Hadoop cluster.</li>
<li>leverages Ganglia for metrics collection.</li>
<li>leverages Nagios for system alerting and will send emails when your attention is needed (e.g. a node goes down, remaining disk space is low, etc).</li>
</ol>


<p>Ambari enables to integrate Hadoop provisioning, management and monitoring capabilities into applications with the Ambari REST APIs.
Ambari Blueprints are a declarative definition of a cluster. With a Blueprint, you can specify a Stack, the Component layout and the Configurations to materialize a Hadoop cluster instance (via a REST API) without having to use the Ambari Cluster Install Wizard.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/ambari-create-cluster.png" alt="" /></p>

<h3>Docker</h3>

<p>Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications. Consisting of Docker Engine, a portable, lightweight runtime and packaging tool, and Docker Hub, a cloud service for sharing applications and automating workflows, Docker enables apps to be quickly assembled from components and eliminates the friction between development, QA, and production environments. As a result, IT can ship faster and run the same app, unchanged, on laptops, data center VMs, and any cloud.</p>

<p>The main features of Docker are:</p>

<ol>
<li>Lightweight, portable</li>
<li>Build once, run anywhere</li>
<li>VM &ndash; without the overhead of a VM</li>
<li>Each virtualized application includes not only the application and the necessary binaries and libraries, but also an entire guest operating system</li>
<li><p>The Docker Engine container comprises just the application and its dependencies. It runs as an isolated process in userspace on the host operating system, sharing the kernel with other containers.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/vm.png" alt="" /></p></li>
<li><p>Containers are isolated</p></li>
<li>It can be automated and scripted</li>
</ol>


<h3>Serf</h3>

<p>Serf is a tool for cluster membership, failure detection, and orchestration that is decentralized, fault-tolerant and highly available. Serf runs on every major platform: Linux, Mac OS X, and Windows. It is extremely lightweight.
Serf uses an efficient gossip protocol to solve three major problems:</p>

<ul>
<li><p>Membership: Serf maintains cluster membership lists and is able to execute custom handler scripts when that membership changes. For example, Serf can maintain the list of Hadoop servers of a cluster and notify the members when nodes come online or go offline.</p></li>
<li><p>Failure detection and recovery: Serf automatically detects failed nodes within seconds, notifies the rest of the cluster, and executes handler scripts allowing you to handle these events. Serf will attempt to recover failed nodes by reconnecting to them periodically.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/serf-gossip.png" alt="" /></p></li>
<li><p>Custom event propagation: Serf can broadcast custom events and queries to the cluster. These can be used to trigger deploys, propagate configuration, etc. Events are simple fire-and-forget broadcast, and Serf makes a best effort to deliver messages in the face of offline nodes or network partitions. Queries provide a simple realtime request/response mechanism.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/serf-event.png" alt="" /></p></li>
</ul>


<p>For updates follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a>, <a href="https://twitter.com/sequenceiq">Twitter</a> or <a href="https://www.facebook.com/sequenceiq">Facebook</a>.</p>
]]></content>
  </entry>
  
</feed>
